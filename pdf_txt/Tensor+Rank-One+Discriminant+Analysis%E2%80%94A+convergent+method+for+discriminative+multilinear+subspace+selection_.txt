1  
  
 
Discriminative Linear and 
Multilinear Subspace Methods 
 
Dacheng TAO 
 
A Thesis Submitted in Partial Fu lfilment of the Requirements 
for the Degree of Doct or of Philosophy 
 
 
 
 
October 2006 
 
School of Computer Scien ce and Information Systems 
 2  
This thesis is submitted to the School of Computer Science and Information Systems, Birkbeck College, University of London in partial fulfilment of the 
requirements for the degree of Doctor of Philosophy. I hereby declare that this 
thesis is my own work and except where otherwise stated. This submission has not been submitted for a degree to any other university or institution. 
 
  
 
   
Dacheng TAO 
School of Computer Science and Information Systems Birkbeck College 
University of London 
 3  
  
 
  
 
To my family. 
 1 Table of Contents 
 
Table of Contents .................................................................................................... 1 List of Figures ......................................................................................................... 1 
List of Tables........................................................................................................... 7 
Abstract ................................................................................................................... 9 Acknowledgement................................................................................................. 10 
1. Introduction ..................................................................................................... 1 
1.1 Thesis Organization............................................................................. 6 1.2 Publications ....................................................................................... 10 
2. Discriminative Linear Subspace Methods..................................................... 11 
2.1 Principal Component Analysis.......................................................... 14 2.2 Linear Discriminant Analysis............................................................ 17 2.3 General Averaged Divergences Analysis.......................................... 32 
2.4 Geometric Mean for Subspace Selection .......................................... 38 2.5 Kullback–Leibler Divergence based Subspace Selection ................. 41 
2.6 Comparison using Synthetic Data ..................................................... 50 
2.7 Statistical Experiments...................................................................... 56 
2.8 Real–World Experiments .................................................................. 63 2.9 Summary ........................................................................................... 65 
3. Discriminative Multilinear Subspace Method............................................... 66 
3.1 Tensor Algebra.................................................................................. 70 3.2 The Relationship between LSM and MLSM .................................... 77 3.3 Tensor Rank One Analysis................................................................ 79 
3.4 General Tensor Analysis ................................................................... 83 3.5 Two Dimensional Linear Discriminant Analysis.............................. 87 
3.6 General Tensor Discriminant Analysis ............................................. 90 
3.7 Manifold Learning using Tensor Representations ............................ 96 
3.8 GTDA for Human Gait Recognition................................................. 99 3.9 Summary ......................................................................................... 120 
4. Supervised Tensor Learning........................................................................ 121 
4.1 Convex Optimization based Learning............................................. 124 4.2 Supervised Tensor Learning: A Framework ................................... 129 2 4.3 Supervised Tensor Learning: Examples.......................................... 136 
4.4 Iterative Feature Extraction Mo del based on Supervised Tensor 
Learning....................................................................................................... 152 
4.5 Experiments..................................................................................... 156 
4.6 Summary ......................................................................................... 188 
5. Thesis Conclusion ....................................................................................... 189 
6. Appendices .................................................................................................. 194 
6.1 Appendix for Chapter 2................................................................... 194 6.2 Appendix for Chapter 3................................................................... 205 
References ........................................................................................................... 211 1 List of Figures 
 
Figure 1.1. The plan of the thesis. ........................................................................... 6 Figure 2.1. LDA fails to find the optimal projection direction for classification, 
because it does not utilize the discriminative information preserved in the class 
covariances. ........................................................................................................... 19 Figure 2.2. The samples in each class ar e drawn from a Gaussian distribution. 
LDA finds a projection direction, which merges class 1 and class 2. One of the 
reasonable projection directions for classification trades the distance between the class 1 and the class 2 against the distance between the class 1, 2 and class 3. This 
example is a sketch of the synthetic data used in Figure 2.7 in §2.6.3. ................ 22 
Figure 2.3. The geometric setting for Bregman divergence.................................. 33 Figure 2.4. Heteroscedastic example: in th is figure, from left to right, from top to 
bottom, there are nine subfigures showing the projection directions (indicated by 
lines in each subfigure) obtained using LDA, HDA, aPAC, WLDA, FS–LDA, 
HLDA, ODA, MGMKLD(0), and MGMKLD(1). In this experiment, the linear 
classifier 
() ()1ˆˆˆˆ lnT
ii i i xm xm−−Σ− + Σ  is applied as the classification method 
after a subspace selection procedure, where ˆim and ˆ
iΣ are the estimated ith 
class mean and covariance matrix in the lower dimensional space, respectively. 
The training classification errors of these methods are 0.3410, 0.2790, 0.3410, 
0.3410, 0.3430, 0.2880, 0.2390, 0.2390, and 0.2390, respectively....................... 51 Figure 2.5. The maximization of the geometric mean of the normalized 
divergences is not sufficient for subspace selection.............................................. 52 
Figure 2.6. Multimodal problem: in this figure, from left to right, from top to 
bottom, there are nine subfigures, which s how the projection directions (indicated 
by lines in each subfigure) by using LDA, HDA, aPAC, FS–LDA(3), FS–LDA(8), HLDA, MODA, M–MGMKLD(0), and M–MGMKLD(1). In this experiment, the 
linear classifier 
() ()1ˆˆˆˆ lnT
ii i i xm xm−−Σ− + Σ  is applied as the classification 
method after a subspace selection procedure, where ˆim and ˆ
iΣ are the 
estimated ith class mean and covariance matrix in the lower dimensional space, 
respectively. The training classification errors of these methods are 0.0917, 2 0.0167, 0.0917, 0.0917, 0.0917, 0.0167, 0.0083, 0.0083, and 0.0083, respectively.
............................................................................................................................... 53 
Figure 2.7. Class separation problem: in this figure, from left to right, from top to 
bottom, there are nine subfigures to describe the projection directions (indicated 
by lines in each subfigure) by using LDA, HDA, aPAC, WLDA, FS–LDA, HLDA, ODA, MGMKLD(0), and MGMKLD(5). In this experiment, the linear 
classifier 
() ()1ˆˆˆˆ lnT
ii i i xm xm−−Σ− + Σ  is applied as the classification method 
after a subspace selection procedure, where ˆim and ˆ
iΣ are the estimated ith 
class mean and covariance matrix in the lower dimensional space, respectively. 
The training classification errors of these methods are 0.3100, 0.3033, 0.2900, 0.3033, 0.0567, 0.3100, 0.3100, 0.1167, and 0.0200, respectively. MGMKLD(5) 
finds the best projection direction for classification. ............................................ 55 
Figure 2.8. Class separation problem: in th is figure, from left to right, there are 
three subfigures to describe the projecti on directions (indicated by lines in each 
subfigure) by using aPAC, FS–LDA, and MGMKLD(5). In this experiment, the 
linear classifier 
() ()1ˆˆˆˆ lnT
ii i i xm xm−−Σ− + Σ  is applied as the classification 
method after a subspace selection procedure, where ˆim and ˆ
iΣ are the 
estimated ith class mean and covariance matrix in the lower dimensional space, 
respectively. The training classification errors of these methods are 0.1200, 
0.1733, and 0.0267, respectively. MGMKLD(5) finds the best projection direction 
for classification. ................................................................................................... 55 
Figure 2.9. Data Generation Model. In this model, 14 w=, 20 w=, 3 4 w=− , 
44 w=, 54 w=, 2k=, ()11 2 0 1Tmk m w=+rr, 22 00Tm=r, ()[]3 3 10 10 0, 1Tmk m w=+rr, 
()[]44 1 0 1 0 1, 0Tmk m w=+rr, and ()[]55 5 5 5 5 1, 0, 1, 0Tmk m w=+rr............................... 56 
Figure 2.10. Initial values: From left to right, from top to bottom, the subfigures 
show the mean value and the corresponding standard deviation of the KL 
divergence between the class i and the class j of these 50 different initial values in 
the 10th (50th, 100th, and 1000th) training iterations. Because there are 5 classes in 
the training set, there are 20 KL divergences to examine. The circles in each subfigure show the mean values of the KL divergences for 50 different initial 
values. The error bars show the corre sponding standard deviations. For better 
visualization, the scale for showing the standard deviations is 10 times larger than 3 the vertical scale in each subfigure. The standard deviations of these 20 KL 
divergences approach 0 as the number of  training iterations increases. ............... 60 
Figure 2.11. MGMKLD(2 c) has no nest structure property.................................. 62 
Figure 2.12. Samples in the USPS database [53].................................................. 63 
Figure 3.1. A gray level face image is a second order tensor, i.e., a matrix. Two indices are required for pixel locations. The face image comes from 
http://www.merl.com/projects/images/face–rec.gif. ............................................. 66 
Figure 3.2. A color image is a third order tensor, which is also a data cuboid, because three indices are required to locate elements. Two indices are used for 
pixel locations and one index is used to local the color information (e.g., R, G, and 
B)........................................................................................................................... 66  
Figure 3.3. A color video shot is a fourth order tensor. Four indices are used to locate elements. Two indices are used fo r pixel locations; one index is used to 
locate the color information; and the other index is used for time. The video shot 
comes from http://www–nlpir.nis t.gov/projects/trecvid/. ..................................... 67 
Figure 3.4. A third order tensor 
123LL LR××∈X ....................................................... 70 
Figure 3.5. The mode–1, mode–2, and mode–3 matricizing of a third order tensor 
322R××∈X . ............................................................................................................. 71 
Figure 3.6. The mode–2 product of a third order tensor 123LL LR××∈X  and a matrix 
22LLUR′×∈  results in a new tensor 123LL LR′××∈Y . ................................................. 73 
Figure 3.7. The HOSVD of a third order tensor 123LL LR××∈X .............................. 74 
Figure 3.8. The best rank one approximation to a third order tensor 123LL LR××∈X .
............................................................................................................................... 79 
Figure 3.9. The best ()123 rank , , RRR−  approximation of a third order tensor 
123LL LR××∈X ........................................................................................................... 83 
Figure 3.10. The columns show the averaged gait images of nine different people 
in the Gallery of the USF database descri bed in §3.8.2.1. The four rows in the 
figure from top to bottom are based on images taken from the Gallery, ProbeB, ProbeH, and ProbeK, respectively. The averaged gait images in a single column 
come from the same person................................................................................. 100 
Figure 3.11. The real part of Gabor functions with five different scales and eight different directions. ............................................................................................. 101 4 Figure 3.12. Gabor gait: the rows show different scales and the columns show 
different directions for an averaged gait image................................................... 102 Figure 3.13. Three new methods for averaged gait image representation using 
Gabor functions: GaborS, GaborD, and GaborSD.............................................. 103 
Figure 3.14. The thirteen columns are Gallery gait, ProbeA gait, ProbeB gait, 
ProbeC gait, ProbeD gait, ProbeE gait, ProbeF gait, ProbeG gait, ProbeH gait, 
ProbeI gait, ProbeJ gait, ProbeK gait, and ProbeL gait, respectively. From the first 
row to the last row are the original gait, GaborD (from 0 to 4), GaborS (from 0 to 7), and GaborSD, respectively. The Gallery gait and ProbeA – ProbeI gaits are 
described in Section 3.8.2.1................................................................................. 105 
Figure 3.15. The averaged gait extraction and the dissimilarity measure........... 108 Figure 3.16. Recognition performance comparison for rank one evaluation. From top–left to bottom, in each of the thirteen subfigures (Probes A, B, C, D, E, F, G, 
H, I, J, K, L, and the average performance), there are eleven bars, which 
correspond to the performance of HMM, IMED+LDA, LDA, LDA+Fusion, 
2DLDA+LDA, GTDA+LDA(H), GTDA+LDA, Gabor+GTDA+LDA(H), 
GaborD+GTDA+LDA(H), GaborS+GTD A+LDA(H), and GaborSD+GTDA+ 
LDA(H), respectively.......................................................................................... 116 Figure 3.17. Recognition performance comparison for rank five evaluation. From 
top–left to bottom, in each of the thirteen subfigures (Probes A, B, C, D, E, F, G, 
H, I, J, K, L, and the average performance), there are ten bars, which correspond to the performance of IMED+LDA, LD A, LDA+Fusion, 2DLDA+LDA, GTDA 
+LDA(H), GTDA+LDA, Gabor+GTDA+LDA(H), GaborD+GTDA+LDA(H), 
GaborS+GTDA+LDA(H), and GaborSD+GTDA+LDA(H), respectively......... 118 Figure 3.18. Experimental based convergence justification for the alternating 
projection method for GTDA. The x–coordinate is the number of training 
iterations and the y–coordinate is the error value Err, as defined in Step 6 in Table 
3.6. From left to right, these four sub–figures show how Err changes with the increasing number of training iterations with different threshold values (88%, 
90% 92% and 94%) defined in (3.37). ................................................................ 119 
Figure 4.1. Tensor based learning machin e vs. vector based learning machine. 122 
Figure 4.2. The geometric interpretation of the optimal solution 
*wr in D for a 
convex optimization problem defined in (4.3). ................................................... 125 5 Figure 4.3. The geometric interpretation of the optimal solution *wr in D for 
LP defined in (4.5)............................................................................................... 126 
Figure 4.4. The geometric interpretation of the optimal solution *wr in D for 
QP defined in (4.6). ............................................................................................. 127 
Figure 4.5. The third order tensor example for the alternating projection in STL.
............................................................................................................................. 13 3 
Figure 4.6. SVM maximizes the margin between positive and negative training 
samples. ............................................................................................................... 136 
Figure 4.7. MPM separates positive samples from negative samples by 
maximizing the probability of the correct classification for future samples. The 
intersection point minimizes the maximum of the Mahalanobis distances between 
positive and negative samples, i.e., it has the same Mahalanobis distances to the mean of the positive samples and the mean of the negative samples.................. 142 
Figure 4.8. FDA separates positive sample s from negative samples by maximizing 
the symmetric Kullback–Leibler divergence between two classes under the 
assumption that the two classes share the same covariance matrix. ................... 146 Figure 4.9. DML obtains a metric, such that “
k–nearest neighbors always belong 
to the same class while examples from different classes are separated by a large 
margin ”................................................................................................................ 149 
Figure 4.10. Iterative feature extractio n model for third order tensors. .............. 152 
Figure 4.11. Attention model for image representation. ..................................... 156 
Figure 4.12. Example images fro m the tiger category. ....................................... 157 
Figure 4.13. Example images fro m the leopard category.................................... 158 
Figure 4.14. One hundred ROIs in the tiger category. ........................................ 159 
Figure 4.15. One hundred ROIs in the leopard category..................................... 160 
Figure 4.16. TMPM converges effectively. ........................................................ 162 
Figure 4.17. TMPM is stable with different initial values. ................................. 163 
Figure 4.18. First 10 EigenGaits (the fi rst column), first 10 FisherGaits (the 
second column), first 10 TR1AGaits (the third column), and first 10 TR1DAGaits 
(the fourth column). From the figure, we  can see that EigenGaits and FisherGaits 
are dense, while TR1AGaits and TR1DAGaits  are sparse, because they take the 
structure information into account to reduce the number of unknown parameters 
in discriminant learning....................................................................................... 166 6 Figure 4.19. GBL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §4.5.2.............. 171 
Figure 4.20. GAL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §4.5.2.............. 173 
Figure 4.21. CBL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §4.5.2.............. 175 
Figure 4.22. CAL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §4.5.2.............. 177 
Figure 4.23. GBBL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §4.5.2.............. 179 
Figure 4.24. GBAL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §4.5.2.............. 181 
Figure 4.25. CBBL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §4.5.2.............. 183 
Figure 4.26. CBAL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §4.5.2.............. 185 7 List of Tables 
 
Table 2.1. Fractional–Step Line ar Discriminant Analysis .................................... 23 
Table 2.2.  Linear Discriminant Analysis via Generalized Singular Value Decomposition....................................................................................................... 26 
Table 2.3. Linear Discriminant Analysis via QR Decomposition......................... 28 
Table 2.4. General Averaged Divergences  Analysis for Subspace Selection ....... 35 
Table 2.5. Optimization procedure for MGMKLD............................................... 42 
Table 2.6. Optimization procedure for M–MGMKLD. ........................................ 44 
Table 2.7: Averaged classification errors (the mean for 800 experiments) of LDA, 
HDA, aPAC, WLDA, FS–LDA, HLDA, ODA, MGMKLD(0), and 
MGMKLD(2 c). (The nearest neighbour rule)....................................................... 58 
Table 2.8: S tandard deviations of classification errors  for 800 experiments of 
LDA, HDA, aPAC, WLDA, FS–LDA, HLDA, ODA, MGMKLD(0), and MGMKLD(2 c). (The nearest neighbour rule)....................................................... 58 
Table 2.9: Averaged classification errors (the mean for 800 experiments) of LDA, 
HDA, aPAC, WLDA, FS–LDA, HLDA, ODA, MGMKLD(0), and 
MGMKLD(2 c). (The linear classifier 
()()1 ˆˆˆˆ lnT
ii i i xm xm−−Σ−+ Σrr rr) ................. 59 
Table 2.10: S tandard deviations of classification errors  for 800 experiments of 
LDA, HDA, aPAC, WLDA, FS–LDA, HLDA, ODA, MGMKLD(0), and 
MGMKLD(2 c). (The linear classifier ()()1 ˆˆˆˆ lnT
ii i i xm xm−−Σ−+ Σrr rr) ................. 59 
Table 2.11. Performances (c lassification errors) of linear methods on the USPS 
database. (The nearest neighbour rule).................................................................. 64 Table 2.12. Performances (error rates) of kernel methods on the USPS database. A 
nine dimensional feature space is selected for each algorithm. (The nearest neighbour rule) ...................................................................................................... 64 
Table 3.1. Alternating Projection for the Higher–Order Singular Value 
Decomposition....................................................................................................... 75 
Table 3.2. Alternating Projection for the Best Rank One Approximation ............ 80 
Table 3.3. Alternating Projection fo r the Tensor Rank One Analysis .................. 81 
Table 3.4. Alternating Projection for the Best Rank
–( )12,,M RRRL  
Approximation....................................................................................................... 84 
Table 3.5. Alternating Projection for General Tensor Analysis............................ 85 8 Table 3.6. Alternating Projection for Ge neral Tensor Discriminant Analysis...... 94 
Table 3.7. Computational complexities of  the alternating projection method based 
optimization procedure of GTDA with Gabor/GaborD/GaborS/GaborSD 
representations..................................................................................................... 106 
Table 3.8. Twelve probe sets for challenge experiments. ................................... 107 Table 3.9. Rank one recognition rate s for human gait recognition. .................... 113 
Table 3.10. Rank five rec ognition rates for human gait recognition................... 114 
Table 4.1. Alternating Projection for the Supervised Tensor Learning .............. 131 
Table 4.2. Alternating Projection for th e Tensor Rank One Discriminant Analysis
............................................................................................................................. 15 4 
Table 4.3. TMPM vs. MPM ................................................................................ 161 Table 4.4. Parameters in convergen ce examination for eight probes.................. 169 
Table 4.5. Rank One recognition precision for eight probes............................... 169 
Table 4.6. Rank five recognition precision for eight probes. .............................. 169 9 Abstract 
 
Linear discriminant analysis (LDA) sheds light on classification tasks in computer 
vision. However, classification based on LD A can perform poorly in applications 
because LDA has: 1) the heteroscedastic  problem, 2) the multimodal problem, 3) 
the class separation problem, and 4) the small sample size (SSS) problem. In this 
thesis, the first three problems are called the model based problems because they 
arise from the definition of LDA. The f ourth problem arises when there are too 
few training samples. The SSS problem is also known as the overfitting problem. 
 To address the model based problems, a new criterion is proposed: maximization 
of the geometric mean of the Kullback–Leibler (KL) divergences and the 
normalized KL divergences for subspace selection when samples are sampled 
from Gaussian mixture models. The new criterion reduces all model based problems significantly, as shown by a large number of empirical studies. 
 
To address the SSS problem in LDA, a general tensor discriminant analysis 
(GTDA) is developed. GTDA makes better use of the structure information of the 
objects in vision research. GTDA is a multilinear extension of a modified LDA. It 
involves the estimation of a series of proj ection matrices in projecting an object in 
the form of a tensor from a high dimensional feature space to a low dimensional 
feature space. Experiments on human ga it recognition demonstrate that GTDA 
combined with LDA and nearest neighbor rule outperforms competing methods. 
 Based on the work above, the standard convex optimization based approach to 
machine learning is generalized to the supervised tensor learning (STL) 
framework, in which tensors are accepted as input. The solution to STL is 
obtained in practice using an alternating projection algorithm. This generalization 
reduces the overfitting problem when there are only a few training samples. An 
empirical study confirms that the overfitting is reduced. 10 Acknowledgement 
 
Professor Stephen J. Maybank is an excellent advisor and friend! During the two years for my PhD study, I owed him a lot because I have obtained everlasting 
helps from him when I was confused; I have obtained inspirations from him when 
I was depressed; and I have obtained rigorous comments from him when I was not correct on mathematics. I am anxious to have his comments, because they are my 
precious stairs, which shape my research from style to content. I would like to 
express my deepest gratitude to Steve.  
I would like to thank my second supervisor and friend Dr. Xuelong Li for his 
discussions, suggestions, and encouragements. With Xuelong’s help, I can do my research here directly and I can become familiar with London with little effort.  
I appreciate my external examiner Profe ssor Andrew Blake at Microsoft Research 
Cambridge Lab and internal examiner Professor Shaogang Gong at Queen Mary 
for their time reviewing this thesis, their patience for my PhD viva, their 
constructive comments to refine this thesis, and their encouragements for my 
work.  
I would like to thank my fantastic coll aborator and friend Professor Xindong Wu 
(UVM) for his valuable and interesting suggestions.  I would like to thank my fantastic collaborator and friend Professor Christos 
Faloutsos (CMU) and Mr. Jimeng Sun (CMU ) for an interesting collaboration 
work on tensor analysis for streaming data. 
 
I would like to thank Professor Mario Figueiredo (Instituto Superior Técnico) for 
his explanation on the boundary problem in the training procedure of the Gaussian mixture model. 
 
I need to thank the financial support from the Overseas Research Students Award Scheme (ORSAS), the Birkbeck College, and the School of Computer Science 11 and Information Systems. Without them, I have no chance to finish my PhD in 
UK.  
I would also like to thank Mr. Phil Gregg, Mr. Graham Sadler, Mr. Andrew 
Watkins, and Mr. Phil Docking for their help on computing resources. Without their help, I would not have obtained my experimental results in time. I occupied 
many computers simultaneously (usually more than five) in the laboratory many 
times.  
I would like to thank Ms. Betty Walters and Ms. Gilda Andreani for their help in 
administration during my study. I thank all members in the department for their encouragements and suggestions.  
I would like to thank my good friends, Dr. Renata Camargo (Birkbeck), Mr. 
Liangliang Cao (UIUC), Dr. Yuzhong Chen (Tokyo), Dr. Jun Cheng (CUHK), Ms. Victoria Gagova (Birkbeck), Dr. Qi Li (UDEL), Dr. Zhifeng Li (CUHK), Mr. 
Wei Liu (CUHK), Ms. Ju Luan (CUHK), Mr. Bo Luo (PSU), Mr. Xiaolong Ma 
(SUNY-SB), Dr. Mingli Song (ZJU), Mr. Jimeng Sun (CMU), Mr. Wei Wang (UC-Davis), Mr. Xiaogang Wang (MIT),  Dr. Yonggang Wen (MIT), Dr. Hui 
Xiong (Rutgers), Dr. Dong Xu (Columbi a), Mr. Jin Xu (NLPR), Mr. Tianqiang 
Yuan (CUHK), Dr. Dell Zhang (Birkbeck), etc. Moreover, I especially need to 
thank Mr. Xiaolong Ma for his friendly support. I also need to thank Dr. Qi Li for useful discussions. 
 Finally, I owe my family a lot for many things. I thank all my family members for 
their extreme understanding and infinite support of my PhD study and academic 
pursuits. 
  1 1. Introduction 
 
Linear subspace methods [58] have been us ed as an important pre–processing step 
in many applications of classificati on for dimension reduction or subspace 
selection, because of the so called curse of dimensionality [7]. The aim of the 
linear subspace methods is to project the original high dimensional feature space 
to a low dimensional subspace. Many methods have been proposed for selecting 
the low dimensional subspace, e.g., principal component analysis (PCA) [64] and 
linear discriminant analysis (LDA) [103]. PCA finds a subspace, which minimizes reconstruction error, while LDA finds a subspace, which separates different 
classes in the projected low dimensional space. 
In this thesis, we mainly focus on discriminative linear subspace methods, especially on LDA, because LDA has been widely used in classification tasks in computer vision research, such as image segmentation [97], relevance feedback in 
content based image retrieval [139][152][153][154], image database indexing [113], video shots classification [43], medical image analysis [116], object 
recognition and categorization [54], na tural scene classification [156], face 
recognition [158][118][34][168], gait rec ognition [46][78][79][166][165][142], 
fingerprint recognition [59], palmpr int recognition [62][179], texture 
classification [133], and hand writing classification [119]. 
The following view of LDA is taken in this thesis: when samples are drawn from 
different Gaussian distributions [60] with identical covariance matrices, LDA maximizes the arithmetic mean of the Kullback–Leibler (KL) [21] divergences 
between all pairs of distributions after projection into a subspace. From this point 
of view, LDA has the following problems: 1) heteroscedastic problem [29][28][35][70][61][81][99]: LDA ignores the discriminative information, which 
is present when the covariance matrices change between classes; 2) multimodal 
problem [51][28]: each class is modelled by a single Gaussian distribution; and 3) 
class separation problem [103][95][96][99][146]: LDA merges classes which are close together in the original feature sp ace. We call these three problems model 
based problems, because they are part of the limitations of the definition of LDA. 
Apart from the model based problems, LDA also has the small sample size (SSS) problem [38][49][139][123][19][175][55][173][174][180], when the number of 2 training samples is less than the dimension of the feature space. In the LDA 
model, two scatter matrices are calculated. These are the between class scatter matrix [39] and the within class scatter matrix [39]. The between class scatter 
matrix measures the separation between different classes. The larger the volume 
(e.g., the trace value) of the matrix is, the better the different classes are separated. The within class scatter matrix describes the scatter of samples around their 
respective class centers. The smaller the volume of the matrix is, the closer the 
samples are to their respective class centers. LDA finds a subspace, which maximizes the ratio between the trace of the projected between class scatter 
matrix and the trace of the projected within class scatter matrix. The solution is 
given by an eigenvalue decomposition [44] of the product of the inverse of the within class scatter matrix and the between class scatter matrix. In many real applications, LDA cannot be applied in th is straightforward way, because the rank 
of the within class scatter is deficient [ 47], i.e., the inverse of the matrix does not 
exist. To deal with the model based problems, it is important to develop a flexible 
framework. Because LDA is equal to the maximization of the arithmetic mean of 
the KL divergences when samples are obtained from different Gaussian distributions with identical covariances, we develop a general averaged 
divergences analysis framework, which extends LDA in two ways: 1) 
generalizing the KL divergences to the Bregman divergence [14], which is a general distortion measure of probability distributions and 2) generalizing the 
arithmetic mean to the generalized mean [48], which includes as special cases a 
large number of mean functions, e.g., the arithmetic mean, the geometric mean [20], and the harmonic mean [1][48]. Because this framework takes different 
covariance matrices into account, it can make better use of the information in 
heteroscedastic data. Then we combine the Gaussian Mixture Model (GMM) [30] 
with the framework to reduce the multimodal problem. In applications, LDA tends to merge classes, which are close in the original high dimensional space. 
This problem is reduced with the proposed general averaged divergences analysis 
framework by using geometric mean for subspace selection. Three criteria for subspace selection are described: 1) ma ximization of the geometric mean of the 
divergences; 2) maximization of the geometric mean of normalized divergences; 
and 3) maximization of the geometric mean of all divergences (both the 3 divergences and the normalized divergences). Experiments with computer 
generated data and digitized hand writing [53] show that the combination of the geometric mean based criteria and the KL divergence significantly reduces the 
heteroscedastic problem, the multimoda l problem and the class separation 
problem. To deal with the SSS problem, the following two steps are conducted:  
1) the LDA criterion is replaced by the difference between the trace of the 
between class scatter matrix in the proj ected subspace and the weighted trace 
of the within class scatter matrix in the projected subspace. The new criterion 
is named the differential scatter disc riminant criterion (DSDC) [143][149]. 
The relationship between LDA and DSDC is discussed in detail in Chapter 2;  2) In computer vision research, samples are often tensors, i.e., multidimensional arrays. For example, the averaged gait image [45][91], 
which is the feature used for gait recogn ition, is a matrix, or a second order 
tensor; a face image, is also a matrix in face recognition [182]; a color image [54] in object recognition is a third order tensor or a three dimensional array; 
and a color video shot (a color image sequence) [43] in video retrieval is a 
forth order tensor or a four dimensional array. Therefore, DSDC is reformulated through operations in mul tilinear algebra [115][75] or tensor 
algebra [115][75]. That is we substitute the multilinear algebra operations, 
e.g., the tensor product, the tensor cont raction, and the mode product, for the 
linear algebra operations, e.g., the matrix product, the matrix transpose, and the trace operation. Then we can directly replace the vectors, which are used 
to represent vectorized samples, with tensors, which are used to represent the original samples. 
The combination of the two steps descri bed above is named the general tensor 
discriminant analysis (GTDA) [144][147].  By this replacement, the SSS problem 
can be significantly reduced, because we need not to estimate a large projection 
matrix at a time. That is we estimate a series of small projection matrices 
iteratively by using the alternating projection method, which obtains each small 
projection matrix with all the other fixed projection matrices in an iterative way, 
i.e., an alternating projection method for optimization decouples a projection 
matrix from the others. In each iteration, the number of unknown parameters (the 
size of a projection matrix) in GTDA is much less than that of in LDA. 4 GTDA is motivated by the successes of the tensor rank one analysis (TR1A) 
[132], general tensor analysis (GTA) [75][162][171], and the two dimensional LDA (2DLDA) [174] for face recognition.  The benefits of GTDA are: 1) 
reduction of the SSS problem for subseque nt classification, e.g., by LDA; 2) 
preservation of discriminative informatio n in training tensors, while PCA, TR1A, 
and GTA do not guarantee this; 3) provision with stable recognition rates because 
the optimization algorithm of GTDA converges, while that of 2DLDA does not; 
and 4) acceptance of the general tens ors as input, while 2DLDA only accepts 
matrices as input. 
We then apply the proposed GTDA to appearance [82] based human gait 
recognition [45][46][91][92]. For appearance  based gait recognition, the averaged 
gait images are suited for human gait recognition, because: 1) the averaged gait images of the same person share similar visual effects under different 
circumstances; and 2) the averaged gait images of different people even under the 
same circumstance are very different. Motivated by the successes of Gabor function [32][80] based image decompositions for image understanding and 
object recognition, we develop three different Gabor function based image 
representations [144]: 1) the sum of Gabor functions over directions based 
representation (GaborD), 2) the sum of  Gabor functions ov er scales based 
representation (GaborS), and 3) the su m of Gabor functions over scales and 
directions based representation (GaborSD).  These representations are applied to 
recognize people from their averaged gait images. A large number of experiments were carried out to evaluate the effectiv eness (recognition rate) of gait recognition 
based on the following three successive steps: 1) the Gabor/GaborD/GaborS/ GaborSD image representations, 2) GTDA to extract features from the Gabor/ 
GaborD/GaborS/GaborSD image representations, and 3) applying LDA for 
recognition. The proposed methods achieve sound performance for gait 
recognition based on the USF HumanID Database [126]. Experimental comparisons are made with nine state of the art classification methods 
[46][66][126][167][173] in gait recognition. 
Finally, vector based learning
1 is extended to accept tensors as input. This results 
in the supervised tensor learning (STL) framework [149][150], which is the 
                                                 
1 Vector based learning means the traditional classification technique, which accepts vectors as 
input. In vector based learning, a projection vector LwR∈r and a bias bR∈ are learnt to 5 multilinear extension of the convex optim ization [11] based learning. To obtain 
the solution of an STL based learning algorithm, an alternating projection method 
is designed. Based on STL and its altern ating projection optimization algorithm, 
we illustrate some examples. That is we extend the soft margin support vector 
machine (SVM) [161][15], the nu–SVM [130][128], the least squares SVM [137][138], the minimax probability machine (MPM) [74][135], the Fisher 
discriminant analysis (FDA) [37][30][69], the distance metric learning (DML) 
[169] to their tensor versions, which are the soft margin support tensor machine (STM), the nu–STM, the least squares ST M, the tensor MPM (TMPM) [150], the 
tensor FDA (TFDA), and the multiple distance metrices learning (MDML), 
respectively. With STL, we also introduce a method for feature extraction through an iterative way [132] and develop the tensor rank one discriminant analysis (TR1DA) [145][143] as an example. The experiments for image classification 
demonstrate TMPM reduces the overfitting problem in MPM. The experiments 
for the elapsed time problem in human gait recognition show TR1DA is more effective than PCA, LDA, and TR1A. 
 
  
                                                                                                                                     
 
determine the class label of a sample LxR∈r according to a linear decision function 
() signTyx wx b ⎡⎤=+⎣⎦rr r. The wr and b are obtained based on a learning model, e.g., minimax 
probability machine (MPM), which is based on N training samples associated with labels 
{},L
iixRy∈r, where iy is the class label, {}1, 1iy∈+− , and 1iN≤≤. 6  Thesis Organization 
 
Model Problems:
1. Heteroscedastic Problem2. Unimodal Problem3. Class Separation ProblemThe Small Sample 
Size ProblemOverfitting
Supervised Tensor 
Learning
Discriminative Linear 
Subspace MethodDiscriminative Multilinear 
Subspace MethodMaximization of the Geometric Mean 
of all Kullback-Leibler Divergences Linear Discriminant Analysis
General Averaged Divergences 
Analysis1. Support Tensor Machine
2. Tensor Minimax Probability Machine
3. Tensor Fisher Discriminant Analysis4. Multiple Distance Metrics Learning                … … … … … … 
Chapter 2 Chapter 3 Chapter 4
General Tensor 
Discriminant Analysis
Manifold Learning 
using Tensor 
Representations
 
Figure 1.1. The plan of the thesis. 
 
LDA selects subspace to separate di fferent classes in the projected low 
dimensional subspace and it has been widely applied for classification tasks in 
computer vision research. However, LDA has two types of problems: 1) the 
model based problems, which are led by its definition and 2) the small sample size 
(SSS) problem, when the number of training  samples is less than the dimension of 
the feature space. To deal with the model based problems, we develop the maximization of the geometric mean of all Kullback-Leibler divergences under 
the general averaged divergences analysis framework in Chapter 2, as shown in 
Figure 1.1. To deal with the SSS problem, we propose a general tensor 
discriminant analysis (GTDA) for subspace selection based on tensor algebra. GTDA is also extended for popular manifold learning algorithms in Chapter 3, as 
shown in Figure 1.1. The SSS problem is relevant to the overfitting problem in 
vector based learning algorithms for classification. Both problems arise when the 
number of training samples is small. In Chapter 4, we apply tensor algebra to 7 extend vector based learning algorithms to accept tensors as input to reduce the 
overfitting problem. A number of examples are provided in this Chapter, as shown in Figure 1.1. 
In Chapter 2, we review two important linear subspace methods [58], namely 
principal component analysis (PCA) [64] and linear discriminant analysis (LDA) [103]. We then analyze the problems of LDA and review some extensions [19] 
[28][29][38][49][51][55][61][70][95] [96][99][103][123][139][173][174][175][18
0] of LDA to reduce these problems. We also give a new point of view [146] on discriminative subspace selection and develop a new framework for subspace 
selection, the general averaged divergences analysis [146]. This framework allows 
a range of different criteri a for assessing subspaces. Ba sed on the new framework, 
we investigate geometric mean [20] for discriminative subspace selection and develop a method for the maximization of the geometric mean of all Kullback–
Leibler (KL) [21] divergences (MGMKLD)  [146] by combining the maximization 
of the geometric mean of all divergences and the KL divergence for subspace selection. A large number of experime nts are conducted to demonstrate the 
effectiveness of the new discriminative s ubspace selection method compared with 
LDA and its representative extensions. In Chapter 3, we focus on multilinear subspace methods, because the objects in 
computer vision research are often tensor s [75]. Firstly, tensor algebra [115][75] 
is briefly introduced. It is the mathematical fundamental material of this Chapter. After that, unsupervised learning techniques, such as TR1A [132] and GTA [75] [162][171], are reviewed. We also give  a brief introduction of 2LDA [174]. 
Motivated by the success of 2DLDA in face recognition, we then develop GTDA, which includes the following parts: 1) the LDA criterion is replaced by DSDC 
[143][149]. The relationship between LDA a nd DSDC is discussed in detail; and 
2) DSDC is reformulated through opera tions in multilinear algebra [115][75]. 
Based on the reformulated DSDC, vectors can be replaced with tensors, i.e., retaining the original format of samples;  3) an alternating projection optimization 
procedure is developed to obtain th e solution of GTDA; 4) provide the 
mathematical proof of the convergence of  the alternating projection optimization 
procedure for calculating the projection  matrices; and 5) the computational 
complexity is analyzed. Finally, the proposed GTDA combined with LDA and the 
nearest neighbour classifier is utiliz ed for appearance based human gait 8 recognition [45][46][91][92]. Compared with previous algorithms, the newly 
presented algorithms achieve better recognition rates. In Chapter 4, a supervised tensor le arning (STL) framework [149][150] is 
developed based on the simi lar idea to reduce the SSS problem in Chapter 3. We 
first introduce convex optimization [11] and convex optimization based learning. Then we propose the STL framework associated with the alternating projection 
method for optimization. Based on STL and its alternating projection optimization 
algorithm, we generalize the support vect or machine (SVM) [161][15][130][128] 
[137][138], the minimax probability machine (MPM) [74][135], the Fisher 
discriminant analysis (FDA) [37][30] [69], and the distance metric learning 
(DML) [169], as the support tensor machine, the tensor minimax probability machine, the tensor Fisher discriminant analysis, and the multiple distance metrics learning, respectively. We also propose  an iterative feature extraction method 
based on STL. As an example, we develop the tensor rank one discriminant 
analysis (TR1DA). Experiments are conducted based on the tensor minimax probability machine and TR1DA. 
Chapter 5 concludes. 
The main contributions of the thesis are three folds: 
1.
 Develop a discriminative subspace se lection framework, i.e., general 
averaged divergences analysis. Based on this framework, a special case, 
i.e., maximization of the geometric m ean of all Kullback–Leibler (KL) 
divergences, is given to significantly reduce the class separation problem raised by imbalanced distributions of KL divergences between different 
classes. Moreover, it is also compatible with the heteroscedastic property of data and deals with samples drawn from mixture of Gaussians naturally. 
Empirical studies demonstrate that it outperforms LDA and its 
representative extensions; 
2.
 Develop the general tensor discrimina nt analysis (GTDA) to reduce the 
small sample size (SSS) problem. Unlike all existing tensor based 
discriminative subspace selection algorithms, GTDA converges in the 
training stage. Moreover, a full mathemat ical proof is given. To our best 
knowledge, this is the first work in the world to give both a converged 
algorithm and a mathematical proof. Again, this proof can be also applied 
to justify whether a tensor based algorithm converges or not by checking 9 the convexity of its objective function. By applying GTDA to human gait 
recognition, we achieve the state-of-the-art recognition accuracy; and 
3. By applying tensor algebra to vector based learning, we finally develop a 
supervised tensor learning framework. The significance of the framework 
is we can conveniently generalize di fferent vector based classifiers to 
tensor based classifiers to reduce the over–fitting problem. For example, 
we generalize the support vector machine to the support tensor machine 
and give out the error bound; we generalize the Fisher discriminant analysis and the minimax probability machine to the tensor Fisher 
discriminant analysis and the tensor minimax probability machine, 
respectively, to overcome the matrix singular problem; and we generalize distance metric learning to multiple distance metrics learning to make it computable for appearance based recognition tasks. Finally, an iterative 
feature extraction model is given ba sed on supervised tensor learning. 
Empirical studies show the power of supervised tensor learning and the iterative feature extraction model. 
 
 10  Publications 
 
The research of this thesis has result ed in the following research papers: 
1. D. Tao, X. Li, X. Wu, and S. J. Maybank, “General Tensor Discriminant 
Analysis and Gabor Features for Gait Recognition,” IEEE Transactions on 
Pattern Analysis and Machine Intelligence  (TPAMI ), 2007. [Chapter 3] 
2. D. Tao, X. Li, X. Wu, and S. J. Maybank, “Tensor Rank One Discriminant Analysis,” Submitted to IEEE Transactions on Pattern Analysis and Machine 
Intelligence  (
TPAMI ). (Under Major Revision) [Chapter 4] 
3. D. Tao, X. Li, X. Wu, and S. J. Maybank, “General Averaged Divergences 
Analysis,” Submitted to IEEE Transactions on Pattern Analysis and Machine 
Intelligence  (TPAMI ). (Under Major Revision) [Chapter 2] 
4. D. Tao, X. Li, and S. J. Maybank, “Negative Samples Analysis in Relevance 
Feedback,” IEEE Transactions on Knowledge and Data Engineering  
(TKDE ), 2007. 
5. D. Tao, X. Li, W. Hu, S. J. Maybank, and X. Wu, “Supervised Tensor 
Learning: A Framework,” Knowledge and Information Systems  (Springer), 
2007. [Chapter 4] 
6. D. Tao, X. Li, X. Wu, and S. J. Maybank, “Human Carrying Status in Visual 
Surveillance,” IEEE Int’l Conf. on Computer Vision and Pattern Recognition  
(CVPR ), pp. 1,670–1,677, 2006. (Acceptance rate: ~20%) [Chapter 3] 
7. D. Tao, X. Li, X. Wu, and S. J. Maybank, “Elapsed Time in Human Gait Recognition: A New Approach,” IEEE Int’l Conf. on Acoustics, Speech, and 
Signal Processing  (ICASSP), vol. 2, pp. 177–180, 2006. [Chapter 4] 
8.
 D. Tao, X. Li, W. Hu, S. J. Maybank, and X. Wu, “Supervised Tensor 
Learning,” IEEE Int’l Conf. on Data Mining  (ICDM ), pp. 450–457, 2005. 
(Acceptance rate: ~11%) [Chapter 4] 
9. D. Tao, X. Li, W. Hu, and S. J. Maybank, “Stable Third–Order Tensor 
Representation for Color Image Classification,” IEEE Int’l Conf. on Web 
Intelligence  (WI), pp. 641–644, 2005. 
 
 11 2. Discriminative Linear Subspace Methods 
 
The linear subspace method (LSM) [2][21][50][109][107] has been developed and demonstrated to be a powerful tool in  pattern recognition and computer vision 
research fields. LSM finds a matrix 
'LLUR×∈  to transform the high dimensional 
sample LxR∈r to a low dimensional sample 'LyR∈r, i.e., TyU x=rr. There are 
two major categories of LSM algorithms, which are focused on either feature 
selection or dimension reduction, respectively. A feature selection algorithm 
selects a (very small) number of most effective features from the entire feature 
pool. That is the un–selected features are not utilized. In feature selection, the 
linear transformation matrix U has the following properties: 1) the entries of U 
are 1 or 0; 2) the inner product of any two columns of U is 0; and 3) the sum of 
all entries of any column of U is 1. A dimension reduction algorithm finds 
several sets of coefficients, and with each set of coefficients the original features 
are weighted and summed to produce a new fe ature. By this means, several (less 
than the number of the original features) new low dimensional samples are 
“generated” to preserve as much as possible the information (e.g., reconstructive 
information or discriminative information) carried by the original high 
dimensional samples. In this thesis, we focus on algorithms for dimension 
reduction. From the viewpoint of modelling, LSM can be used with a large number of 
models varying from reconstructive models to discriminative models. A 
reconstructive LSM minimizes 
()1N
ii H iLx U y=−∑rr, where we have N training 
samples ixr on hand; the projection matrix is U; iyr is the low dimensional 
representation of ixr; H⋅ is a norm; and ()L⋅ is a loss function [161][128]. 
Principal component analysis (PCA) [64] is an example of a reconstructive model. 
On the other side, discriminative models, e.g., linear discriminant analysis (LDA) 
[39], are utilized for classification. A di scriminative LSM maximizes an objective 
function to separate differe nt classes in the projected low dimensional subspace. 
Both reconstructive and discriminative models are widely used in many real–
world applications, such as biometrics [182][68], bioinformatics [31], and 
multimedia information management [24][139]. 12 In this Chapter, we mainly focus on the discriminative LSM, especially LDA, 
because it is the most popular algorithm in dimension reduction (or subspace selection) for classification. If samples are sampled from Gaussian distributions 
with identical covariance matrices, LDA maximizes the arithmetic mean value of 
the Kullback–Leibler (KL) [21] divergences between different classes. Based on this point of view, it is not difficult to see that LDA has the following problems: 
1) Heteroscedastic problem [29][28][70][61][99]: LDA models different classes 
with identical covariance matrices. Theref ore, it fails to take account of any 
variations in the covariance matrices between different classes; 
2) Multimodal problem [51][28]: In many applications, samples in each class can 
not be approximated by a single Gaussi an. Instead, a Gaussian mixture model 
(GMM) [39][30] is required. However, LDA models each class by a single Gaussian distribution; 
3) Class separation problem [103][95][9 6][99][146]: In appl ications, distances 
between different classes are different and LDA tends to merge classes which are close together in the original feature space. 
The first two problems have been well studied in the past few years and a number 
of extensions of LDA have been generated to deal with them. Although some methods [103][95][96][99] have been propos ed to reduce the third problem, it is 
still not well solved. In this Chapter, to further reduce the class separation 
problem, we first generalize LDA to obtain a general averaged divergences analysis, which extends LDA from two aspects: 1) the KL divergence is replaced by the Bregman divergence [14]; and 2) th e arithmetic mean is replaced by the 
generalized mean function [48]. By choos ing different options in 1) and 2), a 
series of subspace selection algorithms are obtained, with LDA included in as a 
special case. 
Under the general averaged divergences analysis, we investigate the effectiveness 
of geometric mean [20] based subspace se lection in solving the class separation 
problem. The geometric mean amplifies the effects of small divergences and at 
the same time reduces the effects of large divergences. Next, the maximization of 
the geometric mean of the normalized divergences is studied. This turns out not to be suitable for subspace selection, because there exist projection matrices which 
make all divergences very small and at the same time make all normalized 
divergences similar in value. We therefore propose a third criterion, maximization 13 of the geometric mean of all divergences (both the divergences and the 
normalized divergences) or briefly MGMD. It is a combination of the first two. With MGMD, it is possible to develop different subspace selection methods by 
choosing different divergences. In this chapter, we select the KL divergence and 
assume that the samples in each class are obtained by sampling Gaussian distributions. This results in the maximization of a function of all KL divergences 
(MGMKLD). The name MGMKLD is chos en because the function is closely 
related to the geometric mean of divergences. We extend MGMKLD to the case in which the samples in each class are sampled from a Gaussian Mixture Model 
[30]. This gives the multimodal extension of MGMKLD, or M–MGMKLD. 
Finally, we kernelize [104][109][128][129] MGMKLD to the kernel MGMKLD or briefly KMGMKLD. Preliminary expe riments based on synthetic data and 
handwriting digital data [53] show that MGMKLD achieves much better 
classification rates than LDA and its several representative extensions taken from 
the literature. The Chapter is organized as follows. In §53 and §53, PCA with its kernel 
extension and LDA with its representati ve extensions are briefly reviewed, 
respectively. In §53, the general averaged divergences analysis is proposed. In §53, the geometric mean for subspace selection is investigated. The KL 
divergence based geometric mean subspa ce selection is developed in §53. 
synthetic data based experiments, stat istical experiments, and hand writing 
recognition for justifying the effectiveness of linear subspace methods are given in §53, §53, and §53, respectively. Finally, summary of this Chapter is given in 
§53. Moreover, all proofs and deductions in this Chapter are given in §53.  14  Principal Component Analysis 
 
Although PCA [64] is a reconstructive model, it has been successfully applied for 
classification tasks in computer vision . It extracts the principal eigenspace 
associated with a set of training samples L
ixR∈r (1in≤≤ ). Let 
() ( ) ( )11n T
ii iS n xm xm==− −∑rrrr be the covariance matrix, alternatively called the 
total–class scatter matrix, of all training samples ixr, where ()11n
i imn x==∑rr. 
One solves the eigenvalue equation iiuS uλ=rr for eigenvalues 0iλ≥. The 
projection matrix *U is spanned by the first 'L eigenvectors with the largest 
eigenvalues, '
1 *|L
ii Uu∗
=⎡⎤=⎣⎦r. If xr is a new sample, then it is projected to 
() ( )*TyU x m=−rr r. The vector yr is used in place of xr for representation and 
classification. 
PCA has the following properties. In the following description, we assume the 
training samples ixr are centralized, i.e., 0m=r. 
Property 2.1:  PCA maximizes the variance in the projected subspace for a given 
dimension 'L, i.e., 
()2
11arg max tr arg maxn
TT
iFroUU iUS U Uxn==∑r. (2.01)
where Fro⋅ is the Frobenius norm. 
Proof: See Appendix. 
Property 2.2:  The principal eigenspace U in PCA diagonalizes the covariance 
matrix of the training samples. 
Property 2.3:  PCA minimizes the reconstruction error, i.e., 
()2
11arg max tr arg minn
TT
iiFroUU iUS U x U Uxn==−∑rr. (2.02)
Proof: See Appendix. 
Property 2.4:  PCA decorrelates the training samp les in the projected subspace. 
Proof: See Appendix. 
Property 2.5:  PCA maximizes the mutual information between xr and yr on 
Gaussian data. 
Proof: See Appendix. 15 We now study the nonlinear extension of PCA, or the kernel PCA (KPCA) [129], 
which takes high order stastistics of the training samples into account. Consider a nonlinear mapping: 
() :,LHR Rx xφφ→rra , (2.03)
where Hn=. Then, in HR, the covariance matrix is 
()() ()()
11nT
ii
iSx m x mnφφ φφφ
==− −∑rrr r, (2.04)
where () ( )11n
i imn xφ φ==∑r r is the mean vector of all training samples in HR. 
The ()()T
iix xφφrr is a linear operator on the range of φ in HR. Suggested by 
Schölkopf et al. [128][129], the mapping is defined as ()(),ii x xx xφφrrr ra . The 
next step in KPCA is to find the eigenvalue decomposition on Sφ. 
vS vφλ=rr. (2.05)
Because all solutions vr with 0λ≠ are the linear combinations of ()ixφr, 
1in≤≤ , we have 
()() ,,iixvx S vφ λφ φ =rr r r, for all 1in≤≤. (2.06)
Replace vr with ()1n
ii ixαφ=∑r in (2.06), we have 
()()
()() ()
() ()()1
1
11
1,
1
1,
1,n
ji j
j
n
kknnk
jinjk
kk j
kxx
xxn
xnxx xnλα φ φ
φφ
αφ
φφ φ=
=
==
=⎛⎞−⎜⎟⎝⎠=
⎛⎞−⎜⎟⎝⎠∑
∑
∑∑
∑rr
rr
r
rr r (2.07)
for all 1in≤≤ . 
In terms of the nn× kernel Gram matrix [128][129] ()() :,ij i jKx xφφ==rr 
(),ijkxxrr, (2.07) is simplified as 
() ()T
ii iK KI M I M Kλαα=− −r r (2.08)
where iαr is a column vector and it is the eigenvectors of () ()TIMIM K−− ; 
iλ is the eigenvalues of K; ()()() ,,iikxz x z φφ=r rr r is the kernel function 
[128][129]; and all entries in 
1;nn
ijij nM mR×
≤≤⎡⎤=∈⎣⎦ are 1n. The projection 16 matrix Λ in HR is spanned by the first ( ) ''HH H n<= iαr with the largest 
eigenvalues, i.e., []1i inα≤≤Λ=r. 
After obtaining the linear combination coefficients, we can project a given sample 
zr to the subspace constructed by KPCA according to 
()() ()
() () ()12,, ,,, , ,TTT
T
nXz X z
kxz kx z kx zφφφφΛ= Λ
=Λ⎡⎤⎣⎦rr
rr rrr rL (2.09)
where ()1 []ii n Xxφφ≤≤=r. 
 17  Linear Discriminant Analysis 
 
LDA [103] finds in the feature space a low dimensional subspace where the 
different classes of samples remain we ll separated after projection to this 
subspace. The subspace is spanned by a set of vectors, which are denoted as 
[]1',,L Uu u=rrK . It is assumed that a training set of samples is available. The 
training set is divided into c classes. The ith class contains in samples 
;ijxr(1ijn≤≤ ), and has a mean value ();11in
ii i j jmn x==∑r r. The between class 
scatter matrix bS and the within class scatter matrix wS are defined by 
() ()
() ()1
;;
111     
1icT
bi i i
i
n cT
wi j i i j i
ijS n mm mmn
Sx m x mn=
==⎧=− −⎪⎪⎨
⎪=− −⎪⎩∑
∑∑rrrr
rrr r (2.10)
where c is the number of classes; 1c
i inn==∑  is the size of the training set; and 
(); 111i cn
ij ijmn x===∑∑rr is the mean vector of all training samples. Meanwhile, 
() ()() ;; 111iT cn
ti j i j ijS n xm xm===− −∑∑rr rr
bwSS=+  is the covariance matrix of all 
samples. 
The projection matrix *U of LDA is chosen to maximize the ratio between bS 
and wS in the projected subspace, i.e., 
()() ( )1*a r g m a x t rTT
wb
UUU S U U S U−= . (2.11)
The projection matrix *U is computed from the eigenvectors of 1
wbSS−, under 
the assumption that wS is invertible. If c equals to 2, LDA reduces to the Fisher 
discriminant analysis [37]; otherwise LDA is known as the Rao discriminant 
analysis [122]. Because () rank 1bSc≤−, we have '1Lc≤−, i.e., the maximum 
dimension of the projected subspace for LDA is () min 1, 1 cL−−. 
If separate classes are sampled from Gaussian distributions, all with identical 
covariance matrices, then LDA maximizes the mean value of the KL divergences between different classes. This result will be proved in §122. 
LDA encounters the following problems, which are: 18 1) Heteroscedastic problem [29][28][70][61][99]: LDA discards the 
discriminative information preserved in co variance matrices of different classes; 
2) Multimodal problem [51][28]: LDA mode ls each class by a single Gaussian 
distribution, so it cannot find a suitable pr ojection for classification when samples 
are sampled from complex distributions, e.g., GMM; 3) Class separation problem [103][95][96 ][99][146]: LDA tends to merge classes 
which are close together in the original feature space. 
Furthermore, when the size of the training set is smaller than the dimension of the feature space, LDA has the small sample  size (SSS) problem [19][30][38][39] 
[49][55][123][139][173][174][175][180]. 
In the following, we review some representative solutions for these problems. Furthermore, we mention some alternatives of LDA [39], namely the nonparametric discriminant analysis [39] [40], and the kernel extension of LDA 
[104][109] [128][129]. 
 
 Heteroscedastic Problem 
LDA does not fully utilize the discriminative information contained in the 
covariances of different classes. As a re sult, it cannot find a suitable projection 
direction when different classes share the same mean, as shown in Figure 2.1. In the past decades, a large number of extensions based on LDA were developed 
to reduce this problem. For example, 
• Decell and Mayekar [29] proposed a method to obtain a subspace to 
maximize the average interclass divergence, which measures the separations between the classes. This  criterion takes into account the 
discriminative information preserve d in the covariances of different 
classes. The projection matrix 
*U is calculated by maximizing, 
() (
() () ))()1
11 ;tr
              1 ',cc
TT
Di j
ij j i
T
ji jiJU S U U S
mmmm U c c L−
== ≠⎡ ⎛=⎢ ⎜⎢ ⎝ ⎣
⎤+− − −−⎥⎦∑∑
rrrr (2.12)
where iS is the ith class covariance matrix ( 1ic≤≤); imr is the mean 
vector of the samples in the ith class (1 ic≤≤); c is the number of classes 19 of the training set; 'L is the number of selected features; and U is the 
projection matrix to obtain the low dimensional representation. 
De la Torre and Kanade [28] developed the oriented discriminant analysis (ODA) based on the objective function de fined in (2.12), but used iterative 
majorization to obtain a solution. The iterative majorization speeds up the 
training stage. 
 
Optimal Projection Direct ion for ClassificationLDA
 
Figure 2.1. LDA fails to find the optimal projection direction for classification, 
because it does not utilize the discriminative information preserved in the class 
covariances.  
• Campbell [17] has shown LDA is related to the maximum likelihood 
estimation of parameters for a Gaussian model based on the following two 
assumptions: 1) all class discrimina tive information resides in a low 
dimensional subspace of the original high dimensiaon feature space and 2) 
the within class covariances are identical for all classes. Kumar and 
Andreou [70] developed the heteroscedastic discriminant analysis (HDA) 20 by dropping the identical class covariances assumption. The projection 
matrix *U is calculated by maximizing, 
'' ' '
1log log 2 logc
TT
KL L L L i L i L
iJ nU S U n U S U nU−−
==+− ∑ , (2.13)
where []''LL L UU U− =  is the full transformation matrix; 'LU is the 
transformation submatrix to sele ct the discriminative subspace; 
()det⋅⋅  ; iS is the ith class covariance matrix ( 1ic≤≤ ); S is the 
covariance matrix of all training samples; c is the number of classes of 
the training set; and 'L is the number of selected features. Furthermore, 
the projection matrix *U is obtained by maximizing KJ through the 
gradient steepest ascent algorithm. 
• Jelinek [61] proposed a different way to deal with the heteroscedastic 
problem in subspace selection by the gradient steepest ascent method to find the projection matrix 
*U by maximizing, 
1log logc
TT
Jb i i
iJ nU S U n U S U
==− ∑ , (2.14)
where () det⋅⋅  ; iS is the covariance matrix of the ith class; bS is the 
between class scatter matrix defined in (2.10); in is the number of 
samples in the ith class ( 1ic≤≤); c is the number of classes of the 
training set; and U is the projection matrix to obtain the low dimensional 
representation. 
• Loog and Duin [99] introduced the Chernoff criterion to heteroscedasticize 
LDA, i.e., the heteroscedastic extension of LDA (HLDA). The projection 
matrix *U in HLDA is obtained by maximizing,  
() () () (
() ()(
() ()))11211 2 1 2 1 2 1 2
11
1212 12 12 12 12
12 12 12 12 121                  log
                  log log ,ccT
L i j w w w i j w w ij ij
ij i
w w ij w w ij w
ij
iw i w jw j w wJq q S S S S S S m m m m
SS S S S S S
SS S SS S Sππ
ππ−−−− −
== +
−−− −
−−=× × − −
×+
−−∑∑rrrr
 (2.15)
where 1c
ii k kqn n==∑  is the prior probability of the ith class; iS is the 
covariance matrix of the ith class; () ii ijqqqπ=+ ; () jj i jqq qπ=+ ; 
ij i i j jSS Sππ=+ ; wS is the within class scatter matrix defined in (2.10); 21 in is the number of samples in the ith class ( 1ic≤≤); and c is the 
number of classes of the training set. 
The projection matrix *U of HLDA is constructed by the eigenvectors of 
the LJ corresponding to the largest eigenvalues. 
 Multimodal problem 
The direct way to deal with the multimodal problem is to model each class by a 
GMM [39][30]. Two representative works are as following, 
• Hastie and Tibshirani [51] combined GMM with LDA based on the fact 
that LDA is equivalent to maximum likelihood classification when each 
class is modelled by a single Gaussian distribution. The extension directly 
replaces the original single Gaussian in each class by a Gaussian mixture 
model in Campbell’s result [17], as shown in (2.13). 
• De la Torre and Kanade [28] generalized ODA defined in (2.12) for the 
multimodal case as the multimodal ODA (MODA) by combining it with 
GMM learnt by the normalized cut [131][177]. Each class is modelled by 
a GMM. The aim of MODA is to find a projection matrix *U to 
maximize 
()
() ()( )1
;
111 1;; ;; ;trj iTTc c cc ik
MODA T
ij k lik jl ik jl jljiUSU U
J
mmmm S U−
====
≠⎛⎞
⎜⎟=⎜⎟×− −+⎜⎟⎝⎠∑∑∑∑ rrrr, (2.16)
where ic is the number of subclusters of the ith class; ;ikS is the 
covariance matrix of the kth subcluster of the ith class; ;ikmr is the mean 
vector of the kth subcluster of the ith class; c is the number of classes of 
the training set; and U is the projection matrix to obtain the low 
dimensional representation. 
 
 Class separation problem 
One of the most severe problems in LD A is the class separation problem, i.e., 
LDA merges classes which are close together in the original feature space. As pointed out by McLachlan in [103], Lotlikar and Kothari in [95], Loog et al. in [98], and Lu et al. in [100], this merging of classes significantly reduces the 
recognition rate. The example in Figure 2.2 shows that LDA is not always optimal 22 for pattern classification. To improve its performance, Lotlikar and Kothari in 
[95] developed the fractional–step LDA (FS–LDA) by introducing a weighting function. Loog et al. in [98] developed another weighting method for LDA, 
namely the approximate pairwise accuracy criterion (aPAC). The advantage of 
aPAC is that the projection matrix can be obtained by the eigenvalue decomposition. Lu et al. in [100] combined the FS–LDA and the direct LDA 
[175] for very high dimensional problems, such as face recognition [182]. 
However, both FS–LDA and aPAC do not use the discriminative information in different class covariances. Therefore, when samples are drawn from Gaussians 
with different covariances, these two methods fail to detect the suitable subspace 
for classification (their performance could be even worse than LDA). The detailed procedures for FS–LDA and aPAC are as follows,  
LDA Projection Direction
Class 1
Class 2 Class 3Optimal Projection Direction
 
Figure 2.2. The samples in each class are drawn from a Gaussian distribution. 
LDA finds a projection direction, which merges class 1 and class 2. One of the 
reasonable projection directions for classification trades the distance between the 
class 1 and the class 2 against the distance between the class 1, 2 and class 3. This example is a sketch of the synthetic data used in Figure 2.7 in §0. 
 23 • Lotlikar and Kothari [95] developed FS–LDA to reduce the class 
separation  problem. They found this problem is invoked by non–uniform 
distances between classes, i.e., some distances between different classes are small while others are large. Ther efore, a weighting method is used to 
reduce this problem. FS–LDA is an iterative procedure for subspace 
selection as shown in Table 2.1. 
 
Table 2.1. Fractional–Step Linear Discriminant Analysis 
Input: Training samples ;ijxr in LR, where i denotes the ith class ( 1ic≤≤ ) and 
j denotes the jth sample in the ith class ( 1ijn≤≤), and the dimension 'L 
('LL<) of the projected subapce. 
Output: Linear projection matrix *U in 'LLR×. 
1. Set LL UI×=  (the identity matrix) 
2. Calculate the mean of the ith class ();11in
ii i j jmn x==∑r r. 
3. for kL= to ()'1L+ with step 1− { 
4. for 0l= to ()1r− with step 1 { 
5. Calculate ;;lT
ij ijyA U x=r r, where A is a diagonal matrix. The kth 
entry of A is less than 1 and the other entries are 1; 
6. Calculate (); 11in
ii i j jnyμ==∑r r in the projected subspace; 
7. Calculate ()()()()()111T cc ij
bi j i j ijSn w d μμμμ===− −∑∑rrrr, where 
()w⋅ is the weighting function and ()pij
ij dm m=−rr. Usually, 
()w⋅ is set as a polynomial like function with degree less than 3− 
(in our experiments, we set it as 8−). 
8. Compute the first k eigenvectors [ ]12,,k ϕϕϕΨ=rrrL  of bS 
associated with the largest k eigenvalues. Set UU←Ψ. 
9. }// for in line 4. 
10. Discard the last column of U. 
11. }// for in line 3. 
 
• Loog et al. [98][96] proposed another way to deal with the class 
separation  problem by combining a weighting function with LDA and 
then resulted in aPAC, 24 ()() ()12 12
11ccT
aPAC i j ij w i j i j w
ijJ q q dS mm mm Sω−−
===− −∑∑rrrr, (2.17)
where () ()1T
ij i j w i jdm m S m m−=− −rrr r; 1c
ii k kqn n==∑  is the prior 
probability of the ith class; wS is the within class scatter matrix defined in 
(2.10); imr is the mean vector of samples in the ith class; in is the 
number of samples in the ith class ( 1ic≤≤); c is the number of classes 
of the training set; ()()()212 e r f 2 2 xx xω=  is the weighting function. 
The projection matrix is 12*w US−=Φ, where Φ is the matrix of leading 
eigenvectors of aPACJ  with the largest eigenvalues. 
This combination aPACJ  approximates the mean classification accuracy 
(one minus Bayes error). The benefit of this method is the projection 
matrix can be obtained by the eigenvalue decomposition. 
 
The weighting function based methods are not effective for the class separation 
problem, because it is not clear how to select an optimal weighting function. 
Although Loog et al. [98][96] considered the Bayesian error in aPAC, it fails to deal with the heteroscedastic problem. Even for the homoscedastic case, aPAC is 
also not optimal, because it only approximats to the Bayesian error. The synthetic 
data based test, shown in Figure 2.7 and Figure 2.8, demonstrates that FS–LDA and aPAC do not often find the optimal pr ojection direction for classification. 
 
 Small Sample Size Problem 
In many practical applications, especially in biometric research, discriminant models encounter the SSS problem [3 8][49][139][123][19][175][55][173][174] 
[180], because the number of training samples is less than the dimension of the 
feature space. To deal with this problem, a number of algorithms were proposed. 
For example, 
• Friedman [38] proposed the regular ized discriminant analysis (RDA), 
which is a classification tool to smooth out the effects of ill– or poorly– conditioned covariance estimates due to the lack of training samples. RDA 
is a combination of ridge–shrinkage [20], LDA, and quadratic discriminant 25 analysis (QDA) [30][39]. It provide s a great number of regularization 
alternatives. In RDA, the regularized class covariance matrix is defined as 
() ()()
()()
()11,1 t r11ii
i
iiSS SSSInn p nnλλ λλ γλγ γλλ λλ⎛⎞ −+ −+=− + ⎜⎟⎜⎟ −+ −+⎝⎠, (2.18)
where iS is the covariance matrix of the ith class; S is the covariance 
matrix of all samples; I is the identity matrix in ppR×; in is the 
number of samples of the ith class; n is the number of all samples; p is 
the dimension of the original high  dimensional feature space; and 
0,1λγ≤≤  are regularization parameters, which are chosen to jointly 
minimize an unbiased estimate of futu re misclassification risk through the 
cross validation [72] or Boostrapping [ 33]. In RDA, the class discriminant 
score for classification is 
() ( ) () ( ) ()1,l o g , 2 l o gTi
ii i i indx xm S xm Snλγ λγ−=− − + −rr r r r, (2.19)
where () det⋅⋅   and imr is the mean vector of samples in the ith class. 
The class label of xr is () arg mini
idxr. 
Furthermore, RDA can also be used for subspace selection by introducing 
the reduced rank step for the sum of ()idxr over all training samples as 
() ()() ()
()1
;;
11,
                  log , ,in cTTT
R ij i i ij i
ij
T
iJ xm U U S U U xm
US Uλγ
λγ−
===− −
+∑∑rr rr
 (2.20)
where () det⋅⋅  . The projection matrix is obtained by minimizing RJ 
over U. When ()() ,,ijSSλγλ γ=  for all ij≠, RDA for subspace 
selection reduces to the regularized LDA (R–LDA). 
• Hastie et al. [49] viewed LDA as multivariate linear regression [106] and 
used the penalized least squares re gression [106] to reduce the SSS 
problem. 
• Swets and Weng [139] introduced PCA as the pre–processing step in LDA 
for face recognition (PCA+LDA). The SSS problem is avoided if the PCA subspace has a small enough dimensi on. PCA+LDA is one of the most 
popular methods to deal with the SSS problem in biometric research. It has 
been being effective in many empirical demonstrations [139][4][89]. It is 26 also easy to be implemented as there are only two main steps: 1) conduct 
PCA on training samples and 2) conduct LDA on the PCA pre–processed data. PCA+LDA achieves top level performance in many applications, 
such as face recognition [4][88][89], gait recognition [45][46][92], and 
image retrieval [139]. The drawbacks [19][175] of this method are that the classification performance is sensitive to the number of features selected 
by PCA and PCA discards some discriminative information. 
•
 Raudys and Duin [123] applied the ps eudo–inverse [44] to the covariance 
matrix in LDA as P–LDA and this utilization can reduce the SSS problem in LDA. The projection matrix is obtained by maximizing 
()() ( ) trTT
Pw bJ US U US U+= , (2.21)
where A+ means the pseudo–inversion of A, as defined in [44]. 
• Chen et al. [19] claimed that the most discriminative dimensions are 
preserved in the null space V (the complement of the range space, i.e., 
0wSV=) of the within class scatter matrix wS when the dimension of the 
feature space is much higher than the number of training samples. First, 
they project all training samples to the null space of wS. Second, PCA is 
utilized on the projected samples to select the projection direction for 
classification. To reduce the time complexity of this method, Cevikalp et al. [18] developed the discriminative common vectors scheme. 
•
 Yu and Yang [175] claimed that the null space V (or the complement of 
the range space, i.e., 0bSV=) of the between class scatter matrix bS 
contains little discriminative information. Therefore, they first remove the 
null space of bS and then select the projection matrix to minimize the 
within class scatter matrix wS. 
• Howland and Park [55] introduced the generalized singular value 
decomposition (GSVD) [44] to reduce the SSS problem in LDA. The 
detailed procedure is listed in Table 2.2. 
 
Table 2.2.  
Linear Discriminant Analysis via Generalized Singular Value Decomposition 
Input: Training samples ;ijxr in LR, where i denotes the ith class ( 1ic≤≤ ) and 27 j denotes the jth sample in the ith class ( 1ijn≤≤), and the dimension 'L 
('LL<) of the projected subapce. 
Output: Linear projection matrix *U in 'LLR×. 
1. Calculate bH and wH according to 
()Lc
bi iHn m m R×⎡⎤=− ∈⎣⎦rr and () ;Ln
bi j iHx m R×⎡⎤=− ∈⎣⎦r, where i and j 
vary over all classes and all samples in each class, separately. 
2. Computer the complete orthogonal decomposition 
0
00T RPK Q⎡⎤=⎢⎥⎣⎦, where ()T
cn L b
T
wHKR
H+× ⎡⎤=∈⎢⎥
⎣⎦ and the rank of K is t. 
3. Conduct the singular value decomposition on () 1: ,1:P ct , i.e., 
() 1: ,1:TUP c tV =Λ. 
4. Let 10
0RVAQ
I−⎛⎞=⎜⎟
⎝⎠. The projection matrix *U is the first 'L 
() '1Lc≤−  columns of A. 
 
• Ye and Li [174] combined the orthogonal triangular decomposition [44], 
or briefly the QR decomposition, with LDA to reduce the SSS problem. 
Compared with LDA/GSVD, LDA/QR has lower time and space 
complexities. The only difference between PCA+LDA and LDA/QR is the 
first stage: PCA+LDA applies PCA to the covariance matrix of total 
training samples, while LDA/QR conducts the QR decomposition on a 
small matrix involving the class means. The detailed description of LDA/QR is given in Table 2.3. 
•
 Recently, Zhang et al. [180] reformulated LDA based on the statistical 
learning theory by defining the following regularized function to reduce the SSS problem: 
()2
;;
111*a r g m a xin c
TT
ij ij
u ijuy u x u unλ
==⎛⎞=− +⎜⎟
⎝⎠∑∑rrr r r r, (2.22)
where 2c=; ;ijy is the label identifying the class of the sample ;ijxr; λ 
is a small value; and Tuurr is the regularization term. Zhang et al. then 
extended (2.22) to multiclass problems and proved the equivalence 
between (2.22) and the regularized version of LDA and reported (2.22) can 28 reduce the SSS problem. This extension has also been recognized by 
Gallinari et al. [42] and Hastie et al. [49]. 
 
Table 2.3. Linear Discriminant  Analysis via QR Decomposition 
Input: Training samples ;ijxr in LR, where i denotes the ith class ( 1ic≤≤ ) and 
j denotes the jth sample in the ith class ( 1ijn≤≤), and the dimension ' L 
('LL<) of the projected subapce. 
Output: Linear projection matrix *U in 'LLR×. 
1. Calculate bH and wH according to 
()Lc
bi iHn m m R×⎡⎤=− ∈⎣⎦rr and () ;Ln
bi j iHx m R×⎡⎤=− ∈⎣⎦r, where i and j 
vary over all classes and all samples in each class, separately. 
2. Apply QR decomposition on bH as bHQ R E= , where Q, R, and E 
are in LtR×, tcR×, and ccR×, respectively. Here t is the rank of bH. 
3. Let T
bSR R=  and TT
ww wSQ H H Q= . Let []1i itφ≤≤Φ=  be the first t 
eigenvectors of 1
bwSS− associated with the first t smallest eigenvalues. 
4. The projection matrix *U is defined by *UQ=Φ. 
 
 Several LDA Alternatives 
Several alternatives to LDA are obtained by varying the objective function 
defined in (2.11) for LDA. That is the projection matrix *U can be obtained by 
maximizing the following criteria, 
()() ( )1
12 1trTTJ US U US U−=  (2.23)
21 2log logTTJ US U US U=−  (2.24)
()() 31 2tr trTTJ US U US Uλ =−  (2.25)
()() 41 2tr trTTJ US U US U=  (2.26)
where () det⋅⋅  ; the pair {}12,SS  could be {},bwSS , {},btSS , or {},twSS ; 
and λ in 3J is a  Lagrange multiplier. The criterion 1J is the traditional 
definition of LDA as shown in (2.11). The maximization of 1J is equivalent to 
the maximization of () 1 trTUS U  with the constraint 2TUS U I=, where I is 
the identity matrix. In 2J, when T
bUS U  is not full rank, we cannot set  1S 29 equal to bS, due to () det 0bS=. The maximization of 2J is equal to the 
maximization of 1J as shown in [39]. 
 
 Nonparametric Model b ased Discriminant Analysis 
In this Section, we review some important nonparametric based discriminant 
analysis algorithms. 
• Hastie et al. [52] viewed LDA as a multivariate linear regression and 
generalized LDA by replacing the multivariate linear regression with a 
multivariate nonparametric regression. They named the nonparametric 
generalization of LDA as flexible discriminant analysis. Furthermore, they also [50] introduced GMM to the flexible discriminant analysis to model 
complex distributions. 
•
 Fukunaga and Mantock [40] extended the between class scatter matrix bS 
and the within class scatter matrix wS from the parametric version to the 
nonparametric version. With this extension, LDA is generalized to the 
nonparametric discriminant analysis (NDA). Using this generalization, 
more features can be selected for classification when 1c− features are 
not enough. Meanwhile, the assumption that each class has a Gaussian 
distribution is dropped. 
• Buturovic [16] used the k–nearest –neighbour based Bayes error 
minimization criterion to select the discriminative subspace. Lotlikar and 
Kothari [94] minimized the Bayes error in the projected subspace when 
each class is modelled by a hyper sphere and each class has its own mean. 
By modelling each class density function via a kernel estimator, this 
method can be utilized for practical a pplications. Liu et al. [90] used a 
stochastic gradient algorithm to obtain linear representations for classification based on a new defined opt imization criterion which utilizes 
the between class distance and within class distance information through a 
nonparametric way.  
 
 Kernel Discriminant Analysis 30 The nonlinear extension of LDA, or the kernel LDA (KDA) [109][104][129], is to 
solve the generalized eigenvalue decomposition 
()() ( )1*a r g m a x t rTT
tb
UUU S U U S U
φφφ
φφ φ φ φ−=  (2.27)
in the higher dimensional space, where the between class scatter matrix bS and 
the total class scatter matrix tS in the higher dimensional space are defined as 
() ()
()() ()(),,
1
;;
111          
1icT
bi i i
i
n cT
ti j i j
ijSn m m m mn
Sx m x mnφ
φφ φφ
φ
φφ φφ=
==⎧=− −⎪⎪⎨
⎪=− −⎪⎩∑
∑∑rr rr
rr rr (2.28)
where () (); 111i cn
ij ijmn xφ φ===∑∑r r, () () ;; 11in
ii i j jmn xφ φ==∑r r, and 1c
i inn==∑ . 
Similar to KPCA, each column in Uφ is also a linear combination of all training 
samples in the higher dimensional space, i.e., UXφφ=Λ, where Λ is a matrix 
to store linear combination coefficients as it is defined in KPCA and Xφ is 
defined as ()() 1;1 1;2,, , Xx xφφφ⎡=⎣rrL ()() 2;1 ;,,
ccn xxφφ ⎤⎦r rL . 
By replacing Uφ in ()() ( )1*a r g m a x t rTT
tb
UUU S U U S U
φφφ
φφ φ φ φ−=  with XφΛ, we 
have 
()() ( )1*a r g m a x t rTT
tbKK−
ΛΛ= Λ Λ Λ Λ  (2.29)
where bK and tK are defined as 
() ()T
bK KW M W M K=− −  (2.30)
() ()T
tK KI M I M K=− − . (2.31)
where ()()() ,,nn
ij i j K kxx x x R φφ× ⎡⎤ ⎡⎤== ∈⎣⎦ ⎣⎦rr r r is the kernel Gram matrix; 
(),k⋅⋅ is a kernel function [128][129]; []1nn
l lcWW R×
≤≤=∈  is a block diagonal 
matrix; all entries in iinn
lWR×∈  are 1in; and all entries in 
1;nn
ijij nM mR×
≤≤⎡⎤=∈⎣⎦ are 1n. 
After obtaining the linear combination coefficients, we can project a given sample 
zr to the subspace constructed by KDA through 31 ()() ()
() () () 1;1 1;2 ;,, ,,, , .
cTTT
T
cnXz X z
kx z kx z kx zφφφφΛ= Λ
⎡⎤=Λ⎣⎦rr
rr rrr rL (2.32)
where ()1 []ii n Xxφφ≤≤=r. 
 32  General Averaged Di vergences Analysis 
 
If different classes are assumed to be sampled from Gaussian densities with 
different expected values but identical  covariances, then LDA maximizes the 
mean value of the KL divergences between different pairs of densities. We 
propose a framework, the General Averaged Divergences Analysis , for choosing a 
discriminative subspace by: 1) generali zing the distortion measure from the KL 
divergence to the Bregman divergence2, and 2) generalizing the arithmetic mean 
to the generalized mean function [48]. Ba sed on this framework, we can develop a 
method to reduce the heteroscedastic problem, the unimodel problem, and the class separation problem, simultaneously. 
 
 Bregman Divergence 
Definition 2.1 (Bregman Divergence):  Let :mapUS R→  be a 1C convex 
function defined on a closed convex set SR+⊆ . The first derivative of mapU  is 
mapU′, which is a monotonic function. The inverse function of mapU′ is 
()1
mapUξ−′= . The probability density for the samples in ith class is 
()() |ipxp x y i==rr, where yi= means the sample xr is sampled from the ith 
class. The difference, as shown in Figure 2.3, at ()() jpxξr between the function 
mapU  and the tangent line to mapU  at ()() ()()() ( ) ,im a p ipx U pxξξr r is given by: 
()()()() ( ) ()()() ()()() { }
() ()() ()() {},
                                         .i j map j map i
ij idp x p x U p x U p x
px p x pxξξ ξ ξ
ξξ=−
−−rr r r
rr r (2.33)
Based on (2.33), the Bregman divergence  for ()ipxr and ()jpxr is 
() ()( ) ()()()() ( ) || ,ij i jDp x p x d p x p x d ξ ξμ =∫rr r r, (2.34)
                                                 
2 There are many distortion measures, such as Bregman divergence, Amari’s α–divergence, and 
Csiszar’s ϕ–divergence. The Bregman divergence is selected as the distortion measure in this 
thesis. 33 where dμ (i.e., () dxμr) is the Lebesgue measure. The right–hand side of (2.34) 
is also called the U–divergence  [110]. Because mapU  is a convex function, 
()()()() ( ) , dp x q xξξrr is non–negative. Consequently, Bregman divergence is 
non–negative. Because ()()()() ( ) , dp x q xξξr r is in general not symmetric, 
Bregman divergence is also not symmetric. Detailed information about the 
Bregman divergence can be found in [110]. 
()()() i Up xξr
()() ipxξr()() jpxξr()()() j Up xξr
()()()()( ) ,ij dp x p xξξr r
() ()()()() { } ()()() ij i jpxp x p x U p xξξ ξ −+r rr r
ξ()Uξ
 
Figure 2.3. The geometric setting for Bregman divergence. 
 
If ()() expmapUx x= , Bregman divergence reduces to the KL–divergence, 
() ()() () () ()()
()
()()
()() ()()|| log
                            log || .j
ij jii
i
i
ii j
jpxD p xp x p xp xp x dpx
pxpxd K L p x p xpxμ
μ⎛⎞=− −⎜⎟⎜⎟⎝⎠
==∫
∫rrr rrrr
rr rrr (2.35)
Further examples can be found in [110]. 
For Gaussian probability density functions, ()() ;,ii ipx N x m Σrrr  , where imr is 
the mean vector of the ith class samples and iΣ is the within class covariance 
matrix of the ith  class, the KL divergence [21] is 34 () ()() ()()
()
() ( )11;,|| ; , ln
;,
                              ln ln tr tr ,ii
ij i i
jj
ji j i j i jNx mKL p x p x N x m dx
Nx m
D−−Σ=Σ
Σ
=Σ −Σ +Σ Σ +Σ∫rrrr r r r
rr (2.36)
where ()() ij i j i jDm m m m=−⊗−rr rr and () detΣΣ  . 
To simplify the notation we denote the KL divergence between the projected 
densities () |TpU x y i=r and () |TpU x y j=r by 
()()() ( ) || | || |TT
Ui jDpp D p U x y i p U x yj ==r r  . (2.37)
 
 General Averaged Divergences Analysis 
We replace the arithmetic mean by the generalized mean [48], 
()()()
11
1||ij U i j
ij c
mn
mncqq D p p
VUqqϕϕ
ϕ≤≠≤−
≤≠≤⎡ ⎤
⎢ ⎥=⎢ ⎥
⎢ ⎥⎣ ⎦∑
∑, (2.38)
where ()ϕ⋅ is a strict monotonic real–valued increasing function defined on 
()0,+∞; ()1ϕ−⋅ is the inverse function of ()ϕ⋅; iq is the prior probability of 
the ith class (usually, we set iiqn n=  or simply set 1iqc= ); ()||Ui jDpp  is 
defined in (2.37); LxR∈r where LR is the feature space containing the training 
samples; and 'LLUR×∈  ('LL>) is the projection matrix. The general averaged 
divergences function measures the averag e of all divergences between pairs of 
classes in the subspace. We obtain the projection matrix *U by maximizing the 
general averaged divergences function () VUϕ  over U, for a fixed ()ϕ⋅. The 
general optimization algorithm for subspace  selection based on (2.38) is given in 
Table 2.4. Usually, the concavity of  the averaged divergences cannot be 
guaranteed. To reduce the effects of loca l maxima [11], we choose a number of 
different initial projection matrices; carry out the separate optimizations; and then 
select the best one, which has the maximal value of () VUϕ . 
If ()VUϕ  depends only on the subspace of nR spanned by the columns of U 
then U can be replaced by UC where C is a kk× matrix, chosen such that 
the columns of UC are orthogonal. 35 On setting ()x xϕ=, we obtain the arithmetic mean based method for choosing a 
subspace, 
()
1
1||
*a r g m a xijU i j
U ij c mn
mncqq D p p
Uqq ≤≠≤
≤≠≤=∑∑()
1arg max ||ijU i j
U ij cqq D p p
≤≠≤=∑ . (2.39)
 
 
Table 2.4. General Averaged Divergences Analysis for Subspace Selection 
Input:  Training samples ;ijxr in LR, where i denotes the ith class ( 1ic≤≤ ) 
and j denotes the jth sample in the ith class ( 1ijn≤≤), the dimension 'L
('LL<) of the projected subspace, and M is the maximum number of different 
initial  values for the projection matrix. 
Output:  Optimal linear projection matrix *U in 'LLR×. 
1. for 1:mM=  { 
2. Randomly initialize m
tU ( 1t=), i.e., all entries of 1mU are random 
numbers. 
3. while  ()()1mm
tt VU VUϕϕ ε−− > (610ε−= ), do{ 
4. Conduct the gradient steepest ascent algorithm3 to maximize the 
averaged divergences defined in (2.38): 
() 11mm m
tt U tUU V Uϕκ−−←+ ⋅ ∂ . Here, κ, a small value (e.g., 0.0001), 
is the learning rate. 
5. 1 tt←+  
6. }// while  in line 3 
7. }// for in line 1 
8. () *a r g m a xm
t
mUV Uϕ ← . 
 
Observation 2.1:  LDA maximizes the arithmetic mean of the KL divergences 
between all pairs of classes, under the as sumption that the Gaussian distributions 
()ipxr for different classes al l have the same covariance matrix. The optimal 
                                                 
3 The gradient steepest ascent algorithm can be replaced by other faster optimization methods, 
such as the conjugate gradient method, to reduce the number of iterations. 36 projection matrix U with respect to LDA can be obtained by maximizing a 
particular ()VUϕ , i.e., 
() ()( )1
1arg max || arg max trTT
ijU i j w b
UU ij cqq D p p U S U U SU−
≤≠≤= ∑ . (2.40)
Proof: See Appendix. 
Example:  Decell and Mayekar [29] maximized the weighted arithmetic mean of 
all symmetric KL divergences between a ll pairs of classes in the projected 
subspace. The weighting factor of the symmetric KL divergence between the ith 
class and the jth class is ijqq, where iq is the prior probability of the ith 
class.The symmetric KL divergence is: 
() () ()
() ( ) () ()( )11 1 111|| || ||22
tr tr .ij ij j i
T
ji i j j i i j i jS K Lp p K Lp p K Lp p
mm mm−− − −=+
=Σ Σ + Σ Σ + Σ + Σ − −rrrr (2.41)
It follows from (2.41) that Decell and Mayekar’s method also maximizes the 
arithmetic mean of all KL divergences. De la Torre and Kanade [28] developed ODA based on the objective function 
(2.12) used in [29], but using the it erative majorization to quickly obtain a 
solution.  
 How to Deal with the Multimodal Problem [50] 
Up to this point it has been assumed that the samples in a given class are sampled 
from a single Gaussian distribution. This assumption often fails in real–world large data sets, such as those used for multi–view face [84] and gait [176] recognition, natural image classification [156] or texture classification [133]. 
To overcome this limitation, each class can be modelled by a GMM. Many 
methods for obtaining GMMs have been de scribed in the literature. Examples 
include KMeans [30], GMM with expectat ion–maximization (EM) [30], graph–
cut [131], and spectrum clustering. Unfortunately, these methods are not adaptive, 
in that the number of subclusters must be specified, and some of them (e.g., EM and KMeans) are sensitive to initial values. In our algorithm we use the recently 
introduced GMM–EM like algorithm, proposed by Figueiredo and Jain [36], 
which is named the GMM–FJ method. The reasons for choosing GMM–FJ are: it finds the number of subclusters; it is less sensitive to the choice of initial values of 37 parameters than EM; and it avoids the boundary problem4 of the parameter space. 
We assume that samples in each class are sampled from a GMM and the 
projection matrix U can be obtained by maximizing the general averaged 
divergences, which measure the averaged distortion between any pair of 
subclusters in different classes, i.e., 
()()( )
11 11
11 1||
ij
ijkl k l
ij U i j
ij c k C l C
st
mn
mnc sC tCqq D p p
VUqqϕϕ
ϕ≤≠≤ ≤≤ ≤≤−
≤≠≤ ≤ ≤ ≤ ≤⎡ ⎤
⎢ ⎥
=⎢ ⎥
⎢ ⎥
⎢ ⎥ ⎣ ⎦∑∑∑
∑∑∑, (2.42)
where k
iq is the prior probability of the kth subcluster of the ith class; k
ip is the 
sample density of the samples in the kth subcluster in the ith class; ()||kl
Ui jDp p  is 
the divergence between the kth subcluster in the ith class and the lth subcluster in 
the jth class. 
 
                                                 
4 The means of each class is equal to one of the samples and the covariances are arbitrarily close 
to singular. For example, we have N samples and we use N Gaussians to model these samples. The 
mean of each Gaussian is equivalent to a sample and the covariance of each Gaussian is singular. 38  Geometric Mean for Subspace Selection 
 
In LDA and ODA, the arithmetic mean of  the divergences is used to find a 
suitable subspace into which to project th e samples. The main benefit of using the 
arithmetic mean in LDA is that the projection matrix can be obtained by the 
generalized eigenvalue decomposition. However, LDA is not optimal for multiclass classification [103] because of the class separation  problem defined in 
§103. Therefore, it is useful to investigate other choices of 
ϕ in (2.38) to see if 
better results can be obtained. 
 
 Criterion 1:  
Maximization of the Geometri c Mean of the Divergences 
The log function is a suitable choice for ϕ because it increases the effects of 
small divergences and at the same time reduces the effects of large divergences. 
On setting  ()() logx x ϕ=  in (2.38), the generalized geometric mean of the 
divergences is obtained. The required subspace *U is given by, 
()1
1*a r g m a x | |ij
mn
mncqq
qq
Ui j
U ij cUD p p
≤≠≤
≤≠≤∑ ⎡⎤ =⎣⎦∏ . (2.43)
It follows from the mean inequality that the generalized geometric mean is upper 
bounded by the arithmetic mean of the divergences, i.e., 
() ()1
1 1
1|| ||ij
mn
mncqq
ijqq
Ui j Ui j
ij c ij c mn
mncqqDpp Dppqq≤≠≤
≤≠≤ ≤≠≤
≤≠≤⎛⎞
⎜⎟∑ ⎡⎤ ≤⎣⎦ ⎜⎟⎜⎟⎝⎠∑ ∏∑. (2.44)
Proof: See Appendix. 
Furthermore, (2.43) emphasizes the total volume of all divergences. For example, 
in the special case ijqq= for all , ij, 
() ()1
11arg max || arg max ||ij
ij
mn
mncqqqq
qq
Ui j Ui j
UU ij c ij cDpp Dpp
≤≠≤
≤≠≤ ≤≠≤∑ ⎡⎤ ⎡⎤ =⎣⎦ ⎣⎦∏∏  
()
1arg max ||Ui j
U ij cDpp
≤≠≤=∏ . (2.45)
 39  Criterion 2:  
Maximization of the Geometri c Mean of the Normalized 
Divergences 
We can further strengthen the effects of small divergences on subspace selection, 
by maximizing the geometric mean5 of all normalized divergences6 in the 
projected subspace, i.e., 
()()
()1
1
11*a r g m a x | | a r g m a x | |cc
Ui j Ui j
UU ij c ij cUE p p E p p−
≤≠≤ ≤≠≤⎡⎤==⎢⎥
⎣⎦∏∏ , (2.46)
where the normalized divergence ()||Ui jEpp  between the ith class and the jth 
class is defined by: 
()()
()
1||
||||ijU i j
Ui j
mn U m n
mncqq D p p
EppqqD p p
≤≠≤=∑. (2.47)
The intuition behind (2.46) is that the product of normalized divergences is large 
when the normalized divergences are similar to each other. Therefore, maximizing the geometric mean of the normalized divergences will tend to make them as 
similar as possible. The effects of the small divergences should then be 
emphasized.  
 Criterion 3:  
Maximization of the Geomet ric Mean of all Divergences 
Although criterion 2 emphasizes small divergences during optimization, direct use 
of the criterion is not desirable for subspace selection. This is because 
experiments in §233066540 show that there exists U for which all divergences 
become small, but all normalized divergences are comparable in size. In such 
case, the projection matrix U is not suitable for classification, because several 
classes may be severely overlapped. 
To reduce this problem, we combine criterion 2 with criterion 1 into a new one. 
The new criterion maximizes the linear combination of: 1) the log of the 
                                                 
5 In (2.46), we use the geometric mean but not the generalized geometric mean because the 
weights (the prior probabilities iq) are moved to the normalized divergences as shown in (2.47). 
In this form, the calculations are simplified. 
6 The sum of all normalized divergences is one. 40 geometric mean of the divergences and 2) the log of the geometric mean of 
normalized divergences. This criterion is named the Maximization of the 
Geometric Mean of all Divergences , or briefly MGMD, 
()()
() ()
() ()11
1
1
1
11log ||
*a r g m a x
1l o g | |
    arg max log || log || ,ij
mn
mnccc
Ui j
ij c
qq U
qq
Ui j
ij c
Ui j i j Ui j
U ij c ij cEpp
U
Dpp
Dpp q q Dppα
α
η≤≠≤−
≤≠≤
≤≠≤
≤≠≤ ≤≠≤⎧⎫⎡⎤⎪⎪⎢⎥⎪⎪⎪⎪⎣⎦ =⎨⎬
⎪⎪∑ ⎡⎤ +−⎪⎪⎣⎦⎪⎪⎩⎭
⎧⎫ ⎛⎞ ⎪⎪=−⎨⎬ ⎜⎟
⎪⎪ ⎝⎠ ⎩⎭∏
∏
∑∑ (2.48)
where 01α<<  is the linear combination coefficient to integrate the criterion 1 
with 2; and ()
() ()1
11
11mn
mnc
mn
mnccc qq
cc qqα
ηαα≤≠≤
≤≠≤−
=−− +∑
∑. The supremum of η is ()1 cc− 
and the infimum of η is 0. When 0α= (or 0η=), (2.48) reduces to (2.43); 
and when 1α= (or ()1 ccη=−), (2.48) reduces to (2.46). The rational of the 
combination coefficient α trades off the geometric mean of divergences of 
different pairs of classes against the geometric mean of normalized divergences of 
different pairs of classes. The first part emphasizes the total volume of all divergences; increases effects of small divergences; and reduces effects of large 
divergences. The second part further stre ngthens the effects of small divergences 
and weakens theeffects of large divergences. Therefore, by tuning the parameter 
α, we can balance the impacts of select ed subspace on the total volume of all 
divergences, the impacts of large divergences, and the impacts of small ones. 
Deduction 2.1: See Appendix. 
Based on (2.48) and (2.42), we directly extend MGMD to the multimodal case, as 
the Multimodal extension of the Maximization of the Geometric Mean of all 
Divergences  (M–MGMD), 
()
()11 1
11 1log ||
*a r g m a x
log ||ij
ijkl
Ui j
ij c k C l C
U kl k l
ij U i j
ij c k C l CDp p
U
qqD p p η≤≠≤ ≤≤ ≤≤
≤≠≤ ≤≤ ≤≤⎧⎫
⎪⎪⎪⎪=⎨⎬⎛⎞⎪⎪−⎜⎟⎜⎟⎪⎪⎝⎠⎩⎭∑∑∑
∑∑∑. (2.49)
In the next Section, we discuss subspace selection based on the choice of the KL 
divergence in (2.48) and (2.49). 41  Kullback–Leibler Divergence based Subspace Selection 
 
In the last Section, we developed a framework MGMD/M–MGMD for subspace 
selection based on the geometric mean of divergences and normalized 
divergences. In this Section, we combine the KL divergence with MGMD/M–
MGMD as an example for practical applic ations. The multimodal problem [50] is 
carefully studied and finally we kern elize the proposed subspace selection 
method. Experimental studies are given in §50, §50, and §50. 
 
 MGMD and KL Divergen ce for Subspace Selection 
By combining the KL divergence defined in (2.36) and MGMD defined in (2.48), 
we obtain the Maximization of the Geometric Mean of all KL Divergences  
(MGMKLD), 
() *a r g m a x
UUL U= , (2.50)
where () LU  is defined by 
() () ()
11log || log ||Ui j i jUi j
ij c ij cLU K L p p q qK L p p η
≤≠≤ ≤≠≤⎛⎞=− ⎜⎟
⎝⎠∑∑ , (2.51)
and ()||Ui jKLpp  is the KL divergence between the ith class and the jth class in 
the projected subspace, 
()
() ()( )()( )111|| log log2
    tr trTT
Ui j j i
TT T T
ji j i jKL p p U U U U
UU U U UU U D U−−=Σ − Σ
+Σ Σ +Σ (2.52)
To better understand MGMKLD, we need to define the Stiefel manifold and the 
Grassmann manifold. 
Definition 2.2:  The Stiefel manifold [10] (),St n r  for nr≥ is defined as a set 
of all nr×  matrices with orthonormal columns, i.e., 
(,) { : }nr T
r St n r U R U U I×=∈ = . (),St n r  is a sub–manifold of nrR× of real 
dimension 22nr r×− . Two elements 1U and 2U in (),St n r  are said to be 
equivalent if their columns span the same subspace, i.e., 12UU Q=  for some 
orthogonal matrix rrQR×∈ . 42  
Table 2.5. Optimization procedure for MGMKLD 
Input:  Training samples ;ijxr in nR, where i denotes the ith class ( 1ic≤≤ ) 
and j denotes the jth sample in the ith class (1ijn≤≤), the dimension k (kn<) 
of the selected subspace, the maximum number M of different initial values for 
the projection matrix U, the learning rate κ(a small value), the combination 
factor η, and a small value ε as the convergence condition. 
Output: An estimate *U in nkR× of the optimal projection matrix. 
1. for 1:mM=  { 
2. Initialize m
tU ( 1t=) randomly. 
3. while  ()()1mm
ttLU LU ε−− > (610ε−= ), 
where ()m
tLU  and ()||m
tij UKL p p  are defined in (2.51) and (2.52), 
respectively. 
do{ 
4. Conduct the gradient steepest ascent step: 
() 11mm m
tt U tUU L U κ−−←+ ⋅ ∂ , where ()m
UtLU∂  is defined in (2.53). 
5. 1 tt←+ . 
5. }// while  in line 3 
6. }//for in line 1 
7. () arg maxm
mUV Uϕ ←  
8. Ortho–normalization Step: () * ortho - normalizeUU← . 
 
Definition 2.3: The Grassmann manifold [10] is the quotient space of (),St n r  
with respect to the above equivalent relation. Each element in the Grassmann 
manifold (), Gr n r  is an equivalent class in (),St n r . (), Gr n r  is a set of all r–
dimensional vector subspaces of nR7. 
As mentioned in the Table 2.4, the procedure for the maximization of the general 
averaged divergences, we prove the projection matrix U is invariant to the 
                                                 
7 http://mathworld.wolfram.com/GrassmannManifold.html 43 ortho–normalization operator for () LU , i.e., U stays in the Grassmann 
manifold. In other words, () LU  will depend only on the subspace defined by 
U. 
Claim 2.1: The ortho–normalization operation does  not change the value of the 
objective function ()LU  of MGMKLD defined in (2.51). 
Proof: See Appendix. 
To obtain the optimization procedure for MGMKLD based on Table 2.4, we need 
the first order derivative of () LU , 
()
()()
() ()1
1
1
11|| ||
     || || ,U
Ui j U U i j
ij c
mn U m n i jU U i j
mnc i jcLU
K Lpp K Lpp
q q KL p p q q KL p pη−
≤≠≤
−
≤≠≤ ≤ ≠≤∂
=∂
⎛⎞ ⎛⎞−∂ ⎜⎟ ⎜⎟⎝⎠ ⎝⎠∑
∑∑ 
(2.53)
where the first order derivative of ()||Ui jKLpp  is given by 
()
()() ()()
() ()()11 1
11||
    .UUi j
TT T
jj i i i i j j
TT T
jj i i j jKL p p
UU U UU U D UU U
UU U U D UU U− −−
−−∂
=Σ − Σ Σ + Σ + Σ
−Σ Σ Σ + ΣΣ  (2.54)
 
 Multimodal Exte nsion of MGMKLD 
In many situations, the distribution of each class is not Guassian. It is reasonable 
to use GMMs to fit each class. The advantages of introducing GMM to 
discriminative subspace selec tion are described in §0. In this Section, we combine 
GMM with MGMKLD as the multimodal extension of MGMKLD (M–MGMKLD). The combination, M–MGMKLD, is obtained from (2.36) and (2.49), 
as 
() *a r g m a xGMM
UUL U=  
()
()11 1
11 1arg max log ||
                                        log || .ij
ijkl
ij
U ij c k C l C
kl k l
ij U i j
ij c k C l CKLU p p
qq K L p p η≤≠≤ ≤≤ ≤≤
≤≠≤ ≤≤ ≤≤⎡⎤ =⎣⎦
⎡ ⎤⎡ ⎤ −⎢ ⎥⎣ ⎦⎢ ⎥ ⎣ ⎦∑∑∑
∑∑∑W
 (2.55)44 Claim 2.2: The ortho–normalization operation does not change the value of the 
objective function ()GMMLU  of M–MGMKLD. 
Proof.  This claim is proved in the same way as the Claim 2.1.          ■ 
 
Table 2.6. Optimization procedure for M–MGMKLD. 
Input:  Training samples ;ijxr in nR, where i denotes the ith class ( 1ic≤≤ ) 
and j denotes the jth sample in the ith class (1ijn≤≤), the dimension k (kn<) 
of the selected subspa ce, the maximum number M of different initial  values for
U, the learning rate κ(a small value), the combination factor η, and a small 
value ε as the convergence condition. 
Output:  Linear projection matrix *U in nkR×. 
1. Conduct the GMM–FJ to cluster samples in each class and obtain the 
corresponding covariance matrix k
iΣ and mean value k
imr, where 
1ic≤≤  and 1i kC≤≤, iC is the number of clusters of the ith class. 
2. for 1:mM=  { 
3. Initialize 1mU randomly. 
4. while  ()()1mm
GMM t GMM tLULU ε− − > (610ε−= ),  
where ()GMMLU  is defined in (2.55). 
do{ 
5. Conduct the gradient steepest step: () 11mm m
tt U G M M tUU LU κ−−←+ ⋅ ∂ , 
where ()UG M MLU∂  is defined in (2.56). 
6. }// while  in line 4 
7. () arg maxm
mUV Uϕ ← . 
8. }// for in line 2 
9. Ortho–normalization Step: () * ortho - normalizeUU← . 
 
With Claim 2.2 and Table 2.4, we can obtain the optimization procedure for M–
MGMKLD. We only need to mention the fi rst order derivative of the objective 
function ()GMMLU  of M–MGMKLD, 45 () ()()
()1
11 1
1
11 1|| ||
                                             ||
                                                ij
ijkl kl
UG M M U i j U U i j
ij c k C l C
st s t
mn U m n
mnc sC tCL U K L pp K L pp
qqK L p p η−
≤≠≤ ≤≤ ≤≤
−
≤≠≤ ≤ ≤ ≤ ≤∂= ∂
⎛⎞
−⎜⎟⎜⎟⎝⎠∑∑∑
∑∑∑
()
11 1  || .
ijkl k l
ij U U i j
ij c k C l Cqq K L p p
≤≠≤ ≤≤ ≤≤⎛⎞
∂ ⎜⎟⎜⎟⎝⎠∑∑∑ (2.56)
By incorporating the merits of GMM–FJ and the proposed MGMKLD model, the 
new method has the following benefits: 
1. Experiments show that the new method reduces the class separation  
problem; 
2. The new method inherits the merits of GMM–FJ. In detail, it determines the number of subclusters in each class automatically; it is less sensitive to 
the choice of initial values of the parameters than EM; and it avoids the boundary problem of the parameter space; 
3.
 The new method is capable of obtaining the projection orthogonal matrix, 
i.e., ()GMMLU  depends only on the subspace defined by U. 
 
 Kernel Extension of MGMKLD 
In this part, we study the kernel extension of MGMKLD. Because of the success 
of the kernel method [129][109] in pattern classification, MGMKLD will be 
generalized from the low dimensional original Hilbert space to the higher 
dimensional Hilbert space. The generalized version is named the kernel 
MGMKLD (KMGMKLD). To utilize the kernel dot product trick [ 129][109] for MGMKLD, we need to have 
the Lemma 2.1. Denote 
⊕ as the direct sum. 
Lemma 2.1:  If U is a solution to MGMKLD and x x UU U⊥=⊕ , then xU is a 
solution to MGMKLD. We have ()()x LUL U= . Here, the column space of xU 
is spanned by the samples {}1
;1|ijn
ij icx≤≤
≤≤r and the column space of xU⊥ is the 
orthogonal complement of the column space of xU. 
Proof: See Appendix. 
From Lemma 2.1, we know that the orthogonal complement component xU⊥ 
(xUU=) does not affect the objective function () LU  of MGMKLD defined in 46 (2.51). Consequently, we can set 0xU⊥=, i.e., the column space of U is 
spanned by the samples {}1
;1|ijn
ij icx≤≤
≤≤r. Based on Lemma 2.1, the kernel trick can be 
utilized to implement the kernel extension of MGMKLD, because () LU  defined 
in (2.51) can be fully expressed in te rms of inner products with the samples 
{}1
;1|ijn
ij icx≤≤
≤≤r only. Without this lemma, the kernelization of MGMKLD cannot be 
implemented. 
Herein, there is a mapping rule :LHR Rφa  to map MGMKLD to a higher–
dimensional space [109][104][129]. The samples xr, which are modelled by 
(),ii NmΣr, are mapped as 
()()() ( ) ;; ~,ij ij i ixx N mφφ φ→Σrr r (2.57)
() ;;
11in
ii j
jimxnφφ
==∑rr (2.58)
()( )()( ) ;; ; ; ;T
ii j i i j iExm xmφφ φφφ⎡⎤Σ= − −⎢⎥⎣⎦rr rr (2.59)
where ;imφr is the mean vector of the ith class in HR and ;iφΣ is the covariance 
matrix of the ith class in HR. 
Based on Lemma 2.1, we can choose the projection matrix in HR as 
() () ;; ; *
11in c
ij ij ij nFFnijUx xφαφ φ××==⎡⎤ == Λ⎣⎦ ∑∑rr, (2.60)
where ;ijα is the linear combination coefficient to combine the training samples 
();ijxφr in HR. 
This means each column of Uφ is a linear combination of all ();ijxφr by 
varying i and j from all classes and samples in each class. 
Therefore, the KL divergence ()()()() ( ) ||Ui jKLp x p xφφr r in the feature space is 
()()()() ()
() ()()( );;
1
;; ;||
log log1
2 trUi j
TT
ji
TT
jj i jKL p x p x
UU U U
UUU D Uφ
φφ φ φφ φ
φφ φ φ φ φ φφφ
−⎛⎞Σ− Σ⎜⎟=⎜⎟+Σ Σ +⎜⎟⎝⎠r r
. (2.61)
Therefore, (2.51) becomes 47 () ()()()() ( )
()() ()() ()1
1log ||
               log ||Ui j
ij c
mn U m n
mncLU K L p x p x
qqK L p x p xφ
φφ φφ
ηφ φ≤≠≤
≤≠≤⎡⎤=⎣⎦
⎡ ⎤−⎢ ⎥⎣ ⎦∑
∑r r
rr. (2.62)
The variable ijD becomes 
() () () () ;; ; ; ;
11 111111jj iiTnn nn
i j ik il ik il
klklijijDx x x xnn nnφ φφφφ
====⎛⎞ ⎛⎞
=− −⎜⎟ ⎜⎟⎜⎟ ⎜⎟⎝⎠ ⎝⎠∑∑∑∑rrrr. (2.63)
To obtain the kernel Gram matrix [129] based representation in (2.61), we need to 
reformulate ();T
i UU Uφ φφΣ  by the kernel dot product trick8 as: 
;
', , , , '11 111
i i ii i i ii i iT
i
T
TT
nH C CC CC CC CC C nH
ii iUU
KI I Knn nφφ φ
××Σ
⎛⎞ ⎛⎞=Λ − − Λ ⎜⎟ ⎜⎟
⎝⎠ ⎝⎠, (2.64)
Deduction 2.2: See Appendix. 
where .;jK is the ()1
1thi
k knj−
=+∑  column of the kernel Gram matrix 
()() ;;T
ij ijFnF nKx xφφ
××⎡⎤ ⎡⎤=⎣⎦ ⎣⎦rr () ;;,ij ijkx x⎡ ⎤ =⎣ ⎦rr, () ;;,ij ijkx xrr is the kernel function 
[129], ,
,CCii
iiCCIR∈  is the identity matrix, ,
,1ii
iiCC
CC R∈  is the unit matrix, 
.;1 iiCjjnKK
≤≤⎡⎤=⎣⎦ is composed of the columns in the kernel Gram matrix from the 
()1
11thi
k kn−
=+∑  column to the ()1thi
k kn=∑  column; ' nH×Λ  is the projection 
matrix in HR and 'H is the number of selected features in HR. 
With the kernel dot product trick we can transform ;T
ij UD Uφφφ  into 
;
' '11 111111
ii j j ii j jT
ij
T
T
n H C C CC C C CC n H
ij ijUD U
KK KKnn nnφφ φ
× ×⎛⎞ ⎛⎞
=Λ − − Λ⎜⎟ ⎜⎟⎜⎟ ⎜⎟⎝⎠ ⎝⎠. (2.65)
Deduction 2.3: See Appendix. 
Therefore, we can reformulate (2.61) as: 
                                                 
8 The kernel dot product trick is used to transform a linear algorithm (for classification, subspace 
selection, regression, and etc.) to a non–linear one by mapping the original samples in a low 
dimensional space into a higher dimensional space. Based on this mapping, the linear algorithm in 
the new space is equivalent to non–linear algorithm in the original space. The trick transforms a 
linear algorithm by replacing the dot product between two vectors with a kernel function. 48 ()
()'' ' '
1
'' ' , , '
1
''' '11log log
11tr2
11trjj i i
j j ij ij
jj i iij
TT T T
nH C C nH nH C C nH
ji
TT T T
nH C C nH nH CC CC nH
j
TT T T
nH C C nH nH C C nH
jiKL U
KK KKnn
KK L Ln
KK KKnnφ
×× × ×
−
×× × ×
−
××× ×⎛
⎜ΛΛ − ΛΛ⎜
⎜
⎜⎛⎞⎛⎞⎜⎜⎟=+ Λ Λ Λ Λ⎜⎟⎜⎟⎜⎟⎝⎠⎝⎠
⎛⎞⎛⎞ ⎛⎞⎜⎟+Λ Λ Λ Λ⎜⎟ ⎜⎟⎜⎟⎜⎟ ⎝⎠ ⎝⎠⎝⎠⎝⎞
⎟
⎟
⎟
⎟⎟
⎜⎟
⎜⎟
⎜⎟⎜⎟
⎜⎟⎜⎟
⎠, (2.66)
where 
jCK and ,ijCCL  are defined by: 
,,11
j j jj jjC C CC CC
jKK In⎛⎞
=−⎜⎟⎜⎟⎝⎠ (2.67)
,1111
ij i i j jCC C C C C
ijLK Knn⎛⎞
=−⎜⎟⎜⎟⎝⎠. (2.68)
For a given sample zr, the corresponding feature ()TUzφφr in the higher 
dimensional space is given by: 
() ()() () '; '; ,TTTT T
nH ij nH ijHnH nUz x z k x zφφφ φ××× ×⎡⎤⎡ ⎤ =Λ =Λ⎣⎦⎣ ⎦r r rr r, (2.69)
where () ;,ijkx zrr is the kernel function with entries ;ijxr and zr. There are many 
typical kernel functions that could be used here, e.g., the Gaussian radial basis 
kernels (() () ( )2 2,e x p 2kx x x x σ ′′=− −rr r r), the homogeneous polynomial kernels 
((),,dkx x x x′′=rr rr), the inhomogeneous polynomial kernels ( (),kx x′=rr 
(),dxxc′+rr), and the sigmoid kernels ( ()( ) ,t a n h ,kx x x x κϑ ′′=−+rrr r). The 
homogeneous polynomial kernels and the Gaussian radial basis kernels are widely 
used in pattern classification. The hom ogeneous polynomial kernels are invariant 
under orthogonal transformations. Liu [87] empiricially demonstrated that the 
fractional powers in the homogeneous polynomial kernels perform better than 
integral powers. The radial basis kernels can be written as ()()() ,,kx x fdx x′′=rrr r, 
where f is a function on R+ and d is a metric function. In Gaussian radial 
basis kenrles, f is an exponential function and d is the Euclidean metric. In 
different applications, we  can choose different f and d through the cross–
validation to achieve reasonable performances. 49 Theorem 2.1:  MGMKLD followed by KPCA is equal to KMGMKLD. 
Proof: See Appendix. 
Theorem 2.2:  M–MGMKLD followed by KPCA is equal to the multimodal 
extension of KMGMKLD, or briefly M–KMGMKLD. 
Proof.  T h e  r e s u l t  f o l l o w s  f r o m  t h e  T h e o r e m  2 . 1 .                     ■ 
In the Theorem 2.1, we show that kernel principal component analysis (KPCA) 
following MGMKLD yields kernel MGMKLD. Based on this theorem, 
KMGMKLD is equivalent to first preprocessing the data using KPCA and then applying orthogonal MGMKLD to the preprocessed data. 
 50  Comparison using Synthetic Data 
 
In this Section, we compare MGMKLD with the previous subspace selection 
methods, which are LDA [39], heteroscedas tic discriminant analysis (HDA) [61], 
approximate pairwise accuracy criter ia (aPAC) [98], weighted LDA (WLDA), 
fractional–step LDA (FS–LDA) [95], he teroscedastic extenstion LDA (HLDA) 
[99], oriented discriminant analysis  (ODA) [28], and multimodal oriented 
discriminant analysis (MODA) [28]. WLDA is similar to aPAC, but the weighting 
function is 8d−. In FS–LDA, the weighting function is 8d− and the number of 
the fractional step is 30. We denote the proposed method as MGMKLD( η), 
where η is the combination factor in (2.48). We do not compare the proposed 
methods with the nonparametric based methods [2][16][94][90], because our 
framework is parametric. 
 
 Heteroscedastic Problem 
To examine the classificati on ability of these subspace selection methods for 
solving the heteroscedastic problem [99], we generate two classes such that each 
class has 500 samples, drawn from a Gaussian distribution. The two Gaussian 
distributions have identical mean values but different covariances. As shown in 
Figure 2.4, LDA, aPAC, WLDA, and FS–LDA separate classes without taking the differences of class covariances into a ccount. All other methods achieve better 
separation of the two classes as shown in Figure 2.4, because they consider the 
differences both in class means and in class covariances. 
 
 51  
 
Figure 2.4. Heteroscedastic example: in this figure, from left to right, from top to 
bottom, there are nine subfigures showing the projection directions (indicated by 
lines in each subfigure) obtained us ing LDA, HDA, aPAC, WLDA, FS–LDA, 
HLDA, ODA, MGMKLD(0), and MGMKLD(1 ). In this experiment, the linear 
classifier () ()1ˆˆˆˆ lnT
ii i i xm xm−−Σ− + Σ  is applied as the classification method 
after a subspace selection procedure, where ˆim and ˆ
iΣ are the estimated ith 
class mean and covariance matrix in the lower dimensional space, respectively. 
The training classification errors of these methods are 0.3410, 0.2790, 0.3410, 0.3410, 0.3430, 0.2880, 0.2390, 0.2390, and 0.2390, respectively. 
 
Based on the same data, we demonstrate that the geometric mean of normalized 
divergences is not sufficient for subspace selection. On setting 
2η=, MGMKLD 
reduces to the maximization of the geometric mean of normalized KL 
divergences. This result is based on the description of (2.48). The left subfigure in 
Figure 2.5 shows the projection direction (i ndicated by lines in each subfigure) 
from the two dimensional space to a one dimensional space found by maximizing the geometric mean of normalized KL divergences. The projection direction 
merges the two classes. The right subfigure in Figure 2.5 shows the geometric 
mean of normalized KL divergences in the one dimensional subspace during training. In this experiment, we observed: 1) KL divergences between the class 1 52 and class 2 is 1.1451 and normalized KL divergences is 0.5091 at the 1000th 
training iteration; and 2) the KL divergence between the class 2 and class 1 is 1.1043 and the normalized KL divergence is 0.4909 at the 1000
th training 
iteration. The right subfigure shows nor malized KL divergences are maximized 
finally, but the left subfigure shows the projection direction is not suitable for classification. The suitable projection dire ction for classification can be found in 
the bottom–left subfigure in Figure 2.4. 
 
0 100 200 300 400 500 600 700 800 900 10000.440.450.460.470.480.490.5
Training IterationsGeometric Mean of the Normalized KLDs
 
Figure 2.5. The maximization of the geometric mean of the normalized 
divergences is not sufficient for subspace selection. 
 
 Multimodal Problem 
In many applications it is useful to mo del the distribution of a class by a GMM, 
because samples in the class may be drawn from a non–Gaussian distribution. To demonstrate the classification ability of  M–MGMKLD, we generated two classes; 
each class has two subclusters; and samp les in each subcluster are drawn from a 
Gaussian distribution. Figure 2.6 shows the subspaces selected by different methods. In this case, LDA, WLDA, FS–LDA, and aPAC do not select good 
subspaces for classification. However, the multimodal extensions of ODA and 
MGMKLD find suitable subspaces. Furthermore, although HDA and HLDA do not take account of multimodal classes, they each select a suitable subspace. This is because the two classes have different class covariance matrices when each 
class is modelled by a single Gaussian distribution. For complex cases, e.g., when each class consists of more than 3 s ubclusters, HDA and HLDA fail to find good 
subspaces for classification. 53  
 
 
Figure 2.6. Multimodal problem: in this figure, from left to right, from top to 
bottom, there are nine subfigures, which show the projection di rections (indicated 
by lines in each subfigure) by using LDA, HDA, aPAC, FS–LDA(3), FS–LDA(8), HLDA, MODA, M–MGMKLD(0), and M–MGMK LD(1). In this experiment, the 
linear classifier 
() ()1ˆˆˆˆ lnT
ii i i xm xm−−Σ− + Σ  is applied as the classification 
method after a subspace selection procedure, where ˆim and ˆ
iΣ are the 
estimated ith class mean and covariance matrix in the lower dimensional space, 
respectively. The training classification errors of these methods are 0.0917, 0.0167, 0.0917, 0.0917, 0.0917, 0.0167, 0.0083, 0.0083, and 0.0083, respectively. 
 
 Class Separation Problem 
The most prominent advantage of MGMKLD is it can significantly reduce the 
classification errors caused by very str ong effects of large divergences between 54 certain classes. To demonstrate this point , we generate three classes for which the 
samples in each class are drawn from Gaussian distributions. The KL divergence between two of these classes is small and the KL divergences between the third 
class and the two classes are large, i.e., two classes are close together and the third 
is further away. In this case, MGMKLD(0) also performs well, but not as well as MGMKLD(5). 
In Figure 2.7, it is shown that MGMKLD(5 ) separates all classes. Furthermore, in 
MGMKLD, we achieve the same results when setting 
η, defined in (2.48), equal 
to 1,2,3,4, and 5. However, LDA, HLDA, and ODA do not give good results. The 
aPCA and FS–LDA algorithms are better than LDA but neither of them gives the 
best projection direction. The result ob tained from aPCA is better than that 
obtained from WLDA, because aPAC uses a better weighting strategy than WLDA. 
 
 
 
 55 Figure 2.7. Class separation problem: in this figure, from left to right, from top to 
bottom, there are nine subfigures to describe the projection directions (indicated by lines in each subfigure) by using LDA, HDA, aPAC, WLDA, FS–LDA, 
HLDA, ODA, MGMKLD(0), and MGMKLD(5 ). In this experiment, the linear 
classifier 
() ()1ˆˆˆˆ lnT
ii i i xm xm−−Σ− + Σ  is applied as the classification method 
after a subspace selection procedure, where ˆim and ˆ
iΣ are the estimated ith 
class mean and covariance matrix in the lower dimensional space, respectively. 
The training classification errors of these methods are 0.3100, 0.3033, 0.2900, 
0.3033, 0.0567, 0.3100, 0.3100, 0.1167, and 0.0200, respectively. MGMKLD(5) 
finds the best projection direction for classification.  
 
Figure 2.8. Class separation problem: in this figure, from left to right, there are 
three subfigures to describe the projection directions (indicated by lines in each 
subfigure) by using aPAC, FS–LDA, and MGMKLD(5). In this experiment, the 
linear classifier () ()1ˆˆˆˆ lnT
ii i i xm xm−−Σ− + Σ  is applied as the classification 
method after a subspace selection procedure, where ˆim and ˆ
iΣ are the 
estimated ith class mean and covariance matrix in the lower dimensional space, 
respectively. The training classification errors of these methods are 0.1200, 
0.1733, and 0.0267, respectively. MGMKLD(5) finds the best projection direction for classification.  
In Figure 2.7, different Gaussians have id entical covariances. In this case, FS–
LDA works better than aPAC. In Figure 2.8, different Gaussians have different 
covariances. In this case, aPAC works better than FS–LDA. In both cases, the 
proposed MGMKLD (5) achieves the best performance. 
 56  Statistical Experiments 
 
jzr
iTimrjnr
;ijxr
inc0tμ=r
tIΣ=
0nμ=r2n IΣ=
iw
2k=mr0mμ=r
mIΣ=
0μ=r
5Σ=
 
Figure 2.9. Data Generation Model. In this model, 14 w=, 20 w=, 3 4 w=− , 
44 w=, 54 w=, 2k=, ()11 2 0 1Tmk m w=+rr, 22 00Tm=r, ()[]3 3 10 10 0, 1Tmk m w=+rr, 
()[]44 1 0 1 0 1, 0Tmk m w=+rr, and ()[]55 5 5 5 5 1, 0, 1, 0Tmk m w=+rr. 
 
In this Section, we utilize a synthetic da ta model, which is a generalization of the 
data generation model used by De la Torre and Kanade [28], to evaluate 
MGMKLD in terms of accuracy and robus tness. The accuracy is measured by 
averaged classification errors and th e robustness is measured by standard 
deviation of the classification errors. In this experiment, the linear classifier [30] 
and the nearest neighbour rule9 [105] are applied for classification after a 
subspace selection procedure. In this data generation model, there are five classes, 
which are represented by the symbols ○, ×, ＋, □, and ◇, as shown in 
Figure 2.11 (on page 62). In our experiments, for the training/testing set, the data 
generator gives 200 samples for each of the five classes (therefore, 1,000 samples 
in total). Moreover, the samples in each class are obtained from a single Gaussian. Each Gaussian density is a linear tran sformed “standard Gaussian distribution”, 
                                                
 
9 The nearest neighbor rule classifies a sample xr to the class C, when x′r is the nearest 
neighbor to xr under the Euclidean metric and x′r belongs to the class C. 57 i.e., ()0,NI . The linear transformations are defined by ;ij i j i jx Tz m n=+ +rr rr, 
where 20
;ijx R∈r, 20 7
iTR×∈ , ()7~0 ,jzNIR ∈r, ()20~0 , 2jnN IR ∈r, i denotes 
the ith class, j denotes the jth sample in this class, and imr is the mean value of the 
normal distribution for the ith class. The imr are assigned with the following 
values: ()( ) 12 020 , 1 4 1TmN=+r, 22 00Tm=r, ()( )[]31 0 1 02 0,1 4 0 ,1TmN=−r, 
()( )[]41 0 1 020 , 1 4 1 , 0TmN=+r, and ()( )[]55 5 5 52 0,1 4 1 ,0 ,1 ,0TmN=+r, where 201 
is a row vector in 20R and all entries in 201 are 1. The notations of 101, 51, 
200,  100 , and 50  have the similar meanings as 201 . The projection matrix iT is 
a random matrix, in which the elements are sampled independently from 
()0,5N , where 5 is the variance. The data generation model is shown in Figure 
2.9. Based on this data generation model, 800 groups (each group consists of 
training and testing samples) of synthetic data are generated. 
For comparison, the subspace selection methods, e.g., MGMKLD, are first 
utilized to select a given number of features. Then the nearest neighbour rule and 
the linear classifier ()()1 ˆˆˆˆ lnT
ii i i xm xm−−Σ−+ Σrr rr are used as the classification 
methods after a subspace selection procedure, where ˆ
imr and ˆ
iΣ are the 
estimated ith class mean and covariance matrix in the lower dimensional space, 
respectively. In this Section, the baseline algorithms are LDA, HDA, aPAC, 
WLDA, FS–LDA, HLDA, and ODA. 
 
 Performance Evaluation 
We conducted the designed experiments 800 times based on randomly generated 
data sets. The experimental results are reported in Table 2.7 – Table 2.10. Table 
2.7 and Table 2.9 show averaged classi fication errors of LDA, HDA, aPAC, 
WLDA, FS–LDA, HLDA, ODA, MGMKLD(0), and MGMKLD(2 c). Table 2.7 
and Table 2.8 show the results of the nearest neighbour rule and Table 2.9 and Table 2.10 show the results of the line ar classifier. For the 800 experiments, 
statistical experimental results are shown in Table 2.7 and Table 2.9, where 
arithmetic mean values are computed on different feature dimensions from 1 to 6 
(by column). Correspondingly, standard deviations under each condition, which 58 measure the robustness of the classifiers,  are given in Table 2.8 and Table 2.10. 
We emphasize that we have twenty feature dimensions for each sample and all samples are divided into one of the five classes, therefore, the maximal feature 
number for LDA, HDA, WLDA, aPAC, and FS–LDA is 5–1=4; in contrast, 
HLDA, ODA, and MGMKLD can extract more features than LDA and HDA. From Table 2.7 – Table 2.10, it can be concluded that MGMKLD outperforms 
LDA, HDA, aPAC, WLDA, FS–LDA, HLDA, and ODA, consistently. Finally, 
the linear classifier distance outperforms the nearest neighbour rule when samples are sampled from Gaussian distributions. 
 
Table 2.7: Averaged classification errors (the mean for 800 experiments) of LDA, 
HDA, aPAC, WLDA, FS–LDA, HLDA, ODA, MGMKLD(0), and MGMKLD(2 c). (The nearest neighbour rule) 
Basis 1 2 3 4 5 6 
LDA 0.2968 0.1552 0.1103 0.1504 –– –– 
HDA 0.3087 0.1850 0.1642 0.1807 –– –– 
aPAC 0.3206 0.1469 0.1088 0.1324 –– –– 
WLDA 0.4320 0.1930 0.1126 0.1092 –– –– 
FS–LDA 0.3098 0.1490 0.1108 0.1104 –– –– 
HLDA 0.2982 0.1561 0.1073 0.1050 0.1043 0.1043 
ODA 0.3029 0.1706 0.1370 0.1266 0.1219 0.1206 
MGMKLD(0) 0.2548 0.1397 0.1054 0.1030 0.1024 0.1018 
MGMKLD(2 c) 0.2430 0.1310 0.1039 0.1018 0.1010  0.1006  
 
Table 2.8: S tandard deviations of classification errors  for 800 experiments of 
LDA, HDA, aPAC, WLDA, FS–LDA, HLDA, ODA, MGMKLD(0), and 
MGMKLD(2 c). (The nearest neighbour rule) 
Basis 1 2 3 4 5 6 
LDA 0.1002 0.0995 0.0985 0.1077 –– –– 
HDA 0.1016 0.1040 0.1007 0.1055 –– –– 
aPAC 0.1270 0.1171 0.0979 0.0983 –– –– 
WLDA 0.1275 0.1239 0.1051 0.0995 –– –– 
FS–LDA 0.1475 0.1216 0.0990 0.0964 –– –– 
HLDA 0.1001 0.0992 0.0968 0.0942 0.0928 0.0920 
ODA 0.1010 0.1037 0.1016 0.0992 0.0965 0.0957 
MGMKLD(0) 0.1094 0.0985 0.0948 0.0919 0.0905 0.0893 
MGMKLD(2 c) 0.1130 0.0979 0.0947 0.0916 0.0902 0.0889 
 59 Table 2.9: Averaged classification errors (the mean for 800 experiments) of LDA, 
HDA, aPAC, WLDA, FS–LDA, HLDA, ODA, MGMKLD(0), and 
MGMKLD(2 c). (The linear classifier ()()1 ˆˆˆˆ lnT
ii i i xm xm−−Σ−+ Σrr rr) 
Basis 1 2 3 4 5 6 
LDA 0.2455 0.1199 0.0811 0.0813 –– –– 
HDA 0.2516 0.1384 0.0971 0.1221 –– –– 
aPAC 0.2626 0.1134 0.0827 0.0813 –– –– 
WLDA 0.3716 0.1498 0.0863 0.0817 –– –– 
FS–LDA 0.3050 0.1363 0.0883 0.1090 –– –– 
HLDA 0.2456 0.1216 0.0821 0.0791 0.0764 0.0741 
ODA 0.2500 0.1327 0.1037 0.0894 0.0829 0.0796 
MGMKLD(0) 0.2226 0.1099 0.0815 0.0776 0.0751 0.0725 
MGMKLD(2 c) 0.2404 0.1036 0.0806 0.0766 0.0742  0.0720  
 
Table 2.10: S tandard deviations of classification errors  for 800 experiments of 
LDA, HDA, aPAC, WLDA, FS–LDA, HLDA, ODA, MGMKLD(0), and 
MGMKLD(2 c). (The linear classifier ()()1 ˆˆˆˆ lnT
ii i i xm xm−−Σ−+ Σrr rr) 
Basis 1 2 3 4 5 6 
LDA 0.0932 0.0843 0.0792 0.0795 –– –– 
HDA 0.0919 0.0880 0.0886 0.1435 –– –– 
aPAC 0.1175 0.0987 0.0816 0.0795 –– –– 
WLDA 0.1231 0.1055 0.0870 0.0804 –– –– 
FS–LDA 0.1678 0.1387 0.0980 0.1272 –– –– 
HLDA 0.0919 0.0852 0.0799 0.0772 0.0745 0.0725 
ODA 0.0923 0.0894 0.0879 0.0810 0.0766 0.0739 
MGMKLD(0) 0.1033 0.0844 0.0788 0.0746 0.0720 0.0695 
MGMKLD(2 c) 0.1373 0.0848 0.0795 0.0743 0.0717 0.0692 
 
 Initial Values Issue 
We generate a training set according to the synthetic data model described at the 
beginning of this Section. We randomly initialize parameters in MGMKLD(2 c) to 
examine how different initial values affect the solution. Note that we omit lines 1, 2, 6, and 7 from the optimization procedure for MGMKLD given in Table 2.5, 
because these lines are used to set different initial values. We can see from Figure 
2.10 that MGMKLD(2 c) is insensitive to choices of  initial values in 50 random 
experiments. 60  
0 2 4 6 8 10 12 14 16 18 20−40−20020406080100120140Kullback−Leibler Divergence
0 2 4 6 8 10 12 14 16 18 20−20020406080100Kullback−Leibler Divergence
 
0 2 4 6 8 10 12 14 16 18 200102030405060708090100Kullback−Leibler Divergence
0 2 4 6 8 10 12 14 16 18 20102030405060708090100Kullback−Leibler Divergence
 
Figure 2.10. Initial values: From left to right, from top to bottom, the subfigures 
show the mean value and the corresponding standard deviation of the KL 
divergence between the class i and the class j of these 50 different initial values in 
the 10th (50th, 100th, and 1000th) training iterations. Because there are 5 classes in 
the training set, there are 20 KL divergences to examine. The circles in each subfigure show the mean values of the KL divergences for 50 different initial 
values. The error bars show the corresponding standard deviations. For better 
visualization, the scale for showing the standard deviations is 10 times larger than 
the vertical scale in each subfigure. The standard deviations of these 20 KL divergences approach 0 as the number of training iterations increases. 
 
 Nest Structure Property 
Definition 2.4:  Given a subspace selection method, []1 ni inWw≤≤=r and 
[]1 mi imUu≤≤=r are projection matrices obtained from the same training data but 
with mn>, i.e., mU contains more features than nW, i.e., mn>. Let nU be 
the first n columns of mU. If nU and nW are identified to the same point on a 61 Grassmann manifold, we say the subspace selection method has the nest structure  
property. For example, PCA has the nest structure property.   
-20 -15 -10 -5 0 5 10 15-10-5051015Select 2 features
1st class
2nd class
3rd class
4th class
5th class
 
-20 -15 -10 -5 0 5 10 15-15-10-50510Select 6 features
1st class
2nd class
3rd class
4th class
5th class
 62 -20 -15 -10 -5 0 5 10 15-10-505101520Select 10 features
1st class
2nd class
3rd class
4th class
5th class
 
Figure 2.11. MGMKLD(2 c) has no nest structure property. 
 
A desirable subspace selection method shoul d adapt to the selected dimensions.  A 
subspace selection method of this type is not expected to have the nest structure 
property. For instance, in our experiment, we first extract two dimensional 
features from the original entire f eature set (twenty features) based on 
MGMKLD(2 c); with these two features, the profile of the five classes of samples 
is illustrated in the first subfigure of Figure 2.11. We then extract six features 
from all based on MGMKLD(2 c) with the same training samples, but we only 
show the first two dimensions of the ex tracted six dimensional features in the 
second subfigure of Figure 2.11. The third subfigure shows the first two 
dimensions when ten features are extracted. Based on the figure, we can see that 
these first two features in the two cases are different. Therefore, MGMKLD(2 c) 
has no nest structure. 
 63  Real–World Experiments 
 
Figure 2.12. Samples in the USPS database [53]. 
 In this Section, we report the experimental results of the proposed algorithms 
using a well known character recognition data set, the United States Postal 
Services (USPS) database [53], in which there are 9,298 handwriting character 
samples divided into ten classes. Twenty samples of each class are given in Figure 2.12 row by row. Each sample is a vector with 256 dimensions. The entire USPS 
data set is divided into two parts [53], a training set with 7,291 samples and a test 
set with 2,007 samples. In our experiments,  we utilize the entire USPS database to 
evaluate performances of LDA, HDA, aPAC, WLDA, FS–LDA, HLDA, ODA, 
MODA, MGMKLD(0), MGMKLD(2 c), and M–MGMKLD(2 c). 
 
 Linear Method 
We apply the algorithms to the USPS database. As illustrated in Table 2.11, when 
the top 3, 5, and 7 discriminative features are required, MGMKLD(2 c) gives the 
best performance for all cases among all algorithms. When the top 9, 15, and 20 features are selected, M–MGMKLD(2 c)
 consistently outperforms all other 
algorithms and gives comparable performance to MGMKLD(2 c) in some cases. 
The error rate is only around half th at of ODA and MODA. The use of GMM in 
M–MGMKLD(2 c) is particularly advantageous for large data sets. Note that in the 
original 256 dimensional feature space, the number of the classes is ten, therefore, 
the maximum dimension of the extracted feature space is nine (10–1=9) for LDA, 64 aPAC, WLDA, HDA, and FS–LDA, while fo r other listed algorithms, including 
our MGMKLD(2 c) and M–MGMKLD(2 c), more features can be selected. This is 
a minor advantage of our algorithms compared with the conventional LDA model. 
 
Table 2.11. Performances (classification errors) of linear methods on the USPS database. (The nearest neighbour rule) 
Basis 3 5 7 9 15 20 
LDA 0.3827 0.1629 0.1186 0.1096 –– –– 
aPAC 0.3144 0.1679 0.2141 0.1106 –– –– 
HDA 0.3518 0.2516 0.1988 0.1734 –– –– 
WLDA 0.4310 0.2342 0.1415 0.1096 –– –– 
FS–LDA 0.3338 0.1794 0.1430 0.1131 –– –– 
HLDA 0.3518 0.2347 0.1779 0.1281 0.0947 0.0827 
ODA 0.3983 0.2617 0.1636 0.1162 0.1060 0.0970 
MODA 0.3950 0.2850 0.1576 0.1032 0.1027 0.0937 
MGMKLD(0) 0.2875 0.1574 0.1116 0.0987 0.0598 0.0583 
MGMKLD(2 c) 0.2720 0.1375 0.1105 0.0867 0.0578 0.0562 
M–MGMKLD(2 c) 0.3179 0.1414 0.1106 0.0822 0.0569 0.0553 
 
 Kernel Method 
We compare KMGMKLD(2 c) and M–KMGMKLD(2 c) with the Kernel LDA 
(KDA) [104] in Table 2.12. Firstly, the results show that all three kernel 
algorithms perform better than the linear algorithms. Secondly, the improvements 
for LDA, MGMKLD(2 c), and M–MGMKLD(2 c) are 13.22%, 8.42%, and 
23.11%, respectively. Herein, nine dimensions are selected to construct the kernel 
feature space. The kernel ()2,,kx x x x′′=rrr r is chosen. 
 
Table 2.12. Performances (error rates) of  kernel methods on the USPS database. A 
nine dimensional feature space is selected for each algorithm. (The nearest neighbour rule) 
Basis KDA KMGMKLD(2 c) M–KMGMKLD(2 c) 
Error 0.0951 0.0794 0.0632 
 65  Summary 
 
General averaged divergences analysis is proposed to enhance the conventional 
Fisher–Rao linear discriminant analysis (LDA). LDA is one of the most important 
subspace methods in pattern classification  research and applications; however, it 
has a tendency to merge together nearby classes when the features are projected to a lower dimensional space. To reduce this merging, new criteria for subspace 
selection are chosen. The new criteria are based on the geometric mean of the 
divergences between different pairs of classes. Three new criteria are defined, 
namely, 1) maximization of the geometric mean of the divergences; 2) maximization of the geometric mean of normalized divergences; and 3) 
maximization of the geometric mean of al l divergences. The third criterion is a 
combination of the first two criteria.  Then, the multimodal extension and the 
kernel extension of the maximization of the geometric mean of all divergences are 
introduced as well. The divergence can be any Bregman divergence. In our 
experiments we use the Kullback–Leibler divergence, which is a special case of the Bregman divergence. 
The new subspace selection methods are tested experimentally using synthetic 
data and handwriting data from the USPS database [53]. The experiments show 
that the third criterion, named the maximization of the geometric mean of all KL 
divergences is more effective than LDA and its representatives. In the future, we 
will utilize this method for other pattern classification tasks, for example biometrics and bioinformatics. 
 66 3. Discriminative Multi linear Subspace Method 
 
In computer vision research, many objects are naturally represented by multidimensional arrays, i.e., tensors [75], such as the gray face image shown in 
Figure 3.1 in face recognition [34][168], the color image shown in Figure 3.2 in 
scene image classification [156], and the video shot shown in Figure 3.3 in motion categorization [43]. However, in current research, the original tensors (images and 
videos) are always scanned into vectors, thus discarding a great deal of useful 
structural information, which is helpful to reduce the small sample size (SSS) problem in subspace selection methods, e. g., linear discriminant analysis (LDA). 
 
Second 
Order Tensor
WidthHeight
Face Image 
Figure 3.1. A gray level face image is a second order tensor, i.e., a matrix. Two 
indices are required for pixel locations. The face image comes from 
http://www.merl.com/projects/images/face–rec.gif. 
 
Third Order 
Tensor
WidthHeightColor
Color Image 
Figure 3.2. A color image is a third order tensor, which is also a data cuboid, 
because three indices are required to locate elements. Two indices are used for 
pixel locations and one index is used to local the color information (e.g., R, G, and B). 67  
Fourth Order 
TensorWidthHeightColor Video Shot
Time
ColorTime
 
Figure 3.3. A color video shot is a fourth order tensor. Four indices are used to 
locate elements. Two indices are used for pixel locations; one index is used to 
locate the color information; and the other index is used for time. The video shot 
comes from http://www–nlpir.nist.gov/projects/trecvid/.  
To utilize the structure information, ma ny dimension reduction algorithms [75] 
[132][162][171] based on the multilinear subspace method (MLSM) have been developed for data representation [75] [162][171][132], pattern classification 
[162][171][132], and network abnormal dete ction [136]. MLSM finds a sequence 
of linear transformation matrices 
iiLL
iUR′×∈  (iiLL′<, 1iM≤≤ ) to transform a 
large tensor 12 M LL LR××∈XL to a smaller tensor 12 M LL LR′′′××∈YL. For example, if 
we have a second order tensor 12LLR×∈X  with large 1L and 2L, in MLSM we 
need to find two linear transformation matrices 11
1LLUR′×∈  (11LL′< ) and 
22
2LLUR′×∈  (22L L′<) to transform X according to 12TUU=YX . After the 
transformation, the dimension is reduced from 12LL×  to 12LL′′× , i.e., 
12LLR′′×∈Y . Similar to LSM, MLSM includes a large number of methodologies 
varying from reconstructive models to discriminative models based on different 
criteria. The reconstructive models, e.g., the general tensor analysis (GTA) 
[75][162][171] and the tensor rank one analysis (TR1A) [132], are utilized to 68 generate low dimensional representations which preserve the original information 
as much as possible. The discriminative models, e.g., the two dimensional linear discriminant analysis (2DLDA) [173] and th e general tensor discriminant analysis 
(GTDA) [144][147], find low dimensiona l representations which preserve as 
much as possible of the information required for reliable classification. In this Chapter, we show that the SSS problem is reduced if the tensor structure of 
data is retained. We also demonstrate th at MLSM is a constrained version of the 
linear subspace method (LSM), i.e., MLSM is equivalent to LSM combined with constraints. 
In this Chapter, we mainly focus on the discriminative MLSM, especially 
2DLDA, which is a two dimensional extension of LDA. That is 2DLDA accepts matrix type data as input while LDA ac cepts vector type data as input. The 
effectiveness and the efficiency of 2DLDA for dimension reduction have been 
demonstrated in face recognition. However, 2DLDA fails to converge during the 
training stage. This nonconvergence has the following disadvantages: 1) it is difficult to determine when to stop the training stage; and 2) different numbers of 
training iterations lead to  different recognition results. To solve the 
nonconvergence problem in 2DLDA, we develop GTDA, which is an M 
dimensional extension of LDA. That is GTDA accepts general tensors (e.g., 
vectors, matrices, data cuboids, and high  dimensional arrays) as input. GTDA has 
the following properties: 1) reduction of the SSS problem for subsequent classification, e.g., by LDA; 2) preservation of the discriminative information in training tensors; 3) provision with a c onverged iterative optimization algorithm, 
which obtains a stable solution for GTDA; and 4) acceptance of general tensors as input. 
The organization of this Chapter is as  follows. In §147, the mathematical 
foundation, tensor algebra, of this th esis is briefly introduced. In §147, we 
describe the relationship between the LSM algorithms and the MLSM algorithms. In §147, the tensor rank one analysis based on the best rank one approximation is 
reviewed. In §147, the general tensor analysis based on the best rank 
( )12,,M RRRL  approximation is reviewed. In §147, we analyze the 
nonconvergence issue of the 2DLDA for discriminative subspace selection. In 
§147, we develop GTDA and GTDA’s manifold learning extension is given in §147. To examine the effectiveness of GTDA, it is applied for human gait 69 recognition and the performance evaluation is given in §147. Finally, the 
summary of this Chapter is given in §147 and the relevant deductions and proofs in this Chapter are given in §147. 
 70  Tensor Algebra 
 
This Section contains the fundamental ma terials on tensor algebra [75], which are 
relevant to this thesis. Tensors are arrays of numbers which transform in certain 
ways under different coordinate transformations. The order of a tensor 
12 M LL LR×××∈XL, represented by a multi–dimensio nal array of real numbers, is M. 
An element of X is denoted as 
12, ,...,M ll lX , where 1iilL≤≤ and 1iM≤≤ . The 
ith dimension (or mode) of X is of size iL. A scalar is a zero–th order tensor; a 
vector is a first order tensor; and a matrix is a second order tensor. The structure 
of a third order tensor is shown in Figure 3.4. In the tensor terminology, we have 
the following definitions. 
 
123LL LR××∈X
1L2L3L
 
Figure 3.4. A third order tensor 123LL LR××∈X . 
 
Definition 3.1 (Tensor Pr oduct or Outer Product) The tensor product ⊗XY  
of a tensor 12 M LL LR×××∈XL and another tensor 12 ''' 'M LL LR×× ×∈YL is defined by 
()12 1 2 '12 1 2 '... ' ' ... ' ... ' ' ... ' M MMMll l l l l ll l l l l ××× × ×× ××× ×× ××⊗=XY X Y , (3.01)
for all index values. 
For example, the tensor product of two vectors 1
1LxR∈r and 2
2LxR∈r is a matrix 
12LLXR×∈ , i.e., 12 1 2TXx x x x=⊗=rrr r. 
Definition 3.2  (Mode –d Matricizing or Matrix Unfolding)  The mode– d 
matricizing or matrix unfolding of an M–th order tensor 12 M LL LR×××∈XL is the set 
of vectors in dLR obtained by keeping the index di fixed and varying the other 71 indices. Therefore, the mode– d matricizing or matrix unfolding of an M–th order 
tensor is a matrix ()ddLL
dXR×∈ , where () di idL L≠=∏ . We denote the mode– d 
matricizing of X as () matdX or briefly ()dX. Figure 3.5 shows the mode–1, 
mode–2, and mode–3 matricizing of a third order tensor 322R××∈X . 
 
22 L=
322R××∈X
22 L=
322R××∈X
22 L=
322R××∈XMode-1
Mode-2
Mode-3
22 L=
322R××∈X
22 L=
32 L=
13L=32 L=
22 L=13L=()34
1X R×∈
()26
2X R×∈
()26
3X R×∈
 
Figure 3.5. The mode–1, mode–2, and mode –3 matricizing of a third order tensor 
322R××∈X . 72  
Definition 3.3 (Tensor Contraction)  The contraction of a tensor is obtained by 
equating two indices and summing over all values of repeated indices. Contraction 
reduces the tensor order by 2.  
A suitable notation for contraction is the Einstein’s summation convention10. For 
example, the tensor product of two vectors ,NxyR∈rr is Zxy=⊗rr; and the 
contraction of Z is T
iiZ xy xy=⋅=rrr r, where the repeated indices imply 
summation. The value of iiZ is the inner product of xr and yr. In general, for 
tensors 11 M M LL LLR′′′ ×× ×××∈XLL and 11 M M LL LLR′′′′′ ′ ×× ×××∈YLL, the contraction on the 
tensor product ⊗XY  is 
() () ( ) ( )1
11 11
111;1 : 1 :M
MM MM
MLL
ll ll ll ll
llMM
′ ′′ ′′′ ′ ′ ′ ××××× ×××××
==⊗= ∑∑ XY X YLL LL   L    , (3.02)
In this thesis, when the convention is conducted on all indices except the ith index 
on the tensor product of X and Y in 12 M LL LR×××L, we denote this procedure as 
()() ( )( )
() ()
() ()() ()11 1
11 1 11 1
11 111 1 1; ; 1: 1 , 1: 1: 1 , 1:
  
  mat mat ,ii M
ii i M ii i M
ii MLL LL
ll l l l ll l l l
ll l l
TT
ii iiii i i M i i M
XY−+
−+ −+
−+×× × × ×× ×× × × ××
== = =⊗= ⊗ − + − +
=
==∑∑ ∑∑XY XY
XY
XYLL LL               
LL  (3.03)
and ()() ;iiLLii R×⊗∈XY      . 
Definition 3.4  (Mode –d product)  The mode– d product dU×X  of a tensor 
12 ...M LL LR×× ×∈X  and a matrix 'ddLLUR×∈  is an 12 1 1 'dd d LL L L L−+ ××× × × ×L  
ML×L  tensor defined by 
() ( )
() ()12 1 112 1 1
                                          ; 2 ,dd d M d ddd d M
ddl l l l l l l l ll l l l l
lUU
Ud−+−+′ ××× ×× ×× × ′ ××× ×× ××
′×=
=⊗∑ XX
XLL LL
     (3.04)
for all index values. The mode– d product is a type of contraction. 
Based on the definition of Mode– d product, we have 
()()dt t dUV V U×× = ××XX  (3.05)
                                                 
10 “When any two subscripts in a tensor expression are given the same symbol, it is implied that 
the convention is formed .” –––––A. Einstein, Die Grundlage der Allgemeinen Relativitatstheorie, 
Ann. Phys., 49:769, 1916. 73 where 12 M LL LR×××∈XL, ddLLUR′×∈ , and ttLLVR′×∈ . Therefore, ()dtUV××X  
can be simplified as dtUV×× X . 
Furthermore, 
() ()dt dUV V U×× = ×XX  (3.06)
where 12 M LL LR×××∈XL, ddLLUR′×∈ , ddLLVR′′′×∈ , and VU is the standard matrix 
product of V and U. 
Figure 3.6 shows a third order tensor 123LL LR××∈X  mode–2 products with a 
matrix 22LLUR′×∈ . The result is a tensor in 123LL LR′××. 
 
123LLLR××∈X
1L2L3L
2×2L
2L′=
123LLLR′××∈Y
1L2L′3L
 
Figure 3.6. The mode–2 product of a third order tensor 123LL LR××∈X  and a matrix 
22LLUR′×∈  results in a new tensor 123LL LR′××∈Y . 
 
To simplify the notation in this thesis, we denote 
11 2 2
1kM
M Mk
kUU U U×
=××× × ∏ XX L   (3.07)
and 
11 1 1 1 1
1;dM
ii ii M M d i i
dd iUU U U U U−− ++ ×
=≠×× × × × × = × ∏ XX XLL   . (3.08)
Definition 3.5 (Frobenius Norm for Tensor)  The Frobenius norm of a tensor 
12 M LL LR×××∈XL is given by  
() () ()1
1
12
11;1 : 1 : .M
M
MLL
ll Fro
llMM××
===⊗ = ∑∑ XX X XL   L     (3.09)
The Frobenius norm of a tensor X measures the “size” of the tensor and its 
square is the “energy” of the tensor. 
Definition 3.6 (Rank–1 tensor) An M–th order tensor X has rank one if it is the 
tensor product of M vectors iL
iuR∈r, where 1 iM≤≤ : 74 12
1M
M k
kuu u u⊗
==⊗⊗⊗ = ∏ Xrrr rL . (3.10)
The rank of an arbitrary M–th order tensor X, denoted by () rankR= X, is the 
minimum number of rank–1 tensors that yield X in a linear combination. 
Definition 3.7 ( d–rank)  The d–rank of a tensor X, represented by 
() rankddR= X, is the dimension of the vector space generated by the mode– d 
matricizing. 
Based on the definition of the d–rank of a tensor X, we have 
() ()()() rank rank mat rankdd d dR X == = XX . (3.11)
The higher–order singular value decompos ition (HOSVD) of a tensor is very 
important in tensor algebra. Most of the algorithms relevant to tensor algebra are 
based on HOSVD. HOSVD is a higher–orde r extension of the singular value 
decomposition (SVD) of a matrix. Th e HOSVD of a third order tensor 
123LL LR××∈X  is shown in Figure 3.7. 
 
123LL LR××∈X123LL LR××∈Y 11
1LLUR×∈
22
2LLUR×∈1×
2×33
3LLUR×∈
3×
 
Figure 3.7. The HOSVD of a third order tensor 123LL LR××∈X . 
 
Theorem 3.1 (Higher–Order Singular Value Decomposition) [76] 
A tensor 12 M LL LR×××∈XL can be decomposed as the product of a tensor 
12 M LL LR×××∈YL with a series of orthogonal matrices kkLL
kUR×∈ , i.e., 75 1kM
k
kU×
==∏ XY , (3.12)
such that, the subtensor of 11 1 kkM
kLLLL
l Rα−+×× ×
=∈ YL, obtained by fixing the kth 
(1kklL≤≤ ) index to α, is orthogonal to 11 1 kkM
kLLLL
l Rβ−+×× ×
=∈ YL, i.e., 
()() ;1 : 1 1 : 1 0
kkll MMαβ==⊗ −− = YY      . (3.13)
when αβ≠. 
Finally, 
12 0
kk k kll l LFro Fro Fro== =≥≥ ≥ ≥ YY Y L . (3.14)
Proof: See Appendix. 
 
Table 3.1. 
Alternating Projection for the Higher–Order Singular Value Decomposition. 
Input: The input tensor 12 M LL LR×××∈XL. 
Output: Projection matrices { } 1|,kkLL M
kk kUU R×
=∈  and the tensor 12 M LL LR×××∈YL. 
1. Initialization: Set 1|M
kkU= be equal to random orthogonal matrices.  
2. Conduct the following steps iteratively until convergence. 
3. For 1k= to M 
4. Calculate T
kkU=×TX ; 
5. Mode– d matricize T as ()kkLL
kTR×∈  
(11 1 kk k ML LL L L−+ =× × ×L ); 
6. Calculate ()()T
kk ST T= , kkllSR×
+∈ . 
7. Update the mode– k projection matrix kU by conducting SVD on 
S: T
kk k SU U=Σ . 
8. End 
9. Convergence checking: if () ( )1T
kt ktFroUU I ε−−≤ (610ε−= ) for all 
modes, the calculation has converged. Here ()ktU  is the current optimized 
projection matrix and ()1ktU− is the previous projection matrix. 
10. Calculate the output tensor 
1kM
T
k
kU×
==∏ YX . 
 76 There is no closed form solution for HOSVD. In this thesis, an alternating 
projection method is used to obtain a solution for HOSVD, as shown in Table 3.1.  77  The Relationship betwe en LSM and MLSM 
Suppose: 1) we have a dimension reduction algorithm 1A, which finds a sequence 
of linear transformation matrices iiLL
iUR′×∈  (iiLL′<, 1iM≤≤ ) to transform a 
large tensor 12 M LL LR××∈XL to a smaller tensor 12
1M LL LR′′ ′××∈YL, i.e., 
11 1 2 2TT T
M M UU U=× × ××YX L ; and 2) we have another dimension reduction 
algorithm 2A, which finds a linear transformation matrix LLUR′×∈  
(12 M LLL L=×× L  and 12 M LL L L′′′ ′=××L ; iiLL′< ) to transform a high 
dimensional vector () vectx= Xr to a lower dimensional vector ()22vecty= Yr, 
i.e., 2TyU x=rr, where () vect⋅ is the vectorization operator; LxR∈r and 
2LyR′∈r. According to [181], we know 
()
()
() ( )11
11 2 2
12vect
   vect
   vect .TT T
MM
T
My
UU U
UU U=
=× × × ×
=⊗ ⊗ ⊗Y
X
Xr
L
L (3.15)
Therefore, if 12 M UU U U=⊗⊗⊗ L , 21yy=rr. That is the algorithm 1A is equal 
to the algorithm 2A, if the linear transformation matrix LLUR′×∈  in 2A is 
equal to 12 M UU U U=⊗⊗⊗ L11. 
The tensor representation helps to reduce the number of parameters needed to 
model data. In 1A, there are 1 1M
ii iNL L=′ =∑  parameters. While in 2A, there are 
2 11MM
ii iiNL L==′ =∏∏  parameters. In statistical lear ning, we usually require that 
the number of the training samples is larger than the number of parameters to 
model these training samples for linear algorithms. In the training stage of the 
MLSM based learning algorithms, we usually use the alternating projection 
method to obtain a solution, i.e., the linear projection matrices are obtained 
independently, so we only need about {}0maxiiiNL L ′ =  training samples to 
obtain a solution. However, 2N training samples are required to obtain a 
                                                 
11 In (3.15) , we conduct the reshape operation on 12 M UU U=⊗⊗⊗U L . That is, originally U 
lies in 112 2 M M LLL L L LR′′ ′××××× × L and after the reshape operation U is transformed to V in 
() ()12 12 MM LL L LL LR′′ ′ ××× ××××LL. Then, we can apply the transpose operation on V. 78 solution for LSM based learning algorithms. That is the MLSM based learning 
algorithms require a much smaller number of training samples than LSM based 
learning algorithms, because 02NN  . Therefore, the tensor representation helps 
to reduce the small sample size (SSS)  problem described in Chapter 2. 
There is a long history of methods to reduce the number of parameters needed to model the data by introducing constraints. Take the methods in Gaussian 
distribution estimation as an example
12: when the data consist of only a few 
training samples embedded in a high dimensional space, we always add some constraints to the covariance matrix, for example by requiring the covariance matrix to be a diagonal matrix. Therefore, to better characterize or classify natural 
data, a scheme should preserve as many as possible of the original constraints. 
When the training samples are limited, these constraints help to give reasonable 
solutions to classification problems. 
Based on the discussions above, we have the following results:  
1) when the number of the training samp les is limited, the vectorization operation 
always leads to the SSS problem. That is, for a small size training set, we need to 
use the MLSM based learning algorithms, because the LSM based learning 
algorithms will over
–fit the data. The vectorization of a tensor into a vector 
makes it harder to keep track of the information in spatial constraints. For 
example, two 4–neighbor connected pixels in an image may be widely separated 
from each other after a vectorization; 
2) when the number of training samples is large, the MLSM based learning algorithms will under–fit the data. In this case, the vectorization operation for the 
data is helpful because it increases the number of parameters available to model 
the data. 
 
                                                
 
12 Constraints in MLSM are justified by the form of the data. However, constrains in the example 
are ad hoc. 79  Tensor Rank One Analysis 
To study the tensor rank one analysis (TR1A) [132]13, we first introduce the best 
rank one approximation to a general tensor. That is, given an M–th order tensor 
12 M LL LR××∈XL, find a scalar λ  and a series of unit vectors 1|M
kku=r to minimize 
the function f defined by 
()2
1
1|M
M
kk k
k Frofu u λ=⊗
==−∏ Xrr, (3.16)
where 
1kM
T
k
ku λ×
==∏Xr. 
The Figure 3.8 shows the best rank one approximation of a third order tensor 
123LL LR××∈X . 
 
123LL LR××∈X 1
1LuR∈r
⊗
⊗
⊗
Rλ∈
 
Figure 3.8. The best rank one approximation to a third order tensor 123LL LR××∈X . 
 
Theorem 3.2 [77] Minimizing ()2
1
1|M
M
kk k
k Frofu u λ=⊗
==−∏ Xrr (
1kM
T
k
ku λ×
==∏Xr) is 
equivalent to maximizing  
()2
1
1|
kM
MT
kk k
k Frogu u=×
==∏Xrr. (3.17)
Moreover, 2
Frof g=−X . 
Proof: See Appendix. 
                                                 
13 TR1A is an extension of the original algorithm, which only accepts matrices as input. 80  
Based on (3.17), a solution of (3.16) can be obtained by the alternating projection 
method, listed in Table 3.2. 
 
Table 3.2. Alternating Projection for the Best Rank One Approximation 
Input: The input tensor 12 M LL LR××∈XL. 
Output: Projection vectors { } 1|,kL M
kk kuu R=∈rr and the scalar λ, such that 
() 1|M
kkfu=r is minimized. 
1. Initialization: Set 1|M
kku=r be equal to random unit vectors.  
2. Conduct the following steps iteratively until convergence. 
3. For 1k= to M 
4. Calculate T
kk vu=×Xrr; 
5. Assignment: vλ←r, /kuvλ=rr. 
6. End 
7. Convergence checking: if ()( )11T
kt ktuu ε−−≤rr (610ε−= ) for all modes, the 
calculated kur has converged. Here ()ktur is the current optimized 
projection vector and ()1ktu−r is the previous projection vector. 
 
TR1A is defined based on (3.16) for a number of samples 12 M LLL
iR××∈XL, 
1iN≤≤  by minimizing the total reconstruction error through 1|M
iiu=r and 1|N
iiλ=, 
i.e., 
()2
1
1 1|M N
M
kk i i k
i k Frofu u λ=⊗
= ==−∑∏ Xrr, 
1kM
T
ii k
ku λ×
==∏Xr. (3.18)
Minimizing (3.18) is equivalent to maximizing 
()2
1
1 1|
kM N
MT
kk i k
i k Frogu u=×
= ==∑∏Xrr. (3.19)
In practical applications, the total reconstruction error (3.18) is not small enough, 
so we need to use a number of rank one tensors 1
,1|rR
kr k Mu≤≤
≤≤r and a number of 
scalars 1
,1|rR
ir i Nλ≤≤
≤≤ to approximate the original tensors iX to minimize the 
function f defined by 81 ()2
11
,1 ,1 , ,
11 1|,|M NR
rR rR
ir i N kr k M i ir kr
ir k Frofu uλλ≤≤ ≤≤
≤≤ ≤≤ ⊗
== ==−∑∑∏ Xrr. (3.20)
There is no closed form solution for (3.20). A solution, listed in Table 3.3, based 
on the alternating projection is suggested by Shashua and Levin [132]. 
(Note: The algorithm is based on the fact: minimizing 2
1 1M N
ii k
i k Frouλ⊗
= =−∑∏ Xr is 
equivalent to maximizing2
1 1kM N
T
ik
i k Frou×
= =∑∏Xr.) 
Table 3.3. Alternating Projection for the Tensor Rank One Analysis 
Input: The input tensors 12 M LL L
iR××∈XL, 1iN≤≤ and the number of rank one 
tensors R. 
Output: Projection vectors { }1
,1 ,|,kL rR
kr k M kruu R≤≤
≤≤∈r r and scalars {1
,1|,rR
ir i Nλ≤≤
≤≤
} ,irRλ∈ , such that (3.20) is minimized. 
1. For 1 r= to R 
2. Set ,1|M
kr ku=r be equal to random unit vectors. 
3. Conduct steps 4–9 iterativ ely until convergence.  
4. For 1k= to M 
5. Calculate ,T
ii k k rvu=×Xr r, 1in≤≤; 
6. Calculate the eigenvector hr
 of 1
1n
ii
iVv v
==⊗∑rr associated with 
the largest eigenvalue; (This step is equivalent to calculating the 
left singular vector hr
 of [ ]21 2 ,, ,n Vv v v=rrrL  associated with the 
largest singular value.) 
7. Assignment: ,kruh←rr; 
8. End 
9. Convergence checking: if ,, ,, 1 1T
krt krtuu ε−−≤rr (610ε−= ) for all indices, 
the calculated ,krur has converged. Here ,,krtur is the current optimized 
projection vector and ,, 1krtu−r is the previous projection vector. 
10. Assignment: ,,
1kM
T
kr i kr
ku λ×
=←∏Xr (1iN≤≤); 
11. Assignment: ,,
1M
i i kr kr
kuλ⊗
=←−∏ XXr (1iN≤≤); 
12. End 82  
Based on the procedure to obtain a solution for TR1A listed in Table 3.3, we can 
calculate the scalars 1|R
rrλ= for any given tensor X through the following 
iterative procedure: 1). Calculate ,
1kM
T
rk r
ku λ×
==∏Xr; 2) Assign 
,
1M
rk r
kuλ⊗
=←−∏ XXr, 1 rR≤≤. Based on TR1A, a general tensor is represented 
by a sequence of rank one approximations. 
 83  General Tensor Analysis 
 
General tensor analysis (GTA) [162] is a multidimensional extension of the 
principal component analysis (PCA). Before we describe GTA, the best 
( )12 rank , , ,M RRR−L  approximation is introduced. The procedure is similar to 
the best rank one approximation. 
Definition 3.8 [77] (Best Rank– ( )12,,M RRRL  Approximation)  Given a tensor 
12 M LL LR××∈XL, the tensor 12 ˆ M LL LR××∈XL with ()()ˆ rank matdd R=X , 
1dM≤≤ , which minimizes the least square cost ˆ
Fro−XX , is the best rank–
( )12,,M RRRL  approximation of the original tensor X. 
The Figure 3.9 shows the best ()123 rank , , RRR−  approximation of a third order 
tensor 123LL LR××∈X . 
 
123LL LR××∈X123RRRR××∈Y
11
1LRUR×∈
22
2LRUR×∈1×
2×33
3LRUR×∈
3×
 
Figure 3.9. The best ()123 rank , , RRR−  approximation of a third order tensor 
123LL LR××∈X . 
 
Theorem 3.3  [75] Given a sequence of unitary matrices 'kkLL
kUR×∈  (1kM≤≤  
and ddLL′< ) and a tensor 12 M LL LR××∈XL, the function ()2ˆˆ
Frof=−XX X  is 
minimized, where ()ˆ rankdd L′=X  and 12 ˆ M LL LR××∈XL 
()
11 1ˆ
kk kMM M
TT
kk k k
kk kUU U U×× ×
== =⎛⎞==⎜⎟⎝⎠∏∏ ∏ XX X . (3.21)
Proof: See Appendix. 84 Theorem 3.4  [75] For a given tensor 12 M LL LR××∈XL, minimizing 
() ()2
1
1|
kM
MT
kk k k
k FrofU U U=×
==−∏ XX  is equivalent to maximizing 
()2
1
1|
kM
MT
kk k
k FrogU U=×
==∏X . (3.22)
Proof: See Appendix. 
 
Table 3.4. 
Alternating Projection for the Best Rank –( )12,,M RRRL  Approximation. 
Input: The input tensors 12 M LL LR××∈XL and the d–ranks {}1|M
ddL=′ . 
Output: Projection matrices { } 1|,kkLL M
kk kUU R′×
=∈  and the tensor 12 M LL LR′′ ′×××∈YL, 
such that (3.22) is maximized. 
1. Initialization: Set 1|M
kkU= be equal to random matrices and each column of 
1|M
kkU= is a unit vector.  
2. Conduct the following steps iteratively until convergence. 
3. For 1k= to M 
4. Calculate T
kkU=×TX ; 
5. Mode– d matricize T as ()kkLL
kTR×∈  
(11 1 kk k ML LL L L−+ =× × ×L ); 
6. Calculate ()()T
kkST T= , kkLLSR×
+∈ . 
7. Update the mode– k projection matrix kU by the first kL′ columns 
of the left singular vectors of S associated with the kL′ largest 
singular values; 
8. End 
9. Convergence checking: if ,1 ,T
Lt LtFroUU I ε−−≤ (610ε−= ) for all modes, 
the calculated kU has converged. Here ,ktU is the current optimized 
projection matrix and ,1ktU− is the previous projection matrix. 
10. Calculate the core tensor 
1kM
T
k
kU×
==∏ YX . 
 85 There is no closed form solution for the best rank –( )12,,M RRRL  approximation. 
The alternating projection method could be  used to estimate this approximation 
based on (3.21) and (3.22) as listed in Table 3.4.  
Table 3.5. Alternating Projection for General Tensor Analysis. 
Input: The input tensor 12 M LL L
iR××∈XL, 1iN≤≤, and the d–ranks {}1|M
ddL=′ . 
Output: Projection unitary matrices { } 1|,kkLL M
kk kUU R′×
=∈  and the tensor 
12 M LL L
iR′′ ′×××∈YL, such that (3.23) is minimized. 
Initialization: Set 1|M
kkU= be equal to random matrices and each column of 
1|M
kkU= is a unit vector. 
1. Conduct the following steps iteratively until convergence. 
2. For 1k= to M 
3. Calculate T
ii k k U=×TX , 1iN≤≤; 
4. Mode– d matricize iT as ();kkLL
ikTR×∈ , 1iN≤≤, 
(11 1 kk k MLL L L L−+ =× × ×L ); 
5. Calculate () ();;
1N
T
ik ik
iST T
==∑ , kkLLSR×
+∈ . 
6. Update the mode– k projection matrix kU by the first kL′ columns 
of the left singular vectors of S associated with the kL′ largest 
singular values; 
7. End 
8. Convergence checking: if ,1 ,T
kt ktFroUU I ε−−≤ (610ε−= ) for all modes, 
the calculated kU has converged. Here ,ktU is the current optimized 
projection matrix and ,1ktU− is the previous projection matrix. 
9. Calculate the core tensor 
1kM
T
ii k
kU×
==∏ YX . 
 
GTA is defined based on (3.21). Given a number of samples 12 M LLL
iR××∈XL, 
1iN≤≤ , GTA minimizes the function f defined by 
()2
1
1 1|
kM N
M
kk i i k
i k FrofU U=×
= ==−∑∏ XY , (3.23)86 where 
1kM
T
ii k
kU×
==∏ YX . 
Based on Theorem 3.4, minimizing (3.23) is equivalent to maximizing 
()2
1
1 1|
kM N
MT
kk i k
i k FrogU U=×
= ==∑∏X . (3.24)
 
There is no closed form solution to (3.23). A solution, listed in Table 3.5, based 
on the alternating projection is similar to Table 3.3.  
(Note: The algorithm is based on the fact: minimizing 2
1 1kM N
ii k
i k FroU×
= =−∑∏ XY  is 
equivalent to maximizing 2
1 1kM N
T
ik
i k FroU×
= =∑∏X .) 
 87  Two Dimensional Linear D iscriminant Analysis 
 
The major difference between LDA and 2DLDA is the representation of the 
training samples. In LDA, the training samples are represented by vectors, while 
they are represented as matrices in 2DLDA. 
Suppose: the training samples are divided into C classes and the ith (1iC≤≤ ) 
class contains Ni samples 12
;LL
ijXR×∈  (1i jN≤≤ ). The mean matrix of all 
training samples is ( )() ; 11 1i CN C
ij i ij iM XN== ==∑∑ ∑  and the mean matrix of the 
ith class samples is (); 1iN
ii j i jM XN==∑ . The class structure in the high 
dimensional space is defined by 
()() ()
()() ();;
11
1tr
tr ,iN CTH
wi j i i j i
ij
CT H
bi i i
iDX M X M
D N MM MM==
=⎛⎞=− −⎜⎟
⎝⎠
⎛⎞=− −⎜⎟⎝⎠∑∑
∑ (3.25)
The class structure in the low dimensional space is defined by 
()() ()
()() ();;
11
1tr
tr ,iN CTL TT
wi j i i j i
ij
CT L TT
bi i i
iD U XM V V XM U
DN U M M V V M M U==
=⎛⎞=− −⎜⎟
⎝⎠
⎛⎞=− −⎜⎟⎝⎠∑∑
∑ (3.26)
The 2DLDA finds two projection matrices 11LLUR′×∈  and 22LLVR′×∈  to 
preserve the class structure of the original high dimensional space in the projected 
low dimensional space by maximizing ()()()1LL
wbDD−
. Ye et al. [173] developed 
the following alternating projection method to obtain a solution of 2DLDA. The 
algorithm conducts the following two steps iteratively: 
Step 1: calculate U, which is the 1L′ eigenvectors of 1
wbAA− associated with the 
largest 1L′ eigenvalues, with the given V (it is a random matrix and each 
column is a unit vector), where wA and bA are defined by 88 () ()
() ();;
11
1.iN CTT
wi j i i j i
ij
CT T
bi i i
iAX M V V X M
A N MM V VMM==
==− −
=− −∑∑
∑ (3.27)
Step 2: calculate V, which is the 2L′ eigenvectors of 1
wbBB− associated with the 
largest 2L′ eigenvalues, with the given U (obtained in Step 1), where wB and 
bB are defined by 
() ()
() ();;
11
1.iN CTT
wi j i i j i
ij
CT T
bi i i
iB XM U U XM
B N MM U UMM==
==− −
=− −∑∑
∑ (3.28)
The 2DLDA reduces the SSS problem and it is more efficient than LDA in terms 
of time complexity and space complexity. The drawback of 2DLDA is the alternating projection algorithm to obtain a solution of 2DLDA does not converge. 
Therefore, the recognition accuracies are not stable over different training 
iterations. For example, in the Probe A of  the human gait recognition task in this 
Chapter, the rank one recognition rate changes from 75% to 91% in the first 100 
training iterations. This is be cause the projection matrices 
U and V do not 
maximize ()()()1LL
wbDD−
. According to steps 1 and 2, the projection matrix U is 
the 1L′ eigenvectors of 1
wbAA− associated with the largest 1L′ eigenvalues, 
which is obtained with the given V by maximizing ()( )1trTT
wb UA U UA U− 
while the projection matrix V is the 2L′ eigenvectors of 1
wbBB− associated with 
the largest 2L′ eigenvalues, which is obtained with the given U by maximizing 
()( )1trTT
wb UB U UB U−. The projection matrices U and V, obtained by 
iteratively conducting steps 1 and 2, do not maximize ()()()1LL
wbDD−
, because 
maximizing the trace ratio between two projected matrices does not be equal to 
maximizing the ratio trace between two projected matrices [103], i.e., with the 
given wA and bA, the argument of maximum ()( )1trTT
wb UA U UA U− by 
varying U does not maximize ()() tr trTT
bw UA U UA U  and with the given wB 89 and bB, the argument of maximum ()( )1trTT
wbVB V VB V− by varying V does 
not maximize ()() tr trTT
bwVB V VB V . 
To deal with the nonconvergence problem and to accept general tensors as input 
for discriminative dimension reduction, we propose GTDA in the next Section. 
 90  General Tensor Discriminant Analysis 
 
Although the effectiveness and the efficiency of 2DLDA, as a preprocessing for 
subsequent classification, e.g., by LDA, have been proved, 2DLDA fails to 
converge in the training stage. The nonconvergence shadows the advantages of 
2DLDA, because: 1) it is difficult to de termine when to stop the training stage; 
and 2) different numbers of training iter ations will lead to different recognition 
results. To solve the nonconvergence problem, we develop GTDA, which 1) 
provides a converged alternating projecti on algorithm for training; 2) accepts 
general tensors as input; 3) preserves the discriminative information for classification; and 4) reduces the SSS pr oblem in the subsequent classification, 
e.g., by LDA. The proposed GTDA is based on the differential scatter 
discriminant criterion (DCDS). 
 
 Differential Scatter Discr iminant Criterion (DSDC) 
The Differential Scatter Discriminant Criterion (DSDC) [39][149][143][145] is 
defined by, 
()() ( ) *a r g m a x t r t r
TTT
bw
UU IUU S U U S U ζ
==− , (3.29)
where ζ is a tuning parameter; LLUR′×∈   ( LL′<) , constrained by 
TUU I=, is the projection matrix; and bS and wS are defined in (2.10). 
According to [39] (pp. 446 –447), the solution to (3.29) is equivalent to the 
solution to (12) for some special ζ in (3.29). If we extract only one feature, i.e., 
U degenerates to a vector, then ()1
max wbSSζλ−= , which is the maximum 
eigenvalue of 1
wbSS−. If we want to extract L′ features simultaneously, we 
estimate ζ as 1L
i iλ′
=∑ , where 1|L
iiλ′
= are the largest L′ eigenvalues of 1
wbSS−. 
From [39] (pp. 446 –447), it is not difficult to show that a suitable ζ in (3.29) is 
()() tr trTT
opt b opt opt w optUS U US U14. An accurate solution of (3.29) can be obtained by 
                                                 
14 The derivative of () () tr trTT
bwUS U US U ζ−  with respect to U is given by bwSU SUζ− . 
To obtain a solution of (3.29), we need to set 0bwSU SUζ−= (as we have a strict condition here, 91 an alternating projection method. Here, we use the approximation (on setting ζ 
as the maximum eigenvalue of 1
wbSS−) in (3.29) to avoid the alternating projection 
method for optimization. 
In realworld applications, because the di stribution of testing samples diverges 
from the distribution of training samples, a manually chosen value of ζ always 
achieves better prediction results than the calculated value. However, the manual 
setting of ζ is not practical for real applications, because we do not know the 
best choice of ζ for classification. In this thesis, ζ is selected automatically 
during the training procedure. 
 
 General Tensor Di scriminant Analysis 
On defining bS and wS by (2.10), it follows from (3.29) that 
()() ( ) arg max tr tr
TTT
bw
UU IUU S U U S U ζ∗
==−  
()() ()() () ()
()() ()() () ()11
1
;1 ;1
11;1 1
   arg max
;1 1i TC
TT
ii i
i
N CUU I TT
ij i ij i
ijNm m U m m U
xmU xmU ζ=
=
==⎛⎞−× ⊗ −×⎜⎟
⎜⎟=⎜⎟−− × ⊗ − ×⎜⎟
⎝⎠∑
∑∑rr rr      
rr rr          
() ()2 2
1; 1
11 1   arg maxi
Tn Cc
TT
ii i j iFro FroUU I ii jNmm U x m U ζ
=== =⎛⎞=− × − − ×⎜⎟
⎝⎠∑∑ ∑rr r r (3.30)
where Fro⋅ is the Frobenius norm and the projection matrix LLUR′×∈  (LL′<) is 
constrained by TUU I=. 
Deduction 3.1 : See Appendix. 
Let ;ijX denote the jth training sample (tensor) in the ith individual class, 
(); 11iN
ii i j jN==∑ MX  is the mean tensor of the samples in the ith class, 
()11C
ii iNN==∑ MM  is the mean tensor of all training samples, and kU 
denotes the kth direction projection matrix for decomposition in the training 
procedure. Moreover, 1
;1|ijN
ij iC≤≤
≤≤ X , 1|C
ii=M , and M are all M–th order tensors in 
                                                                                                                                      
i.e., () 0bw kSS uζ−= , kuU∀∈ , ku is a column vector in U). Consequently, we have 
() ()Tr TrTT
opt b opt opt w optUS U US U ζ= , where optU is a solution to (3.29). 92 12 M LL LR×××L. Based on (3.30), we analogously define GTDA by replacing ;ijxr, 
imr, and mr with ;ijX, iM, and M, respectively, as: 
()
()() ()
()
()() ()11
1
1
1
|
;
1
11
;
1;1 : 1 :
|a r g m a x
;1 : 1 :k
k
TM
kk k
ki
kM
T
ikCk
iMi T
ik
kM
llMUU IT
ij i kN Ck
Mij T
ij i k
kU
NM M
U
U
U
MM
Uζ=×
=
=
×
=∗
=
=
×
=
==
×
=⎛⎛⎞− ⎜⎜⎟⎝⎠⎜
⎜⎛⎞⎜⊗−⎜⎟⎜ ⎝⎠=⎜⎛⎞ ⎜−⎜⎟ ⎜⎝⎠⎜−
⎛⎞⊗−⎜⎟⎝⎠ ⎝∏
∑
∏
∏
∑∑
∏MM
MM
XM
XM                    
                              ⎞
⎟
⎟
⎟
⎟⎟
⎟
⎟⎟
⎟
⎜⎟⎜⎟
⎠. (3.31)
According to the definition of the Frobenius norm of a tensor in (3.09), the right 
hand side of (3.31) can be simplified as 
() ()
11
22
;
| 11 1 11|
arg max .i
kkTM
kk kM
ll
N MM CC
TT
ii k i j i k
UU I ii j kk Fro FroU
NU U ζ
=∗
=
××
= == = ==⎛⎞
=− − −⎜⎟⎜⎟⎝⎠∑∑ ∑ ∏∏ MM X M (3.32)
There is no closed form solution to (3.31), so we choose to use an alternating 
projection method, which is an iterative procedure, to obtain a numerical solution. 
Therefore, (3.31) is decomposed into M different optimization sub–problems, as 
follows, 
()
()() ()
()
()() ()1
1
1
;
1
11
;
1;1 : 1 :
arg max
;1 : 1 :k
k
T
ll
ki
kM
T
ikCk
iMi T
ik
k
lMUU IT
ij i kN Ck
Mij T
ij i k
kU
NM M
U
U
U
MM
Uζ×
=
=
×
=∗
=
×
=
==
×
=⎛⎞⎛⎞− ⎜⎟⎜⎟⎝⎠⎜⎟
⎜⎟⎛⎞⎜⎟⊗−⎜⎟⎜⎟ ⎝⎠=⎜⎟⎛⎞ ⎜−⎜⎟ ⎜⎝⎠⎜−
⎜ ⎛⎞⊗− ⎜ ⎜⎟⎝⎠ ⎝⎠∏
∑
∏
∏
∑∑
∏MM
MM
XM
XM                    
                              ⎟
⎟
⎟
⎟⎟ 
()( )
()()
()()
()()1
;
11;mat
mat
    arg max tr
mat
matT
ll iTCil i l l
TTili l l
T
llTUU I N Cli j i l l
TTijli ji l lnU
U
UU
U
Uζ=
=
==⎛⎞⎛⎞⎡⎤ −×⎜⎟⎜⎟⎢⎥⎜⎟⎜⎟⎢⎥ −×⎣⎦⎜⎟⎜⎟=⎜⎟⎜⎟⎡⎤ −×⎜⎟⎜⎟⎢⎥−⎜⎟⎜⎟⎢⎥⎜− × ⎟⎜⎟⎣⎦ ⎝⎠⎝⎠∑
∑∑MM
MM
XM
XM. (3.33)
Deduction 3.2 : See Appendix. 93  
To simplify (3.33), we define 
()() ()()
1mat matC
TT T
l i l i ll l i ll
iBN U U
=⎡⎤=− × − ×⎣⎦∑ MM MM , (3.34)
and 
()() ()() ;;
11mat matiN C
TT T
l l ij i l l l ij i l l
ijWU U
==⎡⎤=− × − ×⎣⎦∑∑ XM XM . (3.35)
Therefore, (3.33) is simplified as, 
() ( )
11
|*| argmaxtr
TM
ll lMT
ll l l l l
UU IUU B W U ζ
==
==− . (3.36)
As pointed out in 0, ζ is the tuning parameter. 
Table 3.6 lists the alternat ing projection method based optimization procedure for 
GTDA with the given tuning parameter ζ to simplify the proof of the 
convergence theorem. Later, we describe  how to determine the tuning parameter 
ζ and the dimension 12 M LL L′′′××L  of the output tensors automatically. The 
key steps in the alternating projection procedure are Steps 3 –5, which involve 
finding the lth mode projection matrix ,ltU in the tth iteration using 1
,1|l
kt kU−
= 
obtained in the tth iteration and ,1 1|M
kt k lU−=+ obtained in the ( 1t−)th iteration. In 
Steps 3 and 4, we obtain the between class scatter matrix ,1ltB− and the within 
class scatter matrix ,1ltW− with 1
,1|l
kt kU−
= obtained in the tth iteration and 
,1 1|M
kt k lU−= +  obtained in the ( 1t−)th iteration. The singular value decomposition 
(SVD) of ,1 ,1lt ltBWζ−−−  is obtained and ,1ltU− is updated using the eigenvectors 
of ,1 ,1lt ltBWζ−−− , which correspond to the largest eigenvalues of ,1 ,1lt ltBWζ−−− . 
According to the algorithm described in Table 3.6, we can obtain a solution 
1|kkLL M
kkUR′×
=∈  (kkLL′<) by an iterative way. For GTDA, we use the projected 
tensor 
1kM
T
k
kU×
==∏ YX  to represent the original general tensor X. Unlike 
2DLDA [173], the alternating projection  method based optimization procedure for 
GTDA converges, as proved in Theorem 3.5. 
Theorem 3.5  The alternating projection method based optimization procedure for 
GTDA converges. 
Proof: See Appendix. 94  
Table 3.6. Alternating Projection for General Tensor Discriminant Analysis 
Input: Training tensors 12 1
;1|i M jN LL L
ij iC R≤≤ ×××
≤≤∈ XL, the dimension of the output 
tensors 12
;M LL L
ijR′′ ′×××∈YL, the tuning parameters ζ, and the maximum number of 
training iterations T. 
Output: The projection matrix 1|kkLL M
kkUR′×
=∈  (T
kkUU I=) and the output tensors
12 1
;1|i M jN LL L
ij iC R′′ ′ ≤≤ ×××
≤≤∈ YL. 
1. Initialize: Set ,0 1|kkLL M
kkUR′×
=∈  be equal to random matrices and each 
column of ,0 1|M
kkU= is a unit vector. 
2. For 1t= to T{ 
3. For 1l= to M { 
4. Calculate ()( )
()(),1
,1
1,1mat
matTCil i l l t
ltTTili l l tNU
B
U−
−
=−⎡ ⎤ −×
⎢ ⎥ =
⎢ ⎥ −×⎣ ⎦∑MM
MM 
5. Calculate ()( )
()();, 1
,1
11;, 1mat
matiTN Cli j i l l t
ltTTijli ji l l tU
W
U−
−
==−⎡ ⎤ −×⎢ ⎥ =⎢ ⎥−×⎣ ⎦∑∑XM
XM 
6. Optimize ( ) ( ) ,, 1 , 1arg max tr
TT
lt lt lt
UU IUU B W U ζ−−
==−  by SVD on 
,1 ,1lt ltBWζ−−− . 
7. }//For loop in Step 2. 
8. Convergence check: the training stage of GTDA converges if 
(),, 1 1ErrMT
lt lt ltU U I ε− ==− ≤∑  (610ε−= ). 
9. }// For loop in Step 1. 
10. ;;
1kM
T
ij ij k
kU×
==∏ YX . 
 
For practical applications, it is important to determine the tuning parameter ζ 
and the dimension 12 M LLL′′ ′×××L  of the output tensors automatically. In the tth 
training iteration and the lth order, we adjust ζ after Step 4 and before Step 5 by 
setting tζ being equal to the maximum eigenvalue of 1
,1 ,1lt ltWB−
−−. That is ζ is 
varying from one iteration to the next. In the tth training iteration, the lth dimension 
of the output tensors lL′ is determined by the lth projection matrix ,ltU, so we set 95 a threshold value δ to automatically determine lL′ according to the following 
inequality: 
,1, ,1, ,2,
,, ,, 11
1
,, ,, 11
,, ,, 11
,, 1
,, 1             
                                       1,ll
ll
ll
l
llt lt l t
LL
lj t lj t jj
LL
lit lit ii
LL
lj t lj t jj
L
lit i
L
lj t jλλλ
λλ
λλδ
λλ
λ
λ==
′′ +
==
==
=
=+<<
< ≤< <
<=∑∑
∑∑
∑∑
∑
∑L
L. (3.37)
where ,,litλ is the ith eigenvalue of ,1 ,1lt t ltBWζ− −−  and ,, , ,lit l jtλλ≥  if ij<. 
Therefore, there is only one parameter, the threshold value δ, which needs to be 
tuned for recognition tasks. Without this method as shown in (3.37), we have to 
tune 1M+ parameters, comprising of one parameter for each order of the M–th 
order tensors and ζ in (3.36). This multiparameter tuning is too time consuming 
when M is large. 
 
 Computational Complexity 
The time complexity of LDA is ()3
1M
i iOL=⎛⎞⎜⎟⎝⎠∏  in the training stage, where 
training samples X belong to 12 M LL LR×××L. The time complexity of the 
alternating projection method based optimization procedure of GTDA is 
()3
1M
i iOT L=∑ , where T is the number of iterations required for GTDA to 
converge. The space complexity of LDA is ()2
1M
i iOL=⎛⎞⎜⎟⎝⎠∏  in the training stage. 
The space complexity of the alternatin g projection method based optimization 
procedure of GTDA is ()2
1M
i iOL=∑ . 
 96  Manifold Learning us ing Tensor Representations 
 
The proposed GTDA can also be extended for manifold learning [13], which has 
become popular in machine learning. Manifold learning is based on geometrical 
assumptions, i.e., a data set approximately lies in a low dimensional manifold 
embedded in the original high dimensional feature space. Recently, a large number of algorithms have been developed based on different criteria. For 
example, ISOMAP [155] finds the low di mensional representation for data by 
preserving the geodesic distances of data pairs in the original high dimensional 
space. Locally linear embedding (LLE) [ 124][127] produces the low dimensional 
representation for locally sufficient reconstruction, i.e., a sample is reconstructed 
from its neighborhood samples. Laplacian Eigenmap (LE) [5][6]reduces the data 
dimension by preserving the locality character of the samples. Recently, Bengio et 
al. [8] unified a number of manifold learning algorithms together and developed 
the out of sample extension. Yan et al. [172] developed another framework, 
named the graph embedding framework (GEF). GEF finds the projection matrix 
LLUR′×∈  (LL′<) to map the original high dimensional samples LxR∈r to the 
low dimensional space LR′ through a linear projection, i.e., TyU x=rr and 
LyR′∈r. The objective function of GEF is defined by 
()()
()2
11
2
11NN
T
ij i jFroij
NN
T
ij i jFroijbU x x
FU
sU x x==
==−
=
−∑∑
∑∑rr
rr, (3.38)
where ijs and ijb are predefined weighting factors and N is the number of 
training samples. By varying ijs and ijb, different dimension reduction 
algorithms can be obtained, e.g., LLE, ISOMAP, and LDA. The linear projection 
matrix LLUR′×∈  is obtained by maximizing () FU  subject to the constraint 
TUU I=. To extend GEF to accept general tensors as input, we reformulate 
(3.38) as 97 () () ()
() ( )
() ( )
()()()
()()() ()22
11 11
2
11
2
1
11
1
1         
         
         ; 1 1nn nn
TT
ij i j ij i jFro Froij ij
nn
T
ij ij i jFroij
nn
T
ij ij i jFroij
T
ij
ij ijTjijGU b U x x s U x x
bs U x x
bs x x U
xx U
bs
xx Uζ
ζ
ζ
ζ== ==
==
===− − −
=− −
=−− ×
−×
=−
⊗−×∑∑ ∑∑
∑∑
∑∑rr rr
rr
rr
rr        rr       11,nn
i==∑∑ (3.39)
where ζ is a tuning parameter and the linear projection matrix U is obtained 
by maximizing ()GU  constrained by TUU I=. 
Based on (3.33), we analogously define tensorized GEF by replacing ixr with 
iX as 
() ()()
()() ()1
1
11
1| ; 1: 1: ,k
kM
T
ij kNNk M
k k ij ijMij T
ij k
kU
HU b s M M
Uζ×
=
=
==
×
=⎛⎞−⎜⎟⎝⎠=−
⎛⎞⊗−⎜⎟⎝⎠∏
∑∑
∏XX
XX                               (3.40)
where ζ is a tuning parameter and a series of linear projection matrices 1|M
kkU= 
is obtained by maximizing ()1|M
kk HU= constrained by T
kkUU I=  for 
1kM≤≤ . According to (3.09), the tensorized GEF can be defined as 
()
() ()1
111
|
2
| 11 1|a r g m a x |
        arg max .TM
kk k
kTM
kk kMM
kk kk
UU I
M NN
T
ij ij i j k
UU I ij k FroUH U
bs Uζ=
=∗
==
=
×
= == ==
⎛⎞
=− −⎜⎟⎜⎟⎝⎠∑∑ ∏ XX (3.41)
Similar to GTDA, there is no closed form solution. Therefore, (3.41) is 
decomposed into M different optimization sub–problems, as follows, 
()()
()() ()
()()()
()()1
11
1
11arg max ; 1: 1:
mat
    arg max tr
matk
T
kk
k
T
kkM
T
ij kNNk
ki j i jMUU I ij T
ij k
k
T
NNki j k kT
ki j i jTTUU I ijkij k kU
Ub s M M
U
U
Ub s
Uζ
ζ×
= ∗
===
×
=
= ==⎛⎞ ⎛⎞− ⎜⎟ ⎜⎟⎝⎠ ⎜⎟=−⎜⎟⎛⎞⎜⎟ ⊗−⎜⎟ ⎜⎟⎝⎠ ⎝⎠
⎛ ⎡⎤ −×⎜ ⎢⎥ =−⎢⎥−×⎣⎦ ⎝∏
∑∑
∏
∑∑XX
XX
XX
XX                              
.kU⎛⎞ ⎞
⎜⎟ ⎟
⎜⎟⎜⎟⎜⎟⎜⎟⎠ ⎝⎠ (3.42)98 With (3.42), a solution to the tensorized GEF can be obtained iteratively by the 
alternating projection method.  99  GTDA for Human Gait Recognition 
 
A straightforward application of LDA to human gait recognition leads to poor 
results because of the SSS problem. For this reason, principal component analysis 
(PCA) [30] is conducted as a preprocessing step to reduce the SSS problem. 
Unfortunately, some discriminative information is discarded by PCA.  In this Section, we apply the proposed GTDA to reduce the SSS problem in LDA 
for appearance based human gait recognition [126]. Appearance [83] is the basic 
stage of visual information representation and reflects the walking manner 
[45][91]. In the following, we use the averaged gait image [45][91] as the appearance model. As a representation method, the effectiveness of the averaged 
gait image based recognition has be en proved in [45][46][91][92]. 
The averaged gait image is decomposed by Gabor functions [25][26][101] and we 
combine the decomposed images to give new representations for recognition. 
There are three major reasons for introducing the Gabor based representation for 
the averaged gait image based recognition: 1) human brains seem to have a special function to process information in multi–resolution levels [25][26][101]; 
2) it is supposed that Gabor functions are similar to the receptive field profiles in 
the mammalian cortical simple cells [25][101]; and 3) Gabor function based 
representations have been successfully  employed in many computer vision 
applications, such as face recognition [ 87][88][89] and texture analysis [32]. 
Although Gabor function based represen tations are effective for object 
recognition [25][26] and image understanding, the computational cost of the 
representation is high. Therefore, thre e variant methods for representation are 
introduced to utilize Gabor functions to reduce the computational cost in calculating the representation and in training subsequent feature selection 
algorithms by partially or fully summing over Gabor functions. The sum operation 
reduces the number of functions, used for decomposing the averaged gait images. 
These methods are: 1) the sum over directions of Gabor functions based 
representation (GaborD), 2) the sum over scales of Gabor functions based 
representation (GaborS), and 3) the sum over  both scales and directions of Gabor 
functions based representation (GaborSD).  100  
 Gabor Gait Representation 
As demonstrated in [45][91], the averaged gait image is a robust feature for gait 
recognition tasks. In Figure 3.10, the sample averaged gait images are obtained 
from different persons under di fferent circumstances. It can be observed that: 1) 
the averaged gait images of the same person under different circumstances share similar visual effects; and 2) the averag ed gait images of different persons, even 
under the same circumstance, are very differe nt. So, it is possible to recognize a 
person by his or her averaged gait images. Furthermore, according to research results reported in [80], Gabor func tions based image decomposition is 
biologically relevant to and is useful  for image understanding and recognition. 
Consequently, it is reasonable to introduce Gabor functions for the averaged gait 
image based gait recognition.  
 
 
 
 
Figure 3.10. The columns show the averaged  gait images of nine different people 
in the Gallery of the USF database described in §1. The four rows in the figure 
from top to bottom are based on images taken from the Gallery, ProbeB, ProbeH, 
and ProbeK, respectively. The averaged gait images in a single column come from 
the same person.  
1. Gabor Functions 
 101 Marcelja [101] and Daugman [25][26] modell ed the responses of the visual cortex 
by Gabor functions, because they are similar to the receptive field profiles in the mammalian cortical simple cells. Daugman [25][26] developed the 2D Gabor 
functions, a series of local spatial ba ndpass filters, which have good spatial 
localization, orientation selectivity, an d frequency selectivity. Lee [80] gave a 
good introduction to image representation using Gabor functions. A Gabor 
(wavelet, kernel, or filter) function is the product of an elliptical Gaussian 
envelope and a complex plane wave, defined as: 
() ( )2 22
22 2
, 2,kz
ik z
sd kk
xy z e e eδ
δψ ψδ⋅
− −⋅⎡ ⎤
== ⋅ ⋅ − ⎢ ⎥
⎢ ⎥ ⎣ ⎦rr
rr
rr
r, (3.43)
where (), zx y=r is the variable in a spatial domain and kr
 is the frequency 
vector, which determines the scale a nd orientation of Gabor functions, di
s kk eφ=r
, 
where maxs
skk f= , max 2 kπ= , 2f=, 0,1, 2,3, 4s= , and 8d dφπ= , for 
0,1, 2,3, 4,5, 6, 7d= . Examples of the real part of Gabor functions are presented in 
Figure 3.11. Here, we use Gabor functions (full complex functions) with five 
different scales and eight different orientations, making a total of forty Gabor 
functions, for the averaged gait image deco mposition. The number of oscillations 
under the Gaussian envelope is determined by 2δπ= . The term ()2exp 2σ−  
is subtracted in order to make the kernel DC–free, and thus insensitive to the 
average illumination. 
 
 
Figure 3.11. The real part of Gabor functions with five different scales and eight 
different directions. 102  
2. Gabor based Gait Representation 
The Gabor function representation of an averaged gait image is obtained by 
convolving the Gabor functions with the averaged gait image. This yields a fourth 
order tensor in 12 58 LLR×× × constructed by filtering an averaged gait image through 
a series of Gabor functions with five sc ales and eight directions. Two indices are 
required for pixel locations: one index is required for scale information, and one 
index is required for direction information.  The entries of the fourth order tensor 
are complex numbers and the magnitude part of this fourth order tensor is defined 
as the Gabor gait as shown in Figure 3.12. In Gabor gait, there are 40 components 
(images), each of which is the magnitude part of the output, which is obtained by 
convoluting the averaged gait image with a Gabor function. 
 
 
Figure 3.12. Gabor gait: the rows show different scales and the columns show 
different directions for an averaged gait image. 
 
The gait representation method in Figure 3.12  is similar to the face representation 
method [87][88] using Gabor functions. Although this method for representation is powerful, its computational costs bot h for recognition and calculation for 103 representation are higher compared with the original image based recognition. 
The computational cost in recognition is described in §3. We introduce three new methods to decompose averaged gait images based on 
Gabor functions defined in (3.43). These are the sum over directions of Gabor 
functions based representation (GaborD), the sum over scales of Gabor functions based representation (GaborS), and the sum over scales and directions of Gabor 
functions based representation (GaborSD). The most important benefit of these 
new representations is that the cost of computing the gait representation based on them is low. The computational cost of the Gabor based representation and the 
complexity analysis for GTDA based dimension reduction for different 
representations are given in §3.  
∑∑
∑
 
Figure 3.13. Three new methods for aver aged gait image re presentation using 
Gabor functions: GaborS, GaborD, and GaborSD. 
 
GaborD is the magnitude part of outputs generated by convolving an averaged 
gait image (),Ix y  with the sum of Gabor functions over the eight directions 
with a fixed scale, 
()()() (),, GaborD , , ,sd sd
ddxyI x y I x y ψψ⎛⎞=∗ = ∗⎜⎟⎝⎠∑∑ , (3.44)104 where (),,sdxyψ  is the Gabor function defined in (3.43); and () GaborD , xy is 
the output of the GaborD method for representation. Therefore, we have five 
different outputs to represent the original gait image through the GaborD 
decomposition. We then put all () GaborD , xy with different scal es together as a 
third order tensor DG in 12 5 LLR××: two indices are required for pixel locations; 
and one index is required for scale change. The calculation procedure is shown in 
Figure 3.13. Examples of GaborD based ga it representation are shown in Figure 
3.14. 
GaborS is the magnitude part of outputs generated by convolving an averaged gait 
image (),Ix y  with the sum of Gabor functions over the five scales with the 
fixed direction, 
()()() (),, GaborS , , ,sd sd
ssxyI x y I x y ψψ⎛⎞=∗ = ∗⎜⎟⎝⎠∑∑ , (3.45)
where (),,sdxyψ  is the Gabor function defined in (3.43), and () GaborS , xy is 
the output of the GaborS method for representation. Therefore, we have eight 
different outputs to represent the original gait image through the GaborS based 
decomposition. We then put all () GaborS , xy with different scales together as a 
third order tensor SG in 12 8 LLR××: two indices are required for pixel locations; 
and one index is required for directi on change. The calculation procedure is 
shown in Figure 3.13. Examples of GaborS  based gait representation are shown in 
Figure 3.14. 
GaborSD is the magnitude part of the output generated by convolving an averaged 
gait image (),Ix y  with the sum of all forty Gabor functions, 
() ()() (),, GaborSD , , ,sd sd
sd sdxyI x y I x y ψψ⎛⎞=∗ = ∗⎜⎟⎝⎠∑∑ ∑∑ , (3.46)
where (),,sdxyψ  is the Gabor function defined in (3.43), and () GaborSD , xy 
is the output of the GaborSD method for representation. Therefore, it is a second 
order tensor in 12LLR×. Two indices are required for pixel locations. The 
calculation procedure is shown in Figu re 3.13. Examples of GaborD based gait 
representation are shown in Figure 3.14.  105  
 
Figure 3.14. The thirteen columns are Gallery gait, ProbeA gait, ProbeB gait, 
ProbeC gait, ProbeD gait, ProbeE gait, ProbeF gait, ProbeG gait, ProbeH gait, ProbeI gait, ProbeJ gait, ProbeK gait, and Pr obeL gait, respectively. From the first 
row to the last row are the original ga it, GaborD (from 0 to 4), GaborS (from 0 to 
7), and GaborSD, respectively. The Gall ery gait and ProbeA – ProbeI gaits are 
described in Section 3.8.2.1. 106  
3. Computational Complexity 
Gabor functions with different scales and directions are approximated by masks of 
size 12GG× (in experiments, 12 64 GG== ) and averaged gait images are in 
12LLR×. Therefore, the computational complexities for generating a Gabor gait in 
12 58 LLR×× × , a GaborD gait in 12 5 LLR××, a GaborS gait in 12 8 LLR××, and a GaborSD 
gait in 12LLR× are ( )12 1 240OL L G G , ()12 1 25OL L G G , ()12 1 28OL L G G , and 
()12 1 2OL L G G , respectively. Based on the analysis, the GaborD, GaborS, and 
GaborSD based gait representation can re duce the computational complexity of 
the Gabor based representation, because th e number of filters (the sum of Gabor 
functions) for decomposition in the GaborD/GaborS/GaborSD based 
representation is smaller than the number of filters (Gabor functions) for decomposition in Gabor based representa tion. The experiments in §2 show that 
GaborD and GaborS based representations perform slightly better than Gabor 
based representation for gait recognition.  
Table 3.7. Computational complexities of  the alternating projection method based 
optimization procedure of GTDA with Gabor/GaborD/GaborS/GaborSD 
representations. 
 Time Complexity Space Complexity 
Gabor gaits in 12 58 LLR××× ( ) ( )33
11 2637 OT L L ++  ()22
12 89OL L++  
GaborD gaits in 12 5 LLR×× ( ) ( )33
21 2125 OT L L ++  ()22
12 25OL L++  
GaborS gaits in 12 8 LLR×× ( ) ( )33
31 2512 OT L L ++  ()22
12 64OL L++  
GaborSD gaits in 12NNR× ()()33
41 2OT L L+  ()22
12OL L+  
 
The computational complexities of the alternating projection method based 
optimization procedure of GTDA with Gabor/GaborD/GaborS/GaborSD 
representation are listed in Table 2.1. In Table 2.1, 1T (2T, 3T, and 4T) is the 
number of iterations to make the optimization procedures of GTDA with Gabor 
(GaborD, GaborS, and GaborSD) based representations converge. In our 
experiments, we found that 1T, 2T, 3T, and 4T are usually comparable. 107 Therefore, with GaborS/GaborD/GaborSD representations, the computational 
complexities of the alternating projecti on method based optimization procedure of 
GTDA are reduced compared with that of the Gabor based representation. 
 
 Experimental Results 
This Section first briefly describes the US F HumanID gait database [126] (gallery 
and probes). We then compare the performance of our algorithms with several 
other established algorithms for human gait recognition. 
 
1. HumanID Gait Database: Gallery and Probe Data Sets 
 
Table 3.8. Twelve probe sets for challenge experiments. 
Experiment (Probe) # of Probe 
Sets Difference between Gallery and 
Probe Set 
A (G, A, L, NB, M/N) 122 View 
B (G, B, R, NB, M/N) 54 Shoe 
C (G, B, L, NB, M/N) 54 View and Shoe 
D (C, A, R, NB, M/N) 121 Surface 
E (C, B, R, NB, M/N) 60 Surface and Shoe 
F (C, A, L, NB, M/N) 121 Surface and View 
G (C, B, L, NB, M/N) 60 Surface, Shoe, and View 
H (G, A, R, BF, M/N) 120 Briefcase 
I (G, B, R, BF, M/N) 60 Briefcase and Shoe 
J (G, A, L, BF, M/N) 120 Briefcase and View 
K (G, A/B, R, NB, N) 33 Time, Shoe, and Clothing 
L (C, A/B, R, NB, N) 33 Time, Shoe, Clothing, and Surface 
 
We carried out all our experiments upon the USF HumanID [126] outdoor gait (people–walking–sequences) database of version 2.1. The database has been built and widely utilized for vision–based gait recognition. It consists of 1,870 
sequences from 122 subjects (people). For each subject, there are the following covariates: change in viewpoints (Left or Right), change in shoe types (A or B), 
change in walking surface (Grass or C oncrete), change in carrying conditions 
(Briefcase or No Briefcase), and elapsed time (May or November) between 108 sequences being compared. There is a set of pre–designed experiments (12 
experiments) for algorithm comparison. For algorithm training, the database provides a gallery with the following covariates: grass, shoe type A, right camera, 
and no briefcase, which was collected in May and it also includes a number of 
new subjects collected in November. This  gallery dataset has 122 individuals. For 
algorithm testing, 12 probe sets are constructed according to the 12 experiments 
and detailed information about the probe sets  is given in Table 3.8. More detailed 
information about USF HumanID is described in [126].  
 
Figure 3.15. The averaged gait extraction and the dissimilarity measure. 
 
Figure 3.15 shows examples of averaged gait images. The averaged gait image 
stands for the mean image (pixel by pixel)  of silhouettes over a gait cycle within a 
sequence. A gait cycle is two successive half gait cycles. A half gait cycle is a 
series of stances: from heels–together–stance and full–stride–stance, to heels–
together–stance, as shown in Figure 3.15. As suggested in [91], the whole 109 sequence is partitioned into a series of sub–sequences according to the gait period 
(a cycle) length GaitN . Then the binary images within one cycle (a sub–sequence) 
are averaged to acquire a set of averaged silhouette images iAS, i.e.  
()()11
/
1|Gait
Gait
Gaitki N
TN
ii G a i t
ki NAS S k N=+ −
⎢⎥⎣⎦
=
=⎛⎞=⎜⎟⎜⎟⎝⎠∑ . (3.47)
The averaged gait representation is robust against any errors in individual frames, 
so we choose the averaged gait image to represent a gait cycle. One sequence 
yields several averaged gait images and the number of averaged gait images depends on the number of gait cycles in this sequence. In the following 
experiments, averaged gait images are utilized as the original data for the gait 
recognition problem. Some further averaged gait images from the gallery set are 
also shown in Figure 3.10, which demonstrate that averaged gait images can be used for gait recognition, because different people have different averaged gait 
images. 
The dissimilarity measure in gait recognition is the same as in [91]. The distance 
between the gallery sequence and the probe sequence is 
( )
() () ()11Dist ,
       Median min ,p GMethod Method
PG
N N Method Method
ij P GAS AS
ASi A S j===− (3.48)
where ()1|PN Method
PiAS i= is the ith projected AS in the probe data and 
()1|GN Method
GjAS j= is the jth projected AS in the gallery. The right hand side of (3.48) 
is the median of the Euclidean distances between averaged silhouettes from the 
probe and the gallery. It is suggested as a suitable measure for gait recognition by Liu and Sarkar in [91]. 
There are only two parameters in all proposed methods, one for GTDA and the other for LDA. In detail, one pa rameter is the threshold value 
δ for GTDA as 
described in (3.37). In all experiments, we vary δ from 0.85 to 0.995 with a step 
0.005. In 2DLDA, a similar strategy is used, i.e., δ is used to determine the 
dimensions of the projected subspace in  each order. The other parameter is the 
number of selected dimensions in LDA. In all experiments relevant to LDA, we 
vary the number of dimensions from 1 to 121 with a step 1. To speed up all 
experiments, we down sample the original averaged gait images from 128 88× to 
64 44× in all proposed methods. These are indicated by the note “H” in Table 3.9 110 and Table 3.10. We also show some experimental results based on the original 
averaged gait images with the size 128 88×. 
To examine the effectiveness of the automatic selection of ζ for GTDA based 
recognition defined in (3.31), we also manually tune the parameters to achieve 
further improvement by changing the select ed dimensions for each mode and the 
Lagrange multiplier ζ defined in (3.36). This is indicated by the note “M” in 
Table 3.9 and Table 3.10. Although manually tuning parameters improves the 
performance, it is time consuming. Moreover, the improvement is limited, so the automatic parameter selection used in th is Chapter is enough for applications. 
 
2. Performance Evaluation 
Sarkar et al. [126] evaluated the performance of the baseline algorithm on the 
HumanID challenge database using the rank one/five recognition rates: 1) the rank 
one recognition rate is the percentage of the number of the correct subjects in the 
first place of a list of matches obtained by an algorithm and 2) the rank five 
recognition rate is the percentage of the nu mber of the correct subjects in any of 
the first five places of a list of matches obtained by an algorithm. Twelve experiments have been designed, namely experiment A to experiment L, as shown 
in Table 3.8. The baseline algorithm reports the rank one recognition rates of the 
twelve experiments with increasing difficulty from 78% as the easiest to 3% as 
the hardest by examining the effects of the introduced five covariates (under different combinations). 
Table 3.9 and Table 3.10 report all experiments, which compare the proposed 
algorithms with the existing algorithms. The item “Avg” in Table 3.9 and Table 
3.10 means the averaged recognition rates of all probes (A–L), i.e., the ratio of 
correctly recognized subjects to the total number of subjects in all probes. The 
columns labeled A to L are exactly the same tasks as in the baseline algorithm. In both tables, the first rows give the pe rformance of Baseline [126], HMM [66], 
IMED [167], IMED+LDA [167], LDA [45], LDA+Sync [45], LDA+Fusion [45], 
2DLDA [173], and 2DLDA+LDA [173], respectively; while the performance of 
the new algorithms upon the same gallery set and probe set is fully reported on all comparison experiments, which are namely, GTDA(H), GTDA (M & H), GTDA, 
Gabor+GTDA(H), GaborD+GTDA(H), GaborD+GTDA, GaborS+GTDA (H), 111 GaborS+GTDA, GaborSD+GTDA(H), GaborSD+GTDA, GTDA+LDA(H), 
GTDA+LDA, Gabor+GTDA+LDA(H), GaborD+GTDA+LDA(H), GaborD+ GTDA+LDA, GaborS+GTDA+LDA(H), GaborS+GTDA+LDA, and GaborSD+ 
GTDA+LDA(H), respectively. Finally, the last columns of both tables report the 
average performance of the corresponding algorithms on all probe sets. From the comparison results in Table 3.9 and Table 3.10, it is clear that the 
averaged recognition rate of the twelve probes, our new methods (GTDA+LDA, 
Gabor+GTDA+LDA, GaborD+GTDA+LDA, and GaborS+GTDA+LDA) outperform the previous state–of–the–art algorithms (top part in both tables), e.g., 
the HMM algorithm, which is stable in modeling the gait cycles, and the IMED 
algorithm, which is demonstrated to improve the conventional LDA. Figure 3.16 and Figure 3.17 visually compare the results obtained from some important algorithms with the results obtained from the proposed ones. Table 3.9 and Table 
3.10 show that our proposed methods are not very sensitive to the changes in the 
size of averaged gait images, because the recognition rates are only slightly 
decreased when averaged gait images are down sampled from 
128 88×  to 
64 44× . Manually tuning the parameter ζ in GTDA in (15) will slightly 
improve the averaged recognition rate. Furthermore, performances for probes D–
G and K–L are not satisfactory. Therefore, further studies are required to make 
them applicable. Finally, the performances of different methods have the 
following relationship: Baseline < IM ED < LDA < IMED+ LDA < 2DLDA+LDA 
< LDA+Fusion < GTDA+LDA < GaborD+GTDA+LDA < Gabor+GTDA+LDA 
< GaborS+GTDA+LDA. 
In addition, it is worth emphasizing that the effects of the five covariates are also reflected in experimental re sults. In general, the results in Table 3.9 and Table 
3.10, for the proposed GaborS/GaborD+GTDA+LDA, show that: 
•
 Viewpoint and shoe changes have little impact on the recognition rate. This 
point is demonstrated by columns A–C, in which the rank one recognition rates are around 92%; 
•
 Apart from the viewpoint and the shoe covariates, if briefcase is also 
considered, the recognition tasks become more difficult and as a result, in columns H–J the performance is around 87% in rank one evaluation; 112 • If the viewpoint and the shoe issues are studied together with the surface 
covariate, the recognition tasks become hard. This effect leads to a worse 
performance around 35% in columns D–G in rank one evaluation; 
• The most difficult problem in human gait recognition is the elapsed time 
task, i.e., data in gallery and data in probes were captured at different time. In USF HumanID, data in gallery were  obtained in May and data in probes 
were obtained in November. Much work should be done to improve the 
performance on the tasks K and L although our proposed algorithms report better performance around 17% in rank one evaluation compared with 
many previous efforts, such as the baseline [126], IMED [167], 
IMED+LDA, LDA [45][46], and LDA+Fu sion [45][46], in most cases. 
 
 
  
 
   
  
 
 
  
 
   
  
 
 
 113  
  
 
Table 3.9. Rank one recognition rates for human gait recognition. 
Rank One (%) A B C D E F G H I J K L Avg 
Probe Size 122 54 54 121 60 121 60 120 60 120 33 33 –– 
Baseline 73 78 48 32 22 17 17 61 57 36 3 3 40.9572 
HMM 89 88 68 35 28 15 21 85 80 58 17 15 53.5365 
IMED 75 83 65 25 28 19 16 58 60 42 2 9 42.8695 
IMED+LDA 88 86 72 29 33 23 32 54 62 52 8 13 48.6357 
LDA 87 85 76 31 30 18 21 63 59 54 3 6 48.1983 
LDA+Sync 83 94 61 50 48 22 33 48 52 34 18 12 48.0355 
LDA+Fusion 91 94 81 51 57 25 29 62 60 57 9 12 55.8257 
2DLDA 89 93 80 28 33 17 19 74 71 49 16 16 50.9823 
2DLDA+LDA 89 91 82 33 33 23 25 67 78 50 19 19 52.6409 
GTDA (H) 85 88 73 24 25 15 14 53 49 45 4 7 42.9916 
GTDA (M & H) 86 88 73 24 25 17 16 53 49 45 10 7 43.7035 
GTDA 85 88 71 19 23 15 14 49 47 45 7 7 41.5992 
Gabor+GTDA (H) 84 86 73 31 30 16 18 85 85 57 13 10 52.5052 
GaborD+GTDA (H) 88 88 71 28 28 12 19 87 75 59 7 10 51.7359 
GaborD+GTDA 81 88 65 21 23 8 13 92 83 55 13 10 49.2610 
GaborS+GTDA (H) 89 89 69 31 33 13 16 79 76 56 13 13 51.4322 
GaborS+GTDA 82 86 67 22 30 8 14 92 88 62 10 7 50.9990 
GaborSD+GTDA (H) 87 89 71 23 28 8 14 82 69 51 4 13 48.2109 
GaborSD+GTDA 81 82 69 17 26 7 14 91 78 60 10 10 48.8518 
GTDA+LDA (H) 94 95 88 35 42 23 30 65 61 58 16 19 54.5543 
GTDA+LDA 95 95 86 39 44 25 30 61 68 67 16 19 56.5167 
Gabor+GTDA+LDA (H) 89 93 80 45 49 23 30 81 85 53 10 19 57.7296 
GaborD+GTDA+LDA (H) 93 93 84 34 40 23 32 90 80 63 16 19 58.9102 
GaborD+GTDA+LDA 89 93 84 27 35 17 26 93 88 67 16 22 57.5511 
GaborS+GTDA+LDA (H) 93 95 88 39 47 28 33 82 82 63 19 19 60.2390 
GaborS+GTDA+LDA 91 93 86 32 47 21 32 95 90 68 16 19 60.5804 
GaborSD+GTDA+LDA (H) 92 93 78 30 38 21 26 82 75 55 16 19 54.8685 
   
  
 
 114  
  
 
 Table 3.10. Rank five recognition ra tes for human gait recognition. 
Rank Five (%) A B C D E F G H I J K L Avg 
Probe Size 122 54 54 121 60 121 60 120 60 120 33 33 –– 
Baseline 88 93 78 66 55 42 38 85 78 62 12 15 64.5397 
HMM –– –– –– –– –– –– –– –– –– –– –– –– –– 
IMED 91 93 83 52 59 41 38 86 76 76 12 15 65.3132 
IMED+LDA 95 95 90 52 63 42 47 86 86 78 21 19 68.5950 
LDA 92 93 89 58 60 36 43 90 81 79 12 12 67.3674 
LDA+Sync 92 96 91 68 69 50 55 80 78 69 39 30 70.8528 
LDA+Fusion 94 96 93 85 79 52 57 89 86 77 24 21 76.1754 
2DLDA 97 93 93 57 59 39 47 91 94 75 37 34 70.9530 
2DLDA+LDA 97 100 95 58 57 50 50 86 94 77 43 40 72.8507 
GTDA (H) 98 95 95 57 54 34 42 75 80 69 22 16 65.0532 
GTDA (M & H) 100 97 95 57 54 34 45 75 80 70 25 25 66.1472 
GTDA 100 97 95 52 52 34 45 47 71 70 25 25 64.7015 
Gabor+GTDA (H) 96 95 89 59 63 33 49 94 92 76 19 40 70.3205 
GaborD+GTDA (H) 96 95 88 59 49 27 35 95 97 84 28 28 69.0898 
GaborD+GTDA 96 91 82 45 45 23 32 96 94 78 31 37 65.4134 
GaborS+GTDA (H) 98 97 93 60 52 34 37 93 95 79 31 25 70.0605 
GaborS+GTDA 96 91 84 45 54 23 37 96 95 79 22 31 66.0741 
GaborSD+GTDA (H) 95 93 88 54 47 27 30 89 88 71 28 28 64.8361 
GaborSD+GTDA 96 91 82 43 54 23 33 98 94 82 28 34 66.3319 
GTDA+LDA (H) 100 99 97 66 68 50 57 89 85 81 40 31 75.3267 
GTDA+LDA 100 99 97 67 69 50 57 90 90 84 40 37 76.5365 
Gabor+GTDA+LDA (H) 95 97 93 70 71 44 56 94 95 80 31 34 75.1451 
GaborD+GTDA+LDA (H) 98 99 95 62 68 44 50 96 99 87 37 43 76.0731 
GaborD+GTDA+LDA 98 99 93 52 59 37 49 99 99 88 34 43 73.5846 
GaborS+GTDA+LDA (H) 98 99 97 68 68 50 56 95 99 84 40 40 77.5762 
GaborS+GTDA+LDA 98 99 95 58 64 41 52 98 99 87 31 37 74.9008 
GaborSD+GTDA+LDA (H) 99 99 93 57 61 40 47 89 90 78 40 37 71.6534 
 
 
  
 
 115 1234567891011828486889092949698
Algorithm#Rank 1 Recognition RatePerformance Comparison on ProbeA
123456789101180859095
Algorithm#Rank 1 Recognition RatePerformance Comparison on ProbeB
 
1234567891011657075808590
Algorithm#Rank 1 Recognition RatePerformance Comparison on ProbeC
12345678910113035404550
Algorithm#Rank 1 Recognition RatePerformance Comparison on ProbeD
 
1234567891011303540455055
Algorithm#Rank 1 Recognition RatePerformance Comparison on ProbeE
12345678910111416182022242628
Algorithm#Rank 1 Recognition RatePerformance Comparison on ProbeF
 
12345678910112022242628303234
Algorithm#Rank 1 Recognition RatePerformance Comparison on ProbeG
12345678910115560657075808590
Algorithm#Rank 1 Recognition RatePerformance Comparison on ProbeH
 116 1234567891011606570758085
Algorithm#Rank 1 Recognition RatePerformance Comparison on ProbeI
12345678910115055606570
Algorithm#Rank 1 Recognition RatePerformance Comparison on ProbeJ
 
123456789101124681012141618
Algorithm#Rank 1 Recognition RatePerformance Comparison on ProbeK
1234567891011681012141618
Algorithm#Rank 1 Recognition RatePerformance Comparison on ProbeL
 
1234567891011464850525456586062
Algorithm#Rank 1 Recognition RateAverage Performance Comparison
 
Figure 3.16. Recognition performance comparison for rank one evaluation. From 
top–left to bottom, in each of the thirteen subfigures (Probes A, B, C, D, E, F, G, 
H, I, J, K, L, and the average performance), there are eleven bars, which 
correspond to the performance of HMM, IMED+LDA, LDA, LDA+Fusion, 2DLDA+LDA, GTDA+LDA(H), GTDA+LDA, Gabor+GTDA+LDA(H), 117 GaborD+GTDA+LDA(H), GaborS+GTDA+LDA(H), and GaborSD+GTDA+ 
LDA(H), respectively.  
234567891011889092949698100
Algorithm#Rank 5 Recognition RatePerformance Comparison on ProbeA
234567891011889092949698100
Algorithm#Rank 5 Recognition RatePerformance Comparison on ProbeB
 
2345678910118486889092949698100
Algorithm#Rank 5 Recognition RatePerformance Comparison on ProbeC
2345678910115055606570758085
Algorithm#Rank 5 Recognition RatePerformance Comparison on ProbeD
 
234567891011556065707580
Algorithm#Rank 5 Recognition RatePerformance Comparison on ProbeE
23456789101135404550
Algorithm#Rank 5 Recognition RatePerformance Comparison on ProbeF
 
23456789101140455055
Algorithm#Rank 5 Recognition RatePerformance Comparison on ProbeG
234567891011828486889092949698100
Algorithm#Rank 5 Recognition RatePerformance Comparison on ProbeH
 118 23456789101180859095100
Algorithm#Rank 5 Recognition RatePerformance Comparison on ProbeI
234567891011747678808284868890
Algorithm#Rank 5 Recognition RatePerformance Comparison on ProbeJ
 
23456789101115202530354045
Algorithm#Rank 5 Recognition RatePerformance Comparison on ProbeK
23456789101115202530354045
Algorithm#Rank 5 Recognition RatePerformance Comparison on ProbeL
 
234567891011646668707274767880
Algorithm#Rank 5 Recognition RateAverage Performance Comparison
 
Figure 3.17. Recognition performance comparison for rank five evaluation. From 
top–left to bottom, in each of the thirteen subfigures (Probes A, B, C, D, E, F, G, 
H, I, J, K, L, and the average performance), there are ten bars, which correspond 
to the performance of IMED+LDA, LDA, LDA+Fusion, 2DLDA+LDA, GTDA +LDA(H), GTDA+LDA, Gabor+GTDA+LDA(H), GaborD+GTDA+LDA(H), 
GaborS+GTDA+LDA(H), and GaborS D+GTDA+LDA(H), respectively. 119  
3. Convergence Examination 
1 2 3 4 5 6 7 8 910 11 12 13 14 15051015
Number of IterationsErrGabor+GTDA (88%)
1 2 3 4 5 6 7 8 910 11 12 13 14 15051015
Number of IterationsErrGabor+GTDA(90%)
 
1 2 3 4 5 6 7 8 910 11 12 13 14 15024681012141618
Number of IterationsErrGabor+GTDA (92%)
1 2 3 4 5 6 7 8 910 11 12 13 14 15024681012141618
Number of IterationsErrGabor+GTDA (94%)
 
Figure 3.18. Experimental based converge nce justification for the alternating 
projection method for GTDA. The x–coordinate is the number of training iterations and the y–coordinate is the error value Err, as defined in Step 6 in Table 
3.6. From left to right, these four sub–figures show how Err changes with the 
increasing number of training iterations with different threshold values (88%, 
90% 92% and 94%) defined in (3.37). 
 
From Figure 3.18, it can be seen that only 3 to 5 iterations are usually required to 
achieve convergence of the alternati ng projection method based optimization 
procedure of GTDA because errors with different threshold values 
σ approach 
zero rapidly. In contrast, the traditional 2DLDA does not converge during the 
training procedure, as shown in the first figure in [173]. 
 120  Summary 
 
Objects in computer vision research are naturally represented by general tensors. 
The most popular examples are images and video shots, e.g., face images in face 
recognition and video shots in video categor ization. However, tensors have in the 
past been reduced to the vector form, because available subspace selection methods do not accept tensors as input. The vectorization removes the structure 
information, which could reduce the number of parameters needed to model the 
samples (images and video shots). To preserve the structure information, we 
develop the general tensor discriminant analysis (GTDA) for discriminative multilinear susbapce selection. This is an effective and efficient preprocessing 
step for subsequent classification, e.g. by linear discriminant analysis (LDA). 
Compared with existing multilinear subspace selection methods, e.g., the general 
tensor analysis (GTA) and the two dimensional LDA (2DLDA), the advantages of 
the proposed GTDA are: 1) the proposed alternating projection method to obtain a 
solution of GTDA converges; 2) GTDA a ccepts general tensors as input; 3) 
GTDA takes the discriminative informatio n into account; and 4) GTDA reduces 
the SSS problem of the subsequent classification, e.g., by LDA. We further 
develop the manifold learning using tensor  representations, which is an extension 
of GTDA based on the graph embedding framework. With this new framework, 
most of the popular manifold learning al gorithms accept tensors as input. Finally, 
we apply GTDA to human gait recognition and achieve top level performance.  121 4. Supervised Tensor Learning 
 
In vector based learning15 [30][39], a projection vector LwR∈r and a bias bR∈ 
are learnt to determine the class label of a sample LxR∈r according to a linear 
decision function ()signTyx w x b ⎡⎤ = +⎣⎦r rr. The wr and b are obtained based on 
a learning model, e.g., minimax probability machine (MPM) [74][135], based on 
N training samples associated with labels {},L
iixRy∈r, where iy is the class 
label, {}1, 1iy∈+− , and 1iN≤≤. In this Chapter, we focus on the convex 
optimization based learning, which accept vectors as input. 
Supervised tensor learning (STL) [149][ 150] is developed to extend the vector 
based learning algorithms to accept tensors as input. That is we learn a series of 
projection vectors 1|kL M
kkwR=∈r and a bias bR∈ to determine the class label 
{}1, 1+−  of a sample 12 M LL LR××∈XL according to a multilinear decision function 
()
1sign
kM
k
kyw b×
=⎡⎤=+⎢⎥⎣⎦∏ XXr. The 1|M
kkw=r and b are obtained from a learning 
model, e.g., tensor minimax probability machine (TMPM), based on N training 
samples associated with labels { }12,M LL L
iiR y××∈XL, where iy is the class label, 
{}1, 1iy∈+− , and 1iN≤≤ . 
This extension to tensor input is impor tant, because many objects in computer 
vision research are represented by general tensors in 12 M LL LR××L, as described in 
Chapter 3. If we choose to use vector ba sed learning algorithms, the vectorization 
operation () vect⋅ is applied to a general tensor X and form a vector 
() vectLx R =∈ Xr, where 12 M LLL L=××L . The vectorization eliminates the 
structure information of a sample in its original format. However, the information 
is helpful to reduce the number of para meters in a learning model and result in 
alleviating the overfitting problem. Usuall y, the testing error decreases with 
respect to the increasing complexity of training samples. When the complexity of 
training samples (partially represented by the number of training samples) is 
                                                 
15 We refer to the binary classification tasks. 122 limited, the tensor based learning machine performs better than the vector based 
learning machine. Otherwise, the vector based learning machine outperforms the tensor based learning machine, as shown in Figure 4.1. Usually, the size of the 
training set measures the data complexity and the complexity of a suitable 
classifier should consist with the complexity of the training data. 
 
Complexity of Training SamplesTesting ErrorVector Based Learning Machine
Tensor Based Learning Machine
 
Figure 4.1. Tensor based learning machine vs. vector based learning machine. 
 This Chapter is organized as follows. In §0, the convex optimization is briefly reviewed and a framework level formula of the convex optimization based 
learning is introduced. In §0, we deve lop a supervised tensor learning (STL) 
framewok, which is an extension of the convex optimization based learning. An alternating projection method is also developed to obtain the solution to an STL 
based learning algorithm. In §0, we devel op a number of tensor extensions of 
many popular learning machines, such as the support vector machine (SVM) [15][128][130][137][138][161], the minimax probability machine (MPM) [74] 
[135], the Fisher discriminant analysis (FDA) [37][30][69], and the distance 
metric learning (DML) [169]. In §169, an  iterative feature extraction model 
(IFEM) is given as an extension of the STL framework and the tensor rank one 
discriminant analysis (TR1DA) is developed as an example. In §4.5, Two 
experiments are conducted to study the effectiveness of TMPM (for STL) and 123 TR1DA (for IFEM) empirically. The first experiment, for image classification, 
demonstrates that TMPM reduces the ov erfitting problem in MPM. The second 
experiment, for the elapsed time problem in human gait recognition, shows 
TR1DA is more effective than PCA, LDA, and TR1A. Frinally, we summarize 
this Chapter in §169.  124  Convex Optimization  based Learning 
 
Learning models are always formulated  as optimization problems [170][178]. 
Therefore, mathematical programming [170][ 178] is the heart of machine learning 
research [128]. In this Section, we fi rst introduce the fundamentals of convex 
optimization and then give a general formulation for convex optimization based learning. 
A mathematical programming problem [170][178][11] has the form or it can be 
transformed to this form 
()
()
()0 min   
0,    1s.t.   0,    1w
i
ifw
fwi m
hw i p⎡ ⎤
⎢ ⎥
⎢ ⎥ ≤≤ ≤
⎢ ⎥=≤ ≤⎣ ⎦rr
r
r, (4.1) 
where 12[, ,, ]Tn
n ww w w R=∈rL  is the optimization variable in (4.1); the function 
0:nfRR→  is the objective function; the functions :n
ifRR→  are inequality 
constraint functions; and the functions :n
ihR R→  are equality constraint 
functions. A vector *wr is a solution to the problem if 0f achieves its minimum 
among all possible vectors, i.e., all vector s which satisfy the constraint equations 
(1|m
iif= and 1|p
iih=). 
When the objective function ()0fwr and the inequality constraint functions 
()1|m
iifw=r satisfy 
()()()12 1 2
12,    and    1
,ii i
nfw w fw fw
R
ww Rαβ α β
αβ α β++≤ +
∈+ =
∈rrrr
r (4.2) 
(i.e., ()0|m
iifw=r are convex functions) and the equality constraint functions 
()1|p
iihw=r are affine (i.e., ()0ihw=r can be simplified as T
iiaw b=rr), the 
mathematical programming problem defined in (4.1) is named the convex 
optimization problem. Therefore, a convex  optimization problem [11] is defined 
by 125 ()
()0 min   
0,    1s.t.   ,    1w
i
T
iifw
fwi m
aw b i p⎡ ⎤
⎢ ⎥
⎢ ⎥ ≤≤ ≤
⎢ ⎥=≤ ≤ ⎣ ⎦rr
r
rr, (4.3) 
where ()0|m
iifw=r are convex functions. The domain D of the problem in (4.3) is 
the intersection of the domains of ()0|m
iifw=r, i.e., 
0domm
iiDf
==I . The point *wr 
in D is the optimal solution of (4.3) if and only if 
()()0** 0 ,   Tfww w w D∇− ≥ ∀ ∈rrr r. (4.4) 
The geometric interpretation of the optimal solution for a convex optimization 
problem is given in Figure 4.2. 
 
D*wr()0* fw−∇r
 
Figure 4.2. The geometric interpretation of the optimal solution *wr in D for a 
convex optimization problem defined in (4.3). 
 
The convex optimization problem defined in (4.3) has a large number of popular special cases, such as linear programming (LP) [160], linear fractional 
programming (LFP) [11], quadratic pr ogramming (QP) [114], quadratically 
constrained quadratic programming (QCQP) [93], second order cone 
programming (SOCP) [93], semi–definite programming (SDP) [159], and geometric programming (GP) [12]. All of these special cases have been widely 
applied in different areas, such as comp uter networks, machine learning, computer 
vision, psychology, health research, automation research, and economics. The significance of a convex optimization problem is that the solution is unique 
(i.e., the locally optimal solution is also the globally optimal solution), so the 126 convex optimization has been widely applied to machine learning for many years, 
such as LP [160] in the linear programming machine (LPM) [117][134], QP [114] in the support vector machine (SVM) [161][15][130][128][137][138], SDP [159] 
in the distance metric learning (DML) [169] and the kernel matrix learning [73], 
and SOCP [93] in the minimax probability machine (MPM) [74][135]. This section reviews some basic concepts fo r supervised learning based on convex 
optimization, such as SVM, MPM, Fisher discriminant analysis (FDA) [37][30] 
[69], and DML. Now, we introduce LP, QP, QCQP, SOCP, and SDP, which have been widely 
used to model learning problems. 
LP is defined by 
min   
s.t.   
T
wcw
Gw h
Awb⎡ ⎤
⎢ ⎥
⎢ ⎥≤⎢ ⎥
=⎢ ⎥ ⎣ ⎦rrr
rr
rr, (4.5) 
where mnGR×∈  and pnAR×∈ . That is the convex optimization problem reduces 
to LP when the objective and constraint functions in the convex optimization 
problem defined in (4.3) are all affine. Th e geometric interpretation of the optimal 
solution for LP is given in Figure 4.3. 
 
D*wrc−r
 
Figure 4.3. The geometric interpretation of the optimal solution *wr in D for 
LP defined in (4.5). 
 
QP is defined by 127 1min  2
s.t.   TT
ww P wq wr
Gw h
Aw b⎡ ⎤++⎢ ⎥
⎢ ⎥
≤⎢ ⎥
⎢ ⎥=⎣ ⎦rrrr r
rr
rr, (4.6) 
where nPS+∈ , mnGR×∈  and pnAR×∈ . Therefore, the convex optimization 
problem reduces to QP when the objective function in (4.3) is convex quadratic 
and the constraint functions in (4.3) are all affine. The geometric interpretation of 
the optimal solution for QP is given in Figure 4.4.  
D*wr() Pwq−+rr
 
Figure 4.4. The geometric interpretation of the optimal solution *wr in D for 
QP defined in (4.6). 
 If the inequality constraints are not affine but quadratic, (4.6) transfroms to QCQP, i.e., 
0001min  2
10,     12 s.t.   
                                        TT
w
TT
ii iw P wq wr
w P wq wr i m
Aw b⎡⎤++⎢⎥
⎢⎥⎢⎥++ ≤ ≤ ≤⎢⎥
⎢⎥=⎣⎦rrr r r
rr r r
rr, (4.7) 
where n
iPS+∈  for 0im≤≤. 
SOCP has the form 128 min   
,    1s.t.   
                                         T
w
T
ii i i Frofw
Awb c wd i m
Fw g⎡⎤
⎢⎥⎢⎥ +≤ + ≤ ≤⎢⎥=⎣⎦rrr
rr r
rr, (4.8) 
where inn
iAR×∈ , pnFR×∈ , n
icR∈r, pgR∈r, in
ibR∈ , and idR∈. The 
constraint with the form TAwb c wd+≤+rrr is called the second order cone 
constraint. When 0ic=r for all 1im≤≤, SOCP transforms to QCQP. 
Recently, SDP has become an increasingly important technique in machine 
learning and many SDP based learning machines have been developed. SDP 
minimizes a linear function subject to  a matrix semidefinite constraint 
()0
1min   c
s.t.    0T
w
n
ii
iw
Fw F w F
=⎡⎤
⎢⎥
⎢⎥+≥ ⎢⎥⎣⎦∑rrr
r , (4.9) 
where m
iFS∈  for all 0in≤≤ and ncR∈r. 
Here, we provide a general formula for convex optimization based learning as 
()
(),,min   , ,
s.t.    ,    1wb
T
ii i ifw b
yc w x b i Nξξ
ξ⎡⎤
⎢⎥⎢⎥+≥ ≤ ≤⎣⎦rrrr
rr, (4.10)
where 1:LNfRR++→  is a criterion (convex function) for classification; 
:icR R→  for all 1iN≤≤  are convex constraint functions; L
ixR∈r 
(1iN≤≤ ) are training samples and their class labels are given by {}1, 1iy∈+− ; 
[ ]12,,,T N
N R ξξ ξ ξ=∈r
L  are slack variables; and LwR∈r and bR∈ determine 
a classification hyperplane, i.e., 0Twx b+=rr. By defining differe nt classification 
criteria f and convex constraint functions 1|N
iic=, we can obtain a large number 
of learning machines, such as SVM, MPM,  FDA, and DML. We detail this in the 
next Section. 
 129  Supervised Tensor Lear ning: A Framework 
 
STL extends vector based learning algorithm s to accept genera l tensors as input. 
In STL, we have N training samples 12 M LL L
iR××∈XL represented by tensors 
associated with class label information {}1, 1iy∈+− . We want to separate 
positive samples ( 1iy=+ ) from negative samples ( 1iy=−) based on a criterion. 
This extension is obtained by replacing L
ixR∈r (1iN≤≤) and LwR∈r with 
12 M LL L
iR××∈XL (1iN≤≤ ) and kL
kwR∈r (1kM≤≤ ) in (4.10). Therefore, STL 
is defined by 
()
11|, ,
1min   | , ,
s.t.        ,    1M
kk
kM
kkwb
M
ii i k i
kfw b
yc w b i Nξξ
ξ==
×
=⎡⎤
⎢⎥⎢⎥⎛⎞⎢⎥ +≥ ≤ ≤ ⎜⎟⎢⎥ ⎝⎠ ⎣⎦ ∏Xrrrr
r. (4.11)
There are two main differences between vector based learning and tensor based 
learning: 1) training samples are represente d by vectors in vector based learning, 
while they are represented by tensors in tensor based learning; and 2) the 
classification decision function is defined by LwR∈r and bR∈ in vector based 
learning ( ()signTyx w x b ⎡⎤ =+⎣⎦rr r), while it is defined by kL
kwR∈r (1kM≤≤ ) 
and bR∈ in tensor based learning, i.e., ()
1sign
kM
k
kyw b×
=⎡ ⎤= +⎢ ⎥⎣ ⎦∏ XXr. In vector 
based learning, we have a cl assification hyperplane as 0Twx b+=rr. While in 
tensor based learning, we define a classification hyperplane as 
10
kM
k
kwb×
=+=∏Xr. 
The Lagrangian for STL defined in (4.11) is 
() ( )
()11
1 1
1
1 1|, , , |, ,
                           | , ,k
kM N
MM
kk kk i ii i k i
i k
M N
MT
kk i ii i k
i kLw b f w b y c w b
fw b y c w bξαξ α ξ
ξαα ξ== ×
= =
=×
= =⎛⎞⎛⎞=− + − ⎜⎟⎜⎟⎝⎠⎝⎠
⎛⎞=− + + ⎜⎟⎝⎠∑∏
∑∏X
Xrrr rr r
r rr rr, (4.12)
with Lagrange multipliers [ ]12,,, 0T
N αα α α= ≥rL . The solution is determined 
by the saddle point of the Lagrangian 
( )
11|, ,max min | , , ,
M
kkM
kkwbLw b
α ξξα
== r rrrr r, (4.13)130 The derivative of ( ) 1|, , ,M
kkLw b ξα=rr r with respect to jwr is 
()
()
() ()1
1 1
1
1 1
1
1|, ,
       | , ,
       | , , ,jj j k
jj k
jM N
M
ww k k i i w i i k
i k
M N
M i
wk k i i w i k
i k
N
M i
wk k i i i j j
iLf w b y c w b
dcfwb y w bdz
dcfw b y wdzξα
ξα
ξα=×
= =
=×
= =
=
=⎛⎞∂= ∂ − ∂ + ⎜⎟⎝⎠
⎛⎞=∂ − ∂ + ⎜⎟⎝⎠
=∂ − ×∑ ∏
∑ ∏
∑X
X
Xrr r
rr
rrrr
rrr
rrr (4.14)
where 
1kM
ik
kzw b×
==+∏Xr. 
The derivative of ( ) 1|, , ,M
kkLw b ξα=rr r with respect to b is 
()
()
()1
1 1
1
1 1
1
1|, ,
      | , ,
      | , , ,k
kM N
M
bb k k i i b i i k
i k
M N
M i
bk k i i b i k
i k
N
M i
bk k i i
iLf wb y c w b
dcfwb y w bdz
dcfw b ydzξα
ξα
ξα=×
= =
=×
= =
=
=⎛⎞∂= ∂ − ∂ + ⎜⎟⎝⎠
⎛⎞=∂ − ∂ + ⎜⎟⎝⎠
=∂ −∑∏
∑ ∏
∑X
Xrrr
rrr
rr (4.15)
where 
1kM
ik
kzw b×
==+∏Xr. 
To obtain a solution to STL, we need to set 0
jwL∂=r  and 0bL∂=. Accoridng to 
(4.14), we have 
()
10
jjN
i
ww i i i j j
idcLf y wdzα
=∂= ⇒ ∂= × ∑ X rrr. (4.16)
According to (4.15), we have 
10N
i
bb i i
idcLf ydzα
=∂= ⇒ ∂= ∑ . (4.17)
Based on (4.16), we find the solution to jwr depends on kwr (1kM≤≤ , kj≠). 
That is we cannot obtain the solution to STL directly. The alternating projection 
provides a clue to have a solution to STL. The key idea in the alternating 
projection based optimization for STL is to obtain jwr with the given kwr 
(1kM≤≤ , kj≠) in an iterative way. The algorithm is given in Table 4.1. The 
convergence issue is proved in Theorem 4.1. 
 
 131 Table 4.1. Alternating Projection for the Supervised Tensor Learning 
Input: Training samples 12 ...
1|M LL L N
ii R×× ×
=∈ X  and their associated class labels
{}1, 1iy=+− . 
Output: The parameters 1|kL M
kkwR=∈r and bR∈, such that the STL objective 
function () 1|, ,M
kkfw b ξ=rr defined in (4.11) is minimized. 
1. Set 1|M
kkw=r be equal to random unit vectors in kLR. 
2. Carry out steps 3–5 iteratively until convergence.  
3. For 1j= to M 
4. Obtain jL
jwR∈r by solving 
()
(),,min   , ,
s.t.     ,    1jjwb
T
ii j i j j ifwb
yc w w b i Nξξ
ξ⎡⎤
⎢⎥
⎢⎥⎡⎤×+ ≥ ≤ ≤⎢⎥ ⎣⎦ ⎣⎦Xrrrr
rr 
5. End 
6. Convergence checking: if ()2
,, 1 ,
11M
T
kt kt ktFro
kww w ε−
−
=⎡⎤−≤⎢⎥⎣⎦∑rr r(610ε−= ), 
the calculated 1|M
kkw=r have converged. Here ,ktwr is the current 
projection vector and ,1ktw−r is the previous projection vector. 
7. End 
 
The alternating projection procedure to obtai n a solution in STL is illustrated in 
Table 4.1 and Figure 4.5. In this figure, training samples are represented by third 
order tensors. The following three steps are conducted iteratively to obtain the 
solution for STL: 
1) Generate the second projection vector 2wr and third projection vectors 3wr 
randomly according to the Step 1 in Table 4.1; project the original training 
samples (third order tensors) 123LL L
iR××∈X  (1iN≤≤) through 2wr and 3wr as 
()1
11L
iwR×∈Xr; and calculate the first projection vector 1wr according to the 
Step 4 in Table 4.1 based on the projected training samples ()11iw×Xr; 
2) Project the original training samples 1|N
ii=X  to the calculated first projection 
vector 1wr and the original third projection vector 3wr; and calculate the second 132 projection vector 2wr according to the Step 4 in Table 4.1 based on the projected 
training samples ()22iw×Xr; 
3) Project the original training samples 1|N
ii=X  by the previous 1wr and 2wr; and 
calculate 3wr through the Step 4 in Table 4.1 based on the projected training 
samples ()33iw×Xr. 
 133  
Figure 4.5. The third order tensor example for the alternating projection in STL. 
()1
11L
iwR×∈Xr
2
2LwR∈r
(1)
()2
22L
iwR×∈Xr
2
2LwR∈r123
(1 )LLL
iR
iN××∈
≤≤XThe First ModeThe Second ModeThe Third Mode
123
(1 )LLL
iR
iN××∈
≤≤X
(2)Projected 
Samples123
(1 )LLL
iR
iN××∈
≤≤X
Projected 
Samples
2
2LwR∈r(3) Projecte
d Sa
mples
Original Samples
Projected Samples134 Theorem 4.1  The alternating projection based optimization procedure for STL 
converges. 
Proof. 
The alternating projection method never increases the function value 
() 1|, ,M
kkfw b ξ=rr of STL between two successive iterations, because it can be 
interpreted as a type of a monotonic algorithm. We can define a continuous 
function 
121:MNN
Mkkfuu u R R u R R R
=××× ×× = ××× →L ,  
where ddwu∈r and du is the set, which includes all possible dwr. The bias 
bR∈ and the slack variables NRξ∈r
. 
With the definition, f has M different mappings: 
() ()
()1,,
1
11,,,, a r g m i n |, ,
                    arg min , , ; | , | ,dd
ddM
ddd d duu b
dM
dl l l l duu bgw b f w b
fwb w wξ
ξξξ
ξ∗∗∗
=∈
−
== +∈=r r
r rrrrr 
rrr r  
The mapping can be calculated with the given 1
1|d
llw−
=r in the tth iteration and 
1|M
lldw=+r in the ( 1t−)th iteration of the for–loop in Step 4 in Table 4.1. 
Given an initial ddwu∈r (1dM≤≤ ), the alternating projection generates a 
sequence of items { } ,,,,,; 1dt dt dtwb dMξ∗∗∗≤≤rr via 
() ( )1
,,, , 1, 1 1,,,, a r g m i n , , ; | , |
dddM
dt dt dt d lt l lt l duu bgw b f w b w w
ξξξ∗∗∗ −
=− = +∈= r rrrrr r r,  
with each {} 1, 2,dM∈L . The sequence has the following relationship: 
1,1 1,1 1,1 2,1 2,1 2,1 ,1 ,1 ,1 1,2 1,2 1,2
1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2,(, , ) (,,) ( , , ) (,,)
      ( , , ) ( , , ) ( , , ) ( , , )  MMM
ttt t t t TTT T T Ta fw b fw b fw b fw b
fw b fw b fw b fw bξξ ξ ξ
ξξ ξ ξ∗∗∗ ∗∗ ∗ ∗ ∗ ∗ ∗∗ ∗
∗∗∗ ∗∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗=≥ ≥ ≥ ≥ ≥
≥≥ ≥ ≥ ≥ ≥rr r rrr r rL
rr r rrr r rLL
,,,                   ( , , ) .MT MT MTfw b b ξ∗∗∗≥=r rL
where T→+∞ . Here both a and b are limited values in the R space. 
The alternating projection in STL can be illustrated by a composition of M sub–
algorithms, defined as 
() ( )1
1
11:| , , M a p , ,
lldM
M
dd d l d l
ll dwb w w b wξξ−
=× ×
== +Ω× ∏∏rrrr r ra .  135 It follows that 121M
M dd=ΩΩΩ Ω=Ω oo L o o  is a closed algorithm whenever all 
du are compact. All sub–algorithms () ,,dddgw bξ∗∗∗rr decrease the value of f, so 
it should be clear that Ω is monotonic with respect to f. 
Consequently, we can say that the alternating projection method to optimize STL 
defined in (4.11) converges.                                    ■ 
 136  Supervised Tensor Learning: Examples 
 
Based on the proposed STL and its alternating projection training algorithm, a 
large number of tensor based learning algorithms can be obtained by combining 
STL with different learning criteria, such as SVM, MPM, DML, and FDA. 
 
 Support Vector Machine vs . Support Tensor Machine 
SVM [161][15][130][128][137][138][41] find s a classification hyperplane, which 
maximizes the margin between positive samples and negative samples, as shown 
in Figure 4.6. 
 
Support Vectors
iξ
iξ
Positive SampleNegative Sample
Classification Hyperplane
MarginMargin Error
 
Figure 4.6. SVM maximizes the margin between positive and negative training 
samples.  
Suppose there are N training samples 
L
ixR∈r (1iN≤≤) associated with class 
lables {}1, 1iy∈+− . The traditional SVM [161][15], i.e., soft margin SVM, finds 
a projection vector LwR∈r and a bias bR∈ through 
()2
,,11min   , ,2
1,      1
s.t.   
0                                           N
CS V M i Frowbi
T
ii iJw b w c
yw x b i Nξξ ξ
ξ
ξ−
=⎡⎤=+⎢⎥
⎢⎥
⎢⎥⎡⎤+≥ − ≤ ≤⎣⎦⎢⎥
⎢⎥≥⎣⎦∑ rrrrr
rr
r (4.18)137 where [ ]12,,,T N
N R ξξ ξ ξ=∈r
L  is the vector of all slack variables to deal with 
the linearly non–separable problem. The iξ (1iN≤≤) is also called the 
marginal error for the ith training sample, as shown in Figure 4.6. The margin is 
2Frowr. When the classification problem is  linearly separable, we can set 0 ξ=r
. 
The decision function for classification is ()signTyx w x b ⎡⎤ = +⎣⎦r rr. 
The Lagrangian of (4.18) is 
() ()2
11 1
11 11,, , , 12
1                        2NN N
T
ii i i i i i Fro
ii i
NN N
TT T T T
ii i i i
ii iLw b w c y wx b
ww c y wx b yξακ ξ α ξ κξ
ξαα α α ξ κ ξ== =
== =⎡⎤ =+ − + − + −⎣⎦
=+− − + − −∑∑ ∑
∑∑ ∑rrrrr r r
rr r rr rr rr r (4.19)
with Lagrangian multipliers 0iα≥,  0iκ≥ for 1iN≤≤. The solution is 
determined by the saddle point of the Lagrangian 
(), ,,max min , , , ,
wbLw b
ακ ξξακ r rrrrrr r. (4.20)
This can be achieved by 
10
00
00 .N
wi i i
i
T
bLw y x
Ly
Lcξα
α
ακ=∂= ⇒=
∂= ⇒ =
∂=⇒−−=∑r
rr r
rr
rr (4.21)
Based on (4.21), we can have the dual problem of (4.18), 
()
11 11max   2
0 s.t.   
0NN N
T
Di j i j i j i
ij i
TJy y x x
y
cαα αα α
α
α== =⎡⎤=− +⎢⎥
⎢⎥⎢⎥ =⎢⎥≤≤⎢⎥⎣⎦∑∑∑ rr rr
rr
r. (4.22)
Set 
1,T
ij i jij NPy y x x
≤≤⎡⎤=⎣⎦rr, 11N q×=rr, Ay=r, 0b=r
, [ ] ,T
NN NN GI I××=− , and 
111, 0TTT
NN hc××⎡⎤=⎣⎦rrr
 in (4.6), we can see that the dual problem of (4.18) in SVM is a 
QP. 
In the soft margin SVM defi ned in (4.18), the constant c determines the tradeoff 
between 1) maximizing the margin between positive and negative samples and 2) 
minimizing the training error. The constant c is not intuitive. Therefore, 
Bcholkopf et al. [130][128] developed the nu–SVM by replacing the unintuitive 
parameter c with an intuitive parameter ν as 138 ()2
,, ,111min   , , ,2
,    1
s.t.     0,                                            
0                                            N
SVM i Frowbi
T
ii iJw b wN
yw x b i Nνξρξρξ ν ρ
ρξ
ξ
ρ−
=⎡⎤=+−⎢⎥
⎢⎥
⎢⎥⎡⎤+≥− ≤ ≤⎣⎦⎢⎥
⎢⎥≥
⎢⎥≥⎣⎦∑ rrrrr
rr
r. (4.23)
The significance of ν in nu–SVM defined in (4.23) is that it controls the number 
of support vectors and the marginal errors. 
Suykens and Vandewalle [137][138] simplified the soft margin SVM as the least 
squares SVM, 
()2
,,1min   , ,22
s.t.    1 ,     1T
LS SVM Frowb
T
ii iJw b w
yw x b i Nεγε εε
ε−⎡⎤=+⎢⎥
⎢⎥
⎡⎤+= − ≤ ≤ ⎢⎥⎣⎦⎣⎦rrr rr rr
rr. (4.24)
Here the penalty 0γ>. There are two differences between the soft margin SVM 
defined in (4.18) and the least squares SVM defined in (4.24): 1) inequality 
constraints are replaced by equalities; and 2) the loss 1N
i iξ=∑  (0iξ≥) is replaced 
by square loss. These two modifications make the solution of the least square 
SVM be more easily obtained compared with soft margin SVM. 
According to statistical learning theory , a learning machine performs well when 
the number of training samples is larger than the complexity of the model. 
Moreover, the model’s complexity and the number of parameters to describe the model are always in direct proportion. In computer vision research, objects are usually represented by general tensors as  described in Chapter 3 and the number 
of training samples is limited. Therefore,  it is reasonable to have the tensor 
extension of SVM i.e., the support tens or machine (STM), which uses fewer 
parameters than SVM. Based on (4.18) and STL defined in (4.11), it is not 
difficult to obtain the tensor extension of the soft margin SVM, i.e., the soft 
margin STM. 
Suppose we have training samples 
12 M LL L
iR××∈XL (1iN≤≤ ) and their 
corresponding class labels {}1, 1iy∈+− . The decision function is defined by 
()
1sign
kM
k
kyw b×
=⎡⎤=+⎢⎥⎣⎦∏ XXr, where the projection vectors kL
kwR∈r (1kM≤≤ ) 
and the bias b in soft margin STM are obtained from 139 ()
12
1|, ,1 1
11min   | , ,2
1 ,     1
  s.t.     
0                                                     M
kk
kM N
M
CS T M k k k iwbi k Fro
M
ii k i
kJw b wc
yw b i Nξξ ξ
ξ
ξ=−= ⊗
= =
×
=⎡⎤
=+ ⎢⎥
⎢⎥
⎢⎥⎡⎤⎢⎥ +≥ − ≤ ≤⎢⎥⎢⎥ ⎣⎦⎢⎥≥ ⎢⎥⎣⎦∑∏
∏Xrrrrr
r
r. (4.25)
Here, [ ]12,,,T N
N R ξξ ξ ξ=∈r
L  is the vector of all slack variables to deal with the 
linearly non–separable problem. 
The Lagrangian for this problem is 
()2
1
1 1
11 1
1 111|, , , ,2
                                  1
1                               2k
kM N
M
kk k i
i k Fro
M NN
ii i k i i i
ii k
M N
T
kk i i i i k
i kkLw b w c
yw b
ww c y wξακ ξ
α ξκ ξ
ξα=⊗
= =
×
== =
×
= ===+
⎛⎞⎡⎤−+ − + −⎜⎟⎢⎥⎣⎦⎝⎠
=+ −∑∏
∑ ∑ ∏
∑∏X
Xrrr rr
r
rr r
1
1                                  M N
i
N
TT T
i
ibyαα α ξ κ ξ=
=⎛⎞
⎜⎟⎝⎠
−+− −∑∏
∑rr rr rr (4.26)
with Lagrangian multipliers 0iα≥,  0iκ≥ for 1iN≤≤. The solution is 
determined by the saddle point of the Lagrangian 
( )
11, |, ,max min | , , , ,
M
kkM
kkwbLw b
ακ ξξακ
== r rrrrrr r. (4.27)
This can be achieved by 
()
1
110
00
00jN
wj i i i j j kj
T i
kk
k
T
bLw y w
ww
Ly
Lcξα
α
ακ≠
=
=∂= ⇒= ×
∂= ⇒ =
∂ =⇒−−=∑
∏X r
rr r
rr
rr
rr. (4.28)
 
The first equation in (4.28) shows that the solution of jwr depends on kwr 
(1kM≤≤ , kj≠). That is we cannot obtain the solution for the soft margin 
STM directly. This point has been pointed out in the STL framework developed in 
§4.2. Therefore, we use the proposed al ternating projection method in STL to 
obtain the solution of the soft margin STM. To have the alternating projection 
method for the soft margin STM, we need to replace the Step 4 in Table 4.1 by the following optimization problem, 140 ()
()2
,,1min   , ,2
1 ,      1
s.t.    
0                                                         jN
CS T M j j iFro wbi
T
ij i j j iJw b wc
yw w b i Nξηξξ
ξ
ξ−
=⎡⎤=+⎢⎥
⎢⎥
⎢⎥⎡⎤×+ ≥ − ≤ ≤⎣⎦⎢⎥
⎢⎥≥⎣⎦∑
Xrrrrr
rr
r, (4.29)
where 2
1kj
kFro kMw η≠
≤≤=∏r. 
The problem defined in (4.29) is the standard soft margin SVM defined in (4.18). 
Based on nu–SVM defined in (4.23) and STL defined in (4.11), we can also have 
the tensor extension of the nu–SVM, i.e., nu–STM, 
()
12
1|, , ,1 1
111min   | , , ,2
,    1
   s.t.      0,                                                      
0                   M
kk
kM N
M
STM k k k iwbi k Fro
M
ii k i
kJw b wN
yw b i Nνξρξρξ ν ρ
ρξ
ξ
ρ=−= ⊗
= =
×
==+ −
⎡⎤+≥ − ≤ ≤⎢⎥⎣⎦
≥
≥∑ ∏
∏Xrrrrr
r
r
                                   ⎡⎤
⎢⎥
⎢⎥⎢⎥
⎢⎥
⎢⎥⎢⎥
⎢⎥
⎢⎥⎣⎦. (4.30)
Here, 
0ν≥ is a constant. The Lagrangian for this problem is 
()2
1
1 1
1 1
111| , ,,,,,2
                                          
11                                      2kM N
M
kk k i
i k Fro
M N
T
ii i k i
i k
M
T
kk
kLw b wN
yw b
wwξρακτ ξ ν ρτ ρ
α ρξ κ ξ=⊗
= =
×
= =
==+ − −
⎛⎞⎡⎤−+ − + −⎜⎟⎢⎥⎣⎦⎝⎠
=+∑ ∏
∑∏
∏Xrrr rr
rr r
rr
11 1
1                                         ,kM NN
ii i i k
ii k
N
TT T
i
iywN
byξα
αρ α α ξ κ ξ ν ρ τ ρ×
== =
=⎛⎞−⎜⎟⎝⎠
−+ − − − −∑∑ ∏
∑Xr
rr rr rr (4.31)
with Lagrangian multipliers 0τ≥ and 0iα≥,  0iκ≥ for 1iN≤≤ . The 
solution is determined by the saddle point of the Lagrangian 
( )
11,, |, , ,m a x m i n | , ,,,,,
M
kkM
kkwbLw b
ακτ ξρξρακτ
== r rrrrrr r. (4.32)
Similar to the soft margin STM, the solution of jwr depends on kwr (1kM≤≤ , 
kj≠), because  
()
1
110
jN
wj i i i j j kj
T i
kk
kLw y w
wwα≠
=
=∂= ⇒= × ∑
∏X rr r
rr. 
(4.33)141 Therefore, we use the proposed alternating projection method in STL to obtain the 
solution of nu–STM. To have the alternating projection method for nu–STM, we need to replace the Step 4 in Table 4. 1 by the following optimization problem, 
()
()2
,, ,11min   , , ,2
,      1
 s.t.     0                                                           
0                                  jN
STM j j iFro wbi
T
ij i j j iJw b wN
yw w b i Nνξρηξρξ ν ρ
ρξ
ξ
ρ−
==+ −
⎡⎤×+ ≥ − ≤ ≤⎣⎦
≥
≥∑
Xrrrrr
rr
r
                         ⎡⎤
⎢⎥
⎢⎥
⎢⎥⎢⎥
⎢⎥
⎢⎥
⎢⎥⎣⎦, (4.34)
where 2
1kj
kFro kMw η≠
≤≤=∏r. 
The problem defined in (4.34) is the standard nu–SVM defined in (4.23). 
Based on the least squares SVM defined in (4.24) and STL defined in (4.11), we 
can also have the tensor extension of the least square SVM, i.e., least square STM, 
()
12
1|, ,1
11min   | , ,22
  s.t.     1 ,     1M
kk
kM
MT
LS STM k k kwbk Fro
M
ii k i
kJw b w
yw b i Nεγε εε
ε=−= ⊗
=
×
=⎡⎤
=+ ⎢⎥
⎢⎥
⎢⎥⎡⎤⎢⎥ += − ≤ ≤⎢⎥⎢⎥ ⎣⎦ ⎣⎦∏
∏Xrrr rr rr
r, (4.35)
where 0γ> is a constant. Similar to the soft margin STM and nu–STM, there is 
no closed form solution for least squares STM. We use the alternating projection 
method in STL to obtain the solution of the least squares STM. To have the 
alternating projection method for the least squares STM, we need to replace the Step 4 in Table 4.1 by the following optimization problem, 
()
()2
1,,min   | , ,22
 s.t.   1 ,     1jMT
LS STM k k jFro wb
T
ij i j j iJw b w
yw w b i Nεηγε εε
ε−=⎡⎤=+⎢⎥
⎢⎥
⎡⎤⎢⎥ ×+ = − ≤ ≤⎣⎦⎣⎦Xrrr rr rr
rr, (4.36)
where 2
1kj
kFro kMw η≠
≤≤=∏r. 
 
Theorem  4.2 In STM, the decision function is ()
1sign
kM
k
kyw b×
=⎡ ⎤= +⎢ ⎥⎣ ⎦∏ XXr with 
2
2
1M
k
k Frow⊗
=≤Λ∏r and 2 2
FroR≤ X . Let 0ρ> and ν is the fraction of training 
samples with a margin smaller than ρΛ. When STM is obtained from N 
training samples 2 2
iFroR≤ X  (1iN≤≤), sampled from a distribution P with 142 probability at least 1δ− (01δ<<), the misclassification probability of a test 
sample sampled from P is bounded by 
22
2
21ln lnRNNλνρ δ⎛⎞Λ++⎜⎟⎝⎠, (4.37)
where λ is a universal constant. 
Proof:  This is a direct conclusion from the theorem on the margin error bound 
introduced in [128]. More information about other error bounds in SVM can be 
found in [3].  
 Minimax Probability Machine vs.  Tensor Minimax Probability 
Machine 
 
Positive SampleNegative SampleClassification Hyperplane
(),m++Σr
(),m−−ΣrIntersection Point
 
Figure 4.7. MPM separates positive samples from negative samples by 
maximizing the probability of the correct classification for future samples. The 
intersection point minimizes the maximum of the Mahalanobis distances between 
positive and negative samples, i.e., it has the same Mahalanobis distances to the 
mean of the positive samples and the mean of the negative samples.  
The minimax probability machine (MPM) [74][135] has become popular. It is reported to outperform the conventional  SVM consistently and therefore has 
attracted attention as a promising supe rvised learning algorithm. MPM focuses on 
finding a decision hyper–plane, which is 
(){ } ,| 0THw b xw x b=+=rrrr, to separate 
positive samples from negative samples (a binary classification problem) with 
maximal probability with respect to all distributions modelled by given means and covarainces, as shown in Figure 4.7. MPM maximizes the probability of the 143 correct classification rate (classification accuracy) on future samples. For 
Gaussian distributed samples, it mini mizes the maximum of the Mahalanobis 
distances of the positive samples and the negative samples. With given positive 
samples ixr ( 1iy=+ ) and negative samples ixr ( 1iy=−), MPM is defined as, 
()
()(){}
()(){},,
1~ ,
1~ ,max   , ,
inf  Pr 0
 s.t.   
inf  Pr 0ii
iiMPMwb
T
ixy m
T
ixy mJw b
wx b
wx bδδδ
δ
δ++
−−=+ Σ
=− Σ⎡= ⎤
⎢⎥
⎢⎥ +≥ ≥⎢⎥
⎢⎥+≤ ≥⎢⎥⎣⎦r
rr
rrr
rr
rr. (4.38)
Here, the notation ()() 1,iixy m++ =+Σrr   means the class distribution of the 
positive samples has the mean m+r and covariance +Σ, and similarly for the 
notation ()() 1,iixy m−− =− Σrr  . The classification decision function is given by 
()signTyx w x b ⎡⎤=+⎣⎦rr r. 
Recently, based on the powerful Marshall  and Olkin’s theorem [102], Popescu 
and Bertsimas [120] proved a probability bound, 
(){}2
~,1sup Pr1 xmxSd Σ∈=+rrr with () ()21infT
xSdx m x m−
∈=−Σ−rrrr r, (4.39)
where xr stands for a random vector, S is a given convex set, and the 
supremum is taken over all distributions for xr with the mean value as mr and 
the covariance matrix Σ. Based on this result, Lanckriet et al. [74] reformulated 
(4.38) as: 
()
,,max   , ,
,   1 s.t.   
,   1MPMwb
TT
i
TT
iJw b
wm b w w y
wm b w w yκκκ
κ
κ++
−−= ⎡⎤
⎢⎥
⎢⎥+≥+ Σ =+⎢⎥
⎢⎥+≤− Σ =−⎣⎦rr
rr r r
rr r r, (4.40)
where the constraint functions in (4.40) are second order cone functions. There 
MPM is an SOCP. This problem can be further simplified as 
()
()min   
 s.t.   1TT
MPMw
TJ ww w w w
wm m+−
+−⎡⎤ =Σ +Σ⎢⎥
⎢⎥−=⎣⎦rrrrrr
rr r, (4.41)
where b is determined by 
()() ()
() () () ()****
** **T
T
TTwwbw m
ww ww+
+
+−Σ=−
Σ+ Σrrrr
r rrr. (4.42)144 In computer vision research, many objects are represented by tensors. To match 
the input requirments in MPM, we need to vectorize the tensors to vectors. When training samples are limited, the vectorization will be a disaster. This is because 
MPM meets the matrix singular problem seriously (the ranks of 
+Σ and −Σ are 
deficient). To reduce this problem, we pr opose the tensor extension of MPM, i.e., 
tensor MPM (TMPM). TMPM is a combination of MPM and STL. 
Suppose we have training samples 12 M LL L
iR××∈XL (1iN≤≤ ) and their 
corresponding class labels {}1, 1iy∈+− . The decision function is given by 
()
1sign
kM
k
kyw b×
=⎡⎤=+⎢⎥⎣⎦∏ XXr. Projection vectors kL
kwR∈r (1kM≤≤ ) and the 
bias b in TMPM are obtained from 
()
11|, ,
;
1 1 1
;
1 1 1max   | , ,
1I( 1) sup
 s.t.     
1I( 1) supM
kk
k
kM
MPM k kwb
M N
T
ii k l l l
lM i k
M N
T
ii k l l l
lM i kJwb
yw b w wN
yw b w wNκκκ
κ
κ==
×+
≤≤ = = +
×−
≤≤ = = −⎡⎤ =
⎢⎥
⎢⎥⎛⎞⎡⎤⎢⎥ =+ + ≥+ Σ⎜⎟⎢⎥⎢⎥ ⎣⎦⎝⎠⎢⎥
⎛⎞⎡⎤ ⎢⎥=− + ≤− Σ⎜⎟⎢⎥ ⎢⎥⎣⎦⎝⎠ ⎣⎦∑ ∏
∑ ∏X
Xrr
r rr
r rr, (4.43)
where ;l+Σ is the covariance matrix of the projected samples ()il lw−×Xr for all 
1iy=+  and ;l−Σ is the covariance matrix of the projected samples ()il lw−×Xr 
for all 1iy=− . The function I( 1)iy=+ is 1 if iy is 1+, otherwise 0. The 
function I( 1)iy=−  is 1 if iy is 1−, otherwise 0. This problem can be 
simplified as 
()
11|, ,
;
1 1
;
1 1max   | , ,
sup
 s.t.     
supM
kk
k
kM
MPM k kwb
M
T
kl l l
lM k
M
T
kl l l
lM kJwb
wb w w
wb w wκκκ
κ
κ==
+× +
≤≤ =
−× −
≤≤ =⎡⎤ =
⎢⎥
⎢⎥⎢⎥ +≥ + Σ
⎢⎥
⎢⎥⎢⎥ +≤ − Σ
⎢⎥⎣⎦∏
∏M
Mrr
r rr
r rr, (4.44)
where () [ ]11I ( 1 )N
ii iNy++ === +∑ MX , () [ ]11I ( 1 )N
ii iNy−− === −∑ MX , and 
N+ (N−) is the number of positive (negative) samples. 
The Lagrangian for this problem is 145 () 11 ;
1 1
2;
1 1
12
1
1|, , , s u p
                                    sup
                           k
k
kM
M T
kk k l l l
lM k
M
T
kl l l
lM k
M
k
kLw b w b w w
wb w w
bbwNNκα κ α κ
ακ
αακα=+ × +
≤≤ =
−× −
≤≤ =
+×
= +−⎛⎞=− − + − Σ⎜⎟⎝⎠
⎛⎞++ + Σ⎜⎟⎝⎠
=− − + −∏
∏
∏M
M
Mr rr r r
rr r
r
2
1
12
;;
11                              sup supkM
k
k
TT
ll l ll l
lM lMw
ww wwNNα
ακ ακ−×
=
+−
≤≤ ≤≤+−+
+Σ +Σ∏Mr
rr rr (4.45)
with Lagrangian multipliers 0iα≥ ( 1, 2i= ). The solution is determined by the 
saddle point of the Lagrangian 
( )
11|, ,max min | , , ,
M
kkM
kkwbLw b
α κκα
== rrr r. (4.46)
This can be achieved by setting 0
jwL∂=r , 0bL∂=, and 0 Lκ∂= . It is not 
difficult to find that the solution of jwr depends on kwr (1kM≤≤ , kj≠). 
Therefore, there is no closed form solution for TMPM. We use the proposed 
alternating projection method in STL to obtain the solution of TMPM. To have 
the alternating projection method for TMPM, we need to replace the Step 4 in 
Table 4.1 by the following optimization problem, 
()
()
(),,
;
;max   , ,
 s.t.   jMPM jwb
TT
jj j j j j
TT
jj j j j jJw b
ww b w w
ww b w wκκκ
κ
κ++
−−⎡⎤ =
⎢⎥
⎢⎥×+ ≥ + Σ⎢⎥
⎢⎥×+ ≤ − Σ ⎢⎥⎣⎦M
Mrr
r rr r
r rr r. (4.47)
This problem is the standard MPM defined in (4.40). 
 
 Fisher Discriminant Analysis vs . Tensor Fisher Discriminant 
Analysis 
Fisher discriminant analysis (FDA) [37] [30][69] has been widely applied for 
classification. Suppose there are N training samples L
ixR∈r (1iN≤≤ ) 
associated with their class labels {}1, 1iy∈+− . There are N+ positive training 
samples and their mean is () [ ]11I ( 1 )N
ii imN yx++ === +∑r r; there are N− 
negative training samples and their mean can be calculated from 
() [ ]11I ( 1 )N
ii imN yx−− === −∑rr; the mean of all training samples is 146 ()11N
i imN x==∑rr; and the covariance matrix of all training samples is Σ. FDA 
finds a direction to separate the class means while minimizing the total covariance 
of the training samples. Therefore, two qua ntities need to be defined, which are: 
1) the between class scatter () ()2121T
bSm m m m=− −rrrr: measuring the difference 
between two classes; and 2) the within class scatter 
() ()2
1N
wi i iSx m x m N==− − = Σ∑rrrr: the variance of all training samples. The 
projection direction wr maximizes 
() max   T
b
FDA TwwwSwJwwSw⎡ ⎤= ⎢ ⎥⎣ ⎦rrrrrr. (4.48)
 
Positive SampleNegative SampleClassification Hyperplane
(),m+Σr
(),m−Σr
 
Figure 4.8. FDA separates positive samples from negative samples by maximizing 
the symmetric Kullback–Leibler divergence between two classes under the assumption that the two classes share the same covariance matrix. 
 This problem is simplified as 
()()max   T
FDAT wwm m
Jw
ww+−⎡⎤ −
⎢⎥ =
⎢⎥ Σ⎣⎦rrrr
r
rr. (4.49)
According to Chapter 2, we know this procedure is equivalent to maximizing the 
symmetric Kullback–Leibler divergence (KLD) between positive and negative samples with identical covariances in the projected subspace, so that positive 
samples are separated from negative sample s. Based on the definition of FDA, we 
know FDA is a special case of the li near discriminant analysis (LDA). 147 The decision function in FDA is ()signTyx w x b ⎡⎤ = +⎣⎦r rr, where wr is the 
eigenvector of () ()1 Tmmmm−
+−+−Σ− −rrrr associated with the largest eigenvalue 
and the bias b is calculated by 
()TNN N mN mwbNN−+ + +− −
−+−− +=+rrr
. (4.50)
The significance [30][39] of FDA is: FDA is Bayes optimal when the two classes 
are Gaussian distributed with identical covariances. 
When objects are represented by tensors, we need to vectorize the tensors to 
vectors to match the input requirments in FDA. When training samples are 
limited, the vectorization will be a disaster for FDA. This is because wS and bS 
are both singular. To reduce this problem , we propose the tensor extension of 
FDA, i.e., tensor FDA (TFDA). TFDA is a combination of FDA and STL. Moreover, TFDA is a special case of the previous proposed general tensor 
discriminant analysis (GTDA). 
Suppose we have training samples 
12 M LL L
iR××∈XL (1iN≤≤ ) and their 
corresponding class labels {}1, 1iy∈+− . The mean of the positive training 
samples is () [ ]11I ( 1 )N
ii iNy++ === +∑ MX ; the mean of the negative training 
samples is () [ ]11I ( 1 )N
ii iNy−− === −∑ MX ; the mean of all training samples is 
()11N
i iN==∑ MX ; and N+ (N−) is the number of positive (negative) samples. 
The decision function is defined by ()
1sign
kM
k
kyw b×
=⎡ ⎤= +⎢ ⎥⎣ ⎦∏ XXr, where the 
projection vectors kL
kwR∈r (1kM≤≤ ) and the bias b in TFDA are obtained 
from 
()()
()12
1
1 2|
1 1max   |k
M
kk
kM
k
k M
TFDA k kM N w
ik
i kw
Jw
w=+−×
=
=
×
= =⎡⎤
− ⎢⎥
⎢⎥=⎢⎥
⎢⎥ −
⎢⎥⎣⎦∏
∑∏MM
XMrr
r
r, (4.51)
The formula is obtained directly from (3.31) and (3.32). Similar to GTDA, there is 
no closed form solution for TFDA. The alternating projection is applied to obtain 
the solution for TFDA and we need to replace the Step 4 in Table 4.1 by the 
following optimization problem, 148 ()()
()2
2
1max   
jT
jj j
TFDA j NwT
ji j j
iww
Jw
ww+−
=⎡⎤
⎢⎥ ⎡ ⎤ −×⎣ ⎦⎢⎥ =
⎢⎥⎡ ⎤ −×⎣ ⎦ ⎢⎥⎣⎦∑MM
XMrrr
r
rr. (4.52)
This problem is the standard FDA. When we have the projection vectors 1|M
kkw=r, 
we can obtain the bias b from 
()
1kM
k
kNN N N w
bNN−+ + +− − ×
=
−+−− +
=+∏ MMr
. (4.53)
 
 Distance Metric Learning vs.  Multiple Distance Metrics 
Learning 
Weinberger et al. [169] proposed the distance metric learning (DML) to learn a 
metric for k–nearest–neighbor ( kNN) classification. The motivation of DML is 
simple because the performance of kNN is only related to the metric used for 
dissimilarity measure. In traditional kNN, the Euclidean metric fails to capture the 
statistical charateristics of training samples. In DML, the metric is obtained such that “ k–nearest neighbors always belong to the same class while examples from 
different classes are separated by a large margin ”. DML is defined by 
( ) () () ()
() () () (),11 111, ,min   , 1
1 ,     1 , ,
  s.t.     0,                                             ijlNN NNT
DML ijl ij i j i j ij il ijl
ij ijij l N
T T
i l i l i j i j ijl
ijlJx x x x c y
xxx x x xx x i j l Nξξη η ξ
ξ
ξΣ== ==≤≤Σ= −Σ − + −
−Σ − − −Σ − ≥ − ≤ ≤
≥∑∑ ∑∑rr rr
rr rr rr rr
                              1 , ,
0                                                                                                 ij l N⎡⎤
⎢⎥⎢⎥
⎢⎥
⎢⎥⎢⎥≤≤⎢⎥
⎢⎥Σ≥⎣⎦(4.54)
where 1
ijη= and 1ijy= mean that ixr and jxr have the same class label, 
otherwise 0. The constraint function 0Σ≥ indicates that the maxtrix Σ is 
required to be positive semidefinite, so the problem is an SDP. From the learnt 
distance metric Σ, it is direct to have the linear transformation matrix by 
decomposing TWWΣ= . 
The optimization problem defined in (4.54) is equivalent to 149 ()() ()
,111, ,min   , tr 1
1 ,                       1 , ,
  s.t.     0,                                      1 , ,
0                       ijlNN
T
DML ijl ij il ijl
ijij l N
T
ijl ijl ijl
ijlJA A c y
BB i j lN
ij l Nξξ ηξ
ξ
ξΣ==≤≤Σ= Σ + −
Σ≥ − ≤ ≤
≥≤ ≤
Σ≥∑∑
                                      ⎡⎤
⎢⎥⎢⎥
⎢⎥
⎢⎥⎢⎥⎢⎥
⎣⎦ (4.55)
where 
()2 ij i jLNAx xη
×⎡⎤=−⎣⎦rr (1,ij N≤≤) and 
2,ijl i l j iLBx x x x
×⎡ ⎤ =− −⎣ ⎦rrr r. 
Margin Margin
Target NeighborhoodLocal NeighborhoodBefore After
ixrixr
Similarly Labeled
Differently Labeled
Differently Labeled
 
Figure 4.9. DML obtains a metric, such that “ k–nearest neighbors always belong 
to the same class while examples from different classes are separated by a large 
margin ”. 
 
Suppose we have training samples 12 M LL L
iR××∈XL (1iN≤≤ ) and their 
corresponding class labels {} 1, 2, ,iy n∈L . The multiple distance metric learning 
(MDML) learns M metrics T
kk kWWΣ=  (1kM≤≤ ) for M–th order tensors 
1|N
ii=X  to make the samples, which have the same (different) labels, be as close 
(far) as possible. The MDML is defined as 150 ()()
()
() ()12
11 1
11 , ,|,
1, ,
11
22
11min | , |
1
1
  s.t.   0,           1 , ,k
M
k k ijl
kkM NN
ij i j k
ij k M Fro
MDML k k ijl i j l NNN W
ij l N
ij il ijl
ij
MM
i l k i j k ijl
kk Fro Fro
ijlW
JW
cy
WW
ij l Nξη
ξ
ηξ
ξ
ξ=×
== =
=≤ ≤
≤≤
==
××
==⎡ ⎤
− ⎢ ⎥
⎢ ⎥=⎢ ⎥
⎢ ⎥ +−
⎢ ⎥ ⎣ ⎦
−− −≥ −
≥≤ ≤∑∑ ∏
∑∑
∏∏XX
XX XX
                                             
0,      1                                                  T
kkWW k M⎡⎤
⎢⎥⎢⎥⎢⎥⎢⎥⎢⎥⎢⎥⎢⎥⎢⎥⎢⎥⎢⎥⎢⎥≥≤ ≤ ⎢⎥⎣⎦
. (4.56)
As described in the STL framework, ther e is also no closed form solution for 
MDML. The alternating projection method is applied to obtain the solution for 
MDML and we need to replace the Step 4 in Table 4.1 by the following optimization problem,  
() () ()
() ()1, ,,111, ,
22
11min , | tr 1
1
  s.t.   0,           1 , ,                                              pi j l
kkNN
T
MDML p ijl i j l N p p p ij il ijlWijij l N
MM
i l k i j k ijl
kk Fro Fro
ijlJW A A c y
WW
ij l N
Wξξ ηξ
ξ
ξ≤≤
==≤≤
××
===Σ + −
−− −≥ −
≥≤ ≤∑∑
∏∏ XX XX
0,      1                                                  T
kkWk M⎡⎤
⎢⎥⎢⎥⎢⎥⎢⎥⎢⎥⎢⎥⎢⎥⎢⎥≥≤ ≤ ⎣⎦
. (4.57)
Here, 
()()
11matNN
p ij p i j p p
ijA W η
===− ×∑∑ XX , (4.58)
and 
()( ) ;matijl p p j l p pB W =− × XX . (4.59)
This is because  
() ()() ()() ()2
1tr mat mat
kM
T
il k j il j j j jil j j
k FroWW W×
=−= − × Σ − ×∏ XX XX XX . 
Deduction: 
()2
1kM
il k
k FroW×
=−∏ XX  
() () ( ) ( )
11;1 : 1 :
kkMM
il k il k
kkWW M M××
==⎛⎞ ⎛⎞=− ⊗ −⎜⎟ ⎜⎟⎝⎠ ⎝⎠∏∏ XX XX           
()( )()( )() () tr ; 1: 1:i l jj jj i l jj jj WW WW MM ⎡⎤ ⎡⎤= − ××⊗ − ××⎣⎦ ⎣⎦XX XX       
() () ()() ( ) tr ;T
ji l j j i l j j jWW W j j W⎡⎤ ⎡⎤=− × ⊗ − ×⎣⎦ ⎣⎦XX XX       151 ()( )()( ) ( ) tr mat matTT
jj i l j j j i l j j jWW W W=− × − × XX XX  
()( ) ()( ) ( ) tr mat matTT
ji l j j j j ji l j j WW W W =− × − × XX XX  
()( )()( ) ( ) tr mat mat .T
j i lj j j j i lj j WW =− × Σ − × XX XX  
This problem defined in (4.57) is the standard DML. 
 152  Iterative Feature Extract ion Model based on Supervised 
Tensor Learning 
 
The iterative feature extraction model (IFEM ) based on STL is an extension of the 
STL framework for feature extraction and its  procedure is similar to the recursive 
rank one tensor approximation developed by Shashua and Levin in [132].  
Suppose we have training samples 12 M LL L
iR××∈XL (1iN≤≤ ) and their 
corresponding class labels {}1, 1iy∈+− . IFEM is defined by 
,, 1 , 1 , 1
1kM
ir ir ir kr
kwλ− −×−
==−∏ XXr (4.60)
() ,1 ,1 ,1
1kMT
ir ir kr
kw λ−− × −
==∏ Xr (4.61)
()
,1,1|, ,
,,
1min   | , ,
s.t.         ,    1M
krk
kM
kr kwb
M
ii i r k r i
kfw b
yc w b i Nξξ
ξ==
×
=⎡⎤
⎢⎥⎢⎥⎛⎞⎢⎥ +≥ ≤ ≤ ⎜⎟⎢⎥ ⎝⎠ ⎣⎦ ∏Xr rr r
r (4.62)
where ,1ii=XX  and ,00iλ=. The ,1|R
ir rλ= (R is the number of extracted 
features in IFEM) is used to represent the original tensor iX. 
 
123
,1
(1 )LLL
ir R
iN××
−∈
≤≤X
123
,1
(1 )LLL
ir R
iN××
−∈
≤≤X3
,1 ,1
1ir kr
kwλ−⊗−
=∏r123
,
(1 )LLL
irR
iN××∈
≤≤X
2, 1rw−r
1×
3×
2×2, 1rw−r
 ,1itλ−
 
Figure 4.10. Iterative feature extraction model for third order tensors. 153  
From the definition of IFEM, which is defined by Eqs. (4.60), (4.61), and (4.62), we know that IFEM can be calculated by  a greedy approach. The calculation of 
,1|N
ir i= X  is based on the given ,1 1|N
ir i−= X  and ,1 1|M
kr kw−=r. With the given ,1 1|N
ir i−= X  
and,1 1|M
kr kw−=r, we can calculate ,1irλ− via (4.61). The projection vectors ,1|M
kr kw=r 
can be obtained by optimizing (4.62) through the alternating projection method in 
Table 4.1. The flowchart of the algorithm for feature extraction for third order 
tensors is illustrated in Figure 4.10. 
With IFEM, we can obtain 1
,1|rR
kr k Mw≤≤
≤≤r iteratively. The coordinate values ,1|R
ir rλ= 
can represent the original tensor iX. For example, in nearest neighbor based 
recognition, the prototype tensor pX for each individual class in the database 
and the testing tensor tX to be classified are projected onto the bases to get the 
prototype vector ,1|R
pr rλ=and the testing vector ,1|R
tr rλ=. The testing tensor class is 
found by minimizing the Euclidean distance ()2
,, 1R
tr pr rελ λ==−∑  over p. 
As an example, we develop the tensor ra nk one discriminant analysis (TR1DA) by 
combining IFEM with differential scatter discriminant criterion (DSDC) described 
in Chapter 2. TR1DA deals with the multiple classes cl assification problem. Suppose: there are 
N training samples 
12
;M LL L
ijR××∈XL. The ;,ij rX  is the jth (1i jN≤≤ ) training 
sample in the ith (1iC≤≤ ) class for the rth iteration for feature extraction. If r 
equals to 1, we have ;, 1 ;ij ij= XX . The ith class mean tensor in the rth iteration is 
,; ,
11iN
ir i jr
jiN==∑ MX  and the total mean tensor in the  rth iteration is 
;, ,
11 111iN CC
i
ri j r i r
ii i iN
CN N== ===∑∑ ∑ MX M . The kth projection vector in the rth iteration is 
defined by ,krwr. With the given ;,ij rX  and ,krwr, the ()1r+th iteration for 
feature extraction in TR1DA is defined by 
;, 1 ;, ;, ,
1M
ij r ij r ij r k r
kwλ+⊗
==−∏ XXr (4.63)
;, ;, ,
1kM
T
ij r ij r k r
kw λ×
==∏ Xr (4.64)154 ()
(),12
,,
1 1 *
,1 2|
;, , ,
11 1|a r g m a xk
Mikrk
kM C
ii r r k r
i k M
kr kN M C w
ri j r i r k r
ij kNw
w
w ζ=×
= =
=
×
== =⎛⎞
− ⎜⎟
⎜⎟=⎜⎟
⎜⎟−−⎜⎟⎝⎠∑ ∏
∑∑ ∏MM
XMrr
r
r. (4.65)
 
Table 4.2. Alternating Projection for the Tensor Rank One Discriminant Analysis 
Input: Training samples 12 ...
;M LL L
ijR×× ×∈X  (1ic≤≤, 1ijn≤≤), the number R of 
rank one tensors allowed in TR1DA, and tuning parameters rζ, 1rR≤≤  in 
TR1DA. 
Output: Projection vectors { } ,1,|,kL M
kr k krww R=∈rr and scalars ;,ij rλ, 1 rR≤≤ . 
1. For 1r= to R 
2. Set ,1|M
kr kw=r be equal to random unit vectors. 
3. Calculate the class mean tensor (); 11iN
ii i j jN==∑ MX ; 
Calculate the total mean tensor ()1C
ii iNN==∑ MM ; 
4. Carry out steps 5–10 iteratively until convergence.  
5. For 1k= to M 
6. Calculate ()() ()() ,,
1C
TT
ii k k r i k k r
iBN w w
=⎡ ⎤ =− × ⊗ − ×⎣ ⎦ ∑ MM MMrr; 
Calculate ()() ()() ;, ;,
11iN C
TT
i j ik k r i j ik k r
ijWw w
==⎡ ⎤ =− × ⊗ − ×⎣ ⎦ ∑∑ XM XMrr; 
7. Calculate the eigenvector hr
 of lB Wζ−  associated with the 
largest eigenvalue; 
8. Assignment: ,krwh←rr; 
9. End 
10. Convergence checking: if ,, ,, 1 ,, 101T
krt krt krtww w ε−−− ≤rrr (610ε−= ) for 
all modes, the calculated ,krwr has converged. Here ,,krtwr is the 
current projection vector and ,, 1krtw−r is the previous projection vector. 
11. Assignment: ;, ; ,
1kM
T
ij r ij k r
kw λ×
=←∏Xr; 
12. Assignment: ;; ; , ,
1M
ij ij ij r k r
kwλ⊗
=←−∏ XXr; 
13. End 
 155 The algorithm is given in Table 4.2. 
The time complexity of PCA (LDA) is ()3
1M
k kOL=⎛⎞⎜⎟⎝⎠∏  when training samples 
X belong to 12 M LL LR×××L. The time complexity of TR1DA is 
( )11RM
rk rkOTL==∑∑ , where rT is the number of iterations to make TR1DA 
converge for the rth feature extraction procedure (in our experiments for human 
gait recognition [143][145], rT is about 20). The space complexity of PCA 
(LDA) is ()2
1M
k kOL=⎛⎞⎜⎟⎝⎠∏ . The space complexity of TR1DA is ()1M
k kOL=∏ . 
This indicates that the time complexity and the space complexity of feature 
extraction are reduced by working directly with tensor data rather than vectorizing 
the data and applying PCA (LDA).  156  Experiments 
 
In this Section, we provide two experi ments to demonstrate that STL and IFEM 
are powerful tools for classification and feature extraction. For STL, we 
implement TMPM for image classificati on. For IFEM, we implement TR1DA for 
the elapsed time problem in human gait recognition.  
 TMPM for Image Classification 
 
 
Figure 4.11. Attention model for image representation. 
 
To classify images into groups based on their semantic contents is very important and challenging. The simplest classificati on is binary and a hierarchical structure 
can be built from a series of binary classi fiers. If we have semantic classification 
then image databases would be easier to manage [121][125]. The image semantic 
classification is also of grea t help for many applications. 
In this STL based classification experime nt, two groups of images are separated 
from each other by a trained TMPM, which is a generalized learning machine with the STL framework. The input (represen ting features) of TMPM is the region 157 of interest (ROI) within an image, which are extracted by the attention model in 
[56][57][140] and represented as a third order tensor. The attention model [56][57] is capable of reproducing human performances for a 
number of pop–out tasks [157]. In other words, when a target is different from its 
surroundings by its unique orientation, color, intensity, or size, it is always the first attentive location and easy to be noticed by an observer. Therefore, it is 
reasonable to utilize the attention model based ROI to describe the semantic 
information of an image.  
 
Figure 4.12. Example images from the tiger category. 
 
As shown in Figure 4.11, representing an attention region from an image consists  
of several steps: 1) extracting the salient map as introduced by Itti et al.  [56][57]; 158 2) finding the most attentive region, whose center has the largest value in the 
salient map; 3) extracting the attention re gion by a square, i.e., ROI, in size of 
64 64×; and 4) finally, representing this ROI in the hue, saturation, and value 
(HSV) perceptual color space. We have a third order tensor for the image 
representation. 
 
 
Figure 4.13. Example images from the leopard category. 
 
Note that although we only select a small region from an image, the size of the 
extracted third order tensor is already as large as 64 64 3××. If we vectorize this 
tensor, the dimension of the vector will be 12288 . The sizes of training samples 
are only of hundreds, which is much smaller than 12288 . Therefore, the small 
sample size (SSS) problem always arises . On the contrary, our proposed tensor 159 oriented supervised learning scheme can avoid this problem directly and at the 
same time represent the ROIs much more naturally.  
 
Figure 4.14. One hundred ROIs in the tiger category. 
 
The training set and the testing set for the following experiments are built upon 
the Corel photo gallery [164], from which 100 images are selected for each of the 
two categories. Examples are shown in Figure 4.12 and Figure 4.13. These 200 images are then processed to extract the third tensor attention features for TMPM 
as shown in Figure 4.14 and Figure 4.15. 
We choose the “Tiger” category shown in Figure 4.12 and the “Leopard” category 
shown in Figure 4.13 for this binary classification experiment since it is a very 
difficult task for a machine to distinguish them, although a human being can 
easily differentiate between a tiger and a leopard. Basically, the characteristics of a classifier cannot be examined in deta il if the classification problem is very 160 straightforward, for example, classifying grassland pictures from blood pictures. 
The “Tiger” and “Leopard” classification is carried out in this Section. We choose the top N images as training sets according to the image IDs, while all remaining 
images are used to form the corresponding testing set. 
 
 
Figure 4.15. One hundred ROIs in the leopard category. 
 
In our experiments, the third order tens or attention ROIs can mostly be found 
correctly from images. Some successful results, respectively extracted 
automatically from the “Tiger” category and the “Leopard” category, are shown in 
Figure 4.14 and Figure 4.15. By this we mean that, the underlying data structures 
are kept well for the next classification step. However, we should note that the 
attention model sometimes cannot depict th e semantic information of an image. 
This is mainly because the attention model always locates a region that is different from its surroundings and thus might be “cheated” when some complex or bright 161 background exists. Some unsuccessful ROIs can also be found from Figure 4.14 
and Figure 4.15. It should be emphasized that to keep the following comparative experiments fair and automatic, these wrongly extracted ROIs are included in 
training samples. 
We carried out the binary classification (“Tiger” and “Leopard”) experiments upon the above training and testing sets. The proposed TMPM is compared with 
MPM. The experimental results are show n in Table 4.3. Error rates for both 
training and testing are reported according to  the increasing size of the training set 
(STS) from 5 to 30. 
 
Table 4.3. TMPM vs. MPM 
 Training Error Rate Testing Error Rate 
STS TMPM MPM TMPM MPM 
5 0.0000 0.4000 0.4600 0.5050 
10 0.0000 0.5000 0.4250 0.4900 
15 0.0667 0.4667 0.3250 0.4150 
20 0.0500 0.5000 0.2350 0.4800 
25 0.0600 0.4800 0.2400 0.4650 
30 0.1167 0.5000 0.2550 0.4600 
 
From the training error rates in Table 4.3, it can be seen that the traditional 
method (MPM) cannot learn a suitable model for classification when the size of 
the training set is much smaller than the dimension of the feature space. It shows that the proposed TMPM algorithm is more effective than MPM at representing 
the intrinsic discriminative information (in the form of the third order ROIs). 
TMPM learns a better classification model for future data classification than 
MPM and thus has a satisfactory performan ce on the testing set. It is also 
observed that the TMPM error rate is a decreasing function of the size of the 
training set. This is consistent with statistical learning theory. We also evaluate TMPM as a sample algorithm of the proposed STL framework. 
Two important issues in machine learni ng are studied, namely, the training stage 
convergence property and the insensitiveness to the initial values.  
Figure 4.16 shows that as the training st age of TMPM converges efficiently by the 
alternating projection method. Usually, twenty iterations are enough to achieve 
convergence. 
 162 0 10 20 30 40 50 60 70 80 90 100−4−3.5−3−2.5−2−1.5−1
Number of IterationsPositionTensor MPM converges on the training data
0 10 20 30 40 50 60 70 80 90 100−0.1−0.0500.050.10.150.20.250.30.350.4
Number of IterationsError RateTraining and Tesing Error Rate vs. Number of Iterations
Training Error
Testing  Error
 
 
0 10 20 30 40 50 60 70 80 90 100−6−5−4−3−2−10
Number of IterationsPositionTensor MPM converges on the training data
0 10 20 30 40 50 60 70 80 90 10000.050.10.150.20.250.30.350.4
Number of IterationsError RateTraining and Tesing Error Rate vs. Number of Iterations
Training Error
Testing  Error
 
 
0 10 20 30 40 50 60 70 80 90 100−6−5−4−3−2−1012
Number of IterationsPositionTensor MPM converges on the training data
0 10 20 30 40 50 60 70 80 90 1000.10.150.20.250.30.35
Number of IterationsError RateTraining and Testing Error Rate vs. Number of Iterations
Training Error
Tesing   Error
 
Figure 4.16. TMPM converges effectively. 
 
Three sub–figures in the left column of Figure 4.16 show tensor projected values 
of the original general tensors with an  increasing number of learning iterations 
using 10, 20, and 30 training samples for each class, respectively, from top to bottom. We find that the projected values  converge to stable values. Three sub–
figures in the right column of Figure 4.16 show the training error rates and the 
testing error rates according to the increasin g number of learning iterations by 10, 
20, and 30 training samples for each class, respectively, from top to bottom. 163 Based on all sub–figures in Figure 4.16, it can be found that the training error and 
the testing error converge to stable values, which empirically justify the convergence of the alternating projec tion method for TMPM. The theoretical 
proof is given in the Theorem 4.1. 
 
0 10 20 30 40 50 60 70 80 90 10000.050.10.150.20.25
Experimental RoundsError RateTensor Minimax Probability Machine is stable with different initial values
Testing Error Rate
Training Error Rate
 
Figure 4.17. TMPM is stable with different initial values. 
 
Many learning algorithms converge to different values when initial values are 
varied. This is the so–called local minimum  problem. However, the developed 
TMPM does not have this local minimum  problem, which is demonstrated by a set 
of experiments (with 100 different initial parameters, 10 learning iterations, and 
20 training samples), as shown in Figure 4.17. The training error rates and the 
testing error rates are always 0.05 and 0.235, respectively. Moreover, because 
TMPM is a convex optimization problem, theoretically TMPM has a unique 
solution. 
 
 TR1DA for the Elapsed Time Problem in Gait Recognition 
To study the characteristics of the proposed TR1DA, we utilize it on the elapsed 
time problem in human gait recognition. In  this Section we first briefly introduce 
our experimental data (gallery and probe) sets; and then report the performance of 
the TR1DA algorithm and compare its performance with principal component 164 analysis (PCA), linear discriminant analysis (LDA) and tensor rank one analysis 
(TR1A). The experiments provide numerical evidence for the convergence of TR1DA during the learning stage. 
Research efforts in biometrics are mainly motivated by the increasing 
requirements of machine based automatic human authentication/authorization [71]. As a promising biometric source, human gait describes the manner of a 
person’s walking and can be acquired at a distance. It was first analyzed in 
medical surgery [111][112], and then in psychology [23]. In computer vision, 
human motion has been studied for years [1]. An early attempt to recognize a 
person by the gait was probably made by Little and Boyd [86]. Since then, many 
efforts have been made for gait recognition [9][22][63][163]. In this Section, we focus on an appearance based model, which could be a preprocessing step for statistical models, such as Hidden Markov Model [65][66]. 
Our experiments are all carried out on the USF HumanID outdoor gait database 
[126], which consists of the following covariates: change in viewpoints (Left/L or Right/R), change in shoe types (A or B) , change in walking surface (Grass/G or 
Concrete/C), change in carrying conditions  (Briefcase/B or No Briefcase/NB), 
and elapsed time (May or November) between sequences being compared. The detailed description about the database is given in §3.1. 
Among all five covariates, elapsed time is our main concern in this Section 
because human identification systems ar e normally required to work over long 
periods of time. In this thesis, the remaining three conditions are examined thoroughly, namely change in shoe types (A or B), change in walking surface 
(Grass or Concrete), and change in carrying conditions (carrying a Briefcase or No Briefcase). Consequently, we choose our gallery set (May, C, A, L, NB) from 
the May data and consider eight pattern cl assification problems with the test sets 
of CAL (Nov., C, A, L, NB), CBL (Nov., C, B, L, NB) , GAL (Nov., G, A, L, 
NB), GBL (Nov., G, B, L, NB), CBAL (Nov., C, A, L, BF), CBBL (Nov., C, B, L, BF),  GBAL (Nov., G, A, L, BF), and GBBL (Nov., G, B, L, BF). 
In this Section we empirically study the TR1DA approach in terms of accuracy, 
convergence during the learning phase, and impact of parameter values on accuracy. TR1DA is compared with existing methods, including PCA, LDA, and 
TR1A. Experiments are carried out to rec ognize a gait with given examples of 165 gaits collected in the past (the elapsed time problem), which is still one of the 
most difficult problems in human gait recognition. In these experiments, we use EigenGait in a PCA based method for gait 
recognition, FisherGait in an LDA based method [45], TR1AGait in a TR1A for 
gait recognition, and TR1DAGait in a TR1DA for gait recognition. Figure 4.18 shows the first ten EigenGaits, Fish erGaits, TR1AGaits, and TR1DAGaits, 
respectively. We believe that TR1DA is a good method for classifying gaits 
because our experiments support this belief. First, TR1DAGait usually performs better than existing methods (EigenGait, FisherGait, and TR1AGait) in 
recognition tasks. Second, the training procedure for TR1DA converges within 
about 20 iterations. Third, it is not difficult to obtain a reasonable good 
performance of TR1DA by adjusting the tuning parameters 
rς (1 rR≤≤ ). This 
is because there is a wide range of values for rς, over which TR1DA achieves a 
good performance. In our experiments, although we extract R features, given 
R independent tuning parameters, we set a ll tuning parameters be equal to each 
other, in order to reduce the number of pa rameters. It is possible to achieve better 
performance if the values of the tuning parameters are allowed to be different.  
 166 
 
Figure 4.18. First 10 EigenGaits (the first column), first 10 FisherGaits (the 
second column), first 10 TR1AGaits (the  third column), and first 10 TR1DAGaits 
(the fourth column). From the figure, we  can see that EigenGaits and FisherGaits 
are dense, while TR1AGaits and TR1DAGaits are sparse, because they take the 167 structure information into account to reduce the number of unknown parameters 
in discriminant learning.  
Figure 4.19–Figure 4.26 illustrate the experimental results under eight different 
circumstances. In each of them: 
•
 The first sub–figure shows the effects of feature dimension on recognition 
precision (one minus the error rate). In this sub–figure, the x–coordinate is 
the feature dimension and the y–coordina te is the recognition precision. In 
order to keep the graph be in a manageable size, we only show the results for feature dimensions from 10 to 110. In these experiments, we show the top–one recognition precisions of EigenGait, FisherGait, TR1AGait, and 
TR1DAGait. We show the top–one and top–five recognition precisions of 
these algorithms in Table 4.5 and Table 4.6, respectively. Usually, tensor based algorithms (TR1AGait and TR1DAGait) achieve better recognition 
precisions than vector based algorithms (EigenGait and FisherGait) and 
TR1DAGait performs better than TR1AGait. 
 
•
 The second sub–figure shows the effects of feature dimension and the 
tuning parameter ς on the recognition precision in TR1DA. In this sub–
figure, the x–coordinate is the feature dimension; the y–coordinate is the 
tuning parameter ς; and the z–coordinate is the recognition precision. 
The feature dimension changes from 10 to 200 with a step 1 and ς 
changes from 0.01 to 0.5 with a step 0.01. For each probe the tuning 
parameter ς is chosen to achieve the best performance. The detailed 
information about the value of ς is given in Table 4.4. In each case there 
is a range of values of ς for which TR1DA achieves a good 
performance. We only show one value for each probe. It can be observed 
from the sub–figure that for each probe, there are a number of points to 
achieve the best performance according to different ς values. 
 
• The third sub–figure  shows the number of iterations required in the 
training procedure for extracting the ith feature. In this sub–figure, the x–
coordinate is the feature dimension and the y–coordinate is the number of 168 iterations required for convergence. The mean value of the number of 
iterations for convergence is represented by the dashed line. In these experiments, we set the maximum number of training iterations as 1,000. 
The training procedure usually converges within 80 iterations. The mean 
number of training iterations for di fferent features is about 20 and the 
standard deviation is about 10. Detailed information about the number of 
iterations required can be found in Table 4.4. 
 
•
 The four bottom sub–figures examine the training procedure convergence 
property of TR1DA. Because different features share similar convergence 
curves, we only show the convergence curves of the first two features. The 
left column is relevant to the 1st feature and the right column is relevant to 
the 2nd feature. In the upper two of the four bottom–right sub–figures, the 
x–coordinate is the number of the training iterations t and the y–coordinate 
is the log of differences in a proj ection direction between two neighboring 
training iterations, i.e., they demonstrate how 
() () () ( ) 1 0log 1T
kk ktt tuu u−−  and () () () ( ) 1 0log 1T
kk ktt tvv v−−  
(mentioned in Table 4.2) change with the training iterations for the 1st and 
2nd features ( 1, 2k= ). Here, 0x is the dimension of x; ()ktu is the 
first projection vector of the tth training iteration for the kth feature; and 
()ktv is the second projection vector of the tth training iteration for the kth 
feature. In these experiments, the visual objects are averaged gait images, 
which are second order tensors.  To avoid confusion, we use u and v to 
represent the projection vectors in different directions. From the upper two 
sub–figures, we can see that as the numb er of training iterations increases, 
the changes in u and v approach to zero. In the lower two of the four 
bottom–right sub–figures, the x–coordinate denotes the number of training 
iterations and the y–coordi nate is the function value f of TR1DA 
defined in (13), i.e., they show how the function value of TR1DA defined 
in (13) changes with the training iterations for the 1st and 2nd features. 
From the sub–figures, we can see that as  the number of training iterations 
increases, the change of the function value of TR1DA approaches zero. 
All of these sub–figures demonstrate that the training procedure of 169 TR1DA converges after about 20 iterations. If 
() () () ( ) 1 0log 1T
kk ktt tuu u ε−− < and () () () ( ) 1 0log 1T
kk ktt tvv v ε−−< , 
we deem the training procedure converges. The value of ε is 610−. 
 
Table 4.4. Parameters in convergence examination for eight probes. 
TR1DA GBL GAL CBL CAL GBBL GBAL CBBL CBAL
Max 58 63 55 67 65 73 54 53 
Mean 21 19 18 19 20 22 18 19 
Std 11 10 10 11 11 14 11 10 
ς 0.03 0.17 0.25 0.31 0.02 0.05 0.20 0.28 
 
Table 4.5. Rank One recognition precision for eight probes. 
Rank One GBL GAL CBL CAL GBBL GBAL CBBL CBAL
EigenGait 11.00 14.66 13.44 18.28 09.50 10.00 11.29 09.68 
FisherGait 07.50 17.80 13.98 18.82 09.00 13.00 07.53 16.13 
TR1AGait 20.49 17.80 18.11 13.98 23.48 15.00 17.74 15.98 
TR1DAGait 24.59 18.85 21.26 16.13 23.48 16.00 20.97 17.16 
 
Table 4.6. Rank five recognition precision for eight probes. 
Rank Five GBL GAL CBL CAL GBBL GBAL CBBL CBAL
EigenGait 28.00 29.84 33.87 41.94 31.00 24.50 31.72 30.11 
FisherGait 25.00 32.98 40.32 41.94 24.00 29.00 23.12 33.87 
TR1AGait 36.07 30.89 40.94 33.33 38.64 28.50 35.48 35.50 
TR1DAGait 37.70 30.37 44.09 34.41 39.39 29.00 36.29 36.69 
 170 10 20 30 40 50 60 70 80 90 100 11000.050.10.150.20.25
Feature DimensionGBL: Feature Dimension vs. Precision
EigenGait
FisherGait
TR1AGait
TR1DAGait
 
 171 0 20 40 60 80 100 120 140 160 180 2000102030405060Feature Dimension vs. Number of Iterations for Convergence
Feature DimensionNumber of Iterations for Convergence
 
 
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-501st Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-502nd Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
 
0 5 10 15 20 25 30 35 401015202530354045501st Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
0 5 10 15 20 25 30 35 4051015202nd Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
 
Figure 4.19. GBL task: TR1DA performan ces, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §233066540. 
 
  172 10 20 30 40 50 60 70 80 90 100 11000.020.040.060.080.10.120.140.160.180.2
Feature DimensionGAL: Feature Dimension vs. Precision
EigenGait
FisherGait
TR1AGait
TR1DAGait
 
 173 0 20 40 60 80 100 120 140 160 180 200010203040506070Feature Dimension vs. Number of Iterations for Convergence
Feature DimensionNumber of Iterations for Convergence
 
 
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-501st Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-502nd Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
 
0 5 10 15 20 25 30 35 401001502002503001st Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
0 5 10 15 20 25 30 35 4050607080901001101201302nd Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
 
 
Figure 4.20. GAL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §0. 
 
 174  
10 20 30 40 50 60 70 80 90 100 1100.020.040.060.080.10.120.140.160.180.20.22
Feature DimensionCBL: Feature Dimension vs. Precision
EigenGait
FisherGait
TR1AGait
TR1DAGait
 
 175 0 20 40 60 80 100 120 140 160 180 2000102030405060Feature Dimension vs. Number of Iterations for Convergence
Feature DimensionNumber of Iterations for Convergence
 
 
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-501st Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-502nd Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
 
0 5 10 15 20 25 30 35 401502002503003504004501st Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
0 5 10 15 20 25 30 35 40801001201401601802nd Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
 
Figure 4.21. CBL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §0. 
 
  176  
10 20 30 40 50 60 70 80 90 100 11000.020.040.060.080.10.120.140.160.180.2
Feature DimensionCAL: Feature Dimension vs. Precision
EigenGait
FisherGait
TR1AGait
TR1DAGait
 
 177 0 20 40 60 80 100 120 140 160 180 200010203040506070Feature Dimension vs. Number of Iterations for Convergence
Feature DimensionNumber of Iterations for Convergence
 
 
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-501st Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-502nd Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
 
0 5 10 15 20 25 30 35 401502002503003504004505005506001st Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
0 5 10 15 20 25 30 35 401001201401601802002202402nd Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
 
Figure 4.22. CAL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §0. 
 
  178 10 20 30 40 50 60 70 80 90 100 11000.050.10.150.20.25
Feature DimensionGBBL: Feature Dimension vs. Precision
EigenGait
FisherGait
TR1AGait
TR1DAGait
 
 
 
 179 0 20 40 60 80 100 120 140 160 180 200010203040506070Feature Dimension vs. Number of Iterations for Convergence
Feature DimensionNumber of Iterations for Convergence
 
0 5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-501st Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
0 5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-502nd Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
 
0 5 10 15 20 25 30 35 40510152025301st Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
0 5 10 15 20 25 30 35 4023456789101112132nd Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
 
Figure 4.23. GBBL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §0. 
 
 
  180 10 20 30 40 50 60 70 80 90 100 11000.020.040.060.080.10.120.140.16
Feature DimensionGBAL: Feature Dimension vs. Precision
EigenGait
FisherGait
TR1AGait
TR1DAGait
 
 
 181 0 20 40 60 80 100 120 140 160 180 20001020304050607080Feature Dimension vs. Number of Iterations for Convergence
Feature DimensionNumber of Iterations for Convergence
 
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-501st Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-502nd Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
 
0 5 10 15 20 25 30 35 4020304050607080901st Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
0 5 10 15 20 25 30 35 4015202530352nd Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
 
 
Figure 4.24. GBAL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §0. 
 
  182 10 20 30 40 50 60 70 80 90 100 11000.020.040.060.080.10.120.140.160.180.2
Feature DimensionCBBL: Feature Dimension vs. Precision
EigenGait
FisherGait
TR1AGait
TR1DAGait
 
 183 0 20 40 60 80 100 120 140 160 180 2000102030405060Feature Dimension vs. Number of Iterations for Convergence
Feature DimensionNumber of Iterations for Convergence
 
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-501st Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-502nd Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
 
0 5 10 15 20 25 30 35 401001502002503003501st Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
0 5 10 15 20 25 30 35 40607080901001101201301401502nd Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
 
Figure 4.25. CBBL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §0. 
 
 
  184 10 20 30 40 50 60 70 80 90 100 11000.020.040.060.080.10.120.140.160.18
Feature DimensionCBAL: Feature Dimension vs. Precision
EigenGait
FisherGait
TR1AGait
TR1DAGait
 
 
 
  185 0 20 40 60 80 100 120 140 160 180 2000102030405060Feature Dimension vs. Number of Iterations for Convergence
Feature DimensionNumber of Iterations for Convergence
 
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-501st Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
5 10 15 20 25 30 35 40-40-35-30-25-20-15-10-502nd Feature Convergence Examination
Iteration NumberLog of ErrorU Direction
V Direction
 
0 5 10 15 20 25 30 35 401502002503003504004505005501st Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
0 5 10 15 20 25 30 35 40801001201401601802002202nd Feature TR1DA Funcation Value
Iteration NumberTR1DA Function Value
 
Figure 4.26. CBAL task: TR1DA performances, convergence, and parameters 
(feature dimension and ς) analysis. Detailed explanation is in §0. 
 
Based on Figures 4.19 – 4.26, we have the following observations: 
1. The first sub–figure of each figure shows the classification performance versus the selected feature dimension. Most of these sub-figures show that 186 tensor based subspace selection methods (TR1DA and TR1A) perform better 
than vector based ones (LDA and PCA) and the discriminative subspace selection methods (TR1DA and LDA) perform better than reconstructive 
subspace selection methods (TR1A and PCA). This is because the size of the 
gallery, i.e., the training set, in different tasks is much less than the dimension of original features, i.e., the appearance of the averaged gait image for 
representation. According to the discussions at the beginning of this Chapter, 
we know tensor based subspace select ion methods reduce the over fitting 
problem when training samples are limited, so TR1DA and TR1A should 
perform better than LDA and PCA, re spectively. Moreover, tasks here are 
classification so discriminative based methods perform better than reconstructive based methods as discussed at the beginning of Chapter 2; 
2.
 The second sub–figure of each figure shows the classification performance of 
TR1DA versus the parameters in TR 1DA. There are two parameters for 
TR1DA, which are ς and selected feature dime nsion. Some of these sub-
figures show that the more the selected features are, the higher the 
classification accuracy is for the first selected 200 features; and some of them 
show that with the increasing number of selected features, the classification 
accuracy is initially increasing and then decreasing. From a large number of experiences in subspace selection, we know the latter is more reasonable than 
the former, so we believe we can have the similar observations as the latter if 
more features are selected for evaluation; 
3.
 The third sub–figure of each figure shows the number of iterations required to 
meet the training stage convergence condition in TR1DA for different 
features. According to Table 4.4, normally, TR1DA needs about 20 iterations to converge and the corresponding standard deviation is about 10. Therefore, 
for most features, we need about 30 iterat ions for training. The distributions of 
the number of training iterations are similar to noise, i.e., there is no clear relationship between the number of training iterations and the selected features. This is because the training processes for different features are 
independent, as shown in Section 4.4; and 
4.
 The last four sub–figures of each figu re demonstrate the convergence property 
of the training stage of TR1DA. The first two sub–figures show that the 
difference between the projection vectors of two consecutive training 187 iterations is decreasing with the increa sing number of the training iterations; 
and the last two sub–figures show the TR1DA function value is increasing 
with the increasing number of the training iterations. These two points consist 
with the mathematical proof of the theorem 4.1.  
  188  Summary 
 
In this Chapter, the vector based learni ng is extended to accept tensors as input. 
The result is a supervised tensor learning (STL) framework, which is the 
multilinear extension of convex optimization based learning. To obtain the 
solution of an STL based learning algorithm,  the alternating projection method is 
used. Based on STL and its alternating projection optimization algorithm, we 
illustrate several examples. That is we extend the soft margin support vector 
machine (SVM), the nu–SVM, the least squares SVM, the minimax probability 
machine (MPM), the Fisher discriminant analysis (FDA), the distance metric learning (DML) to their tensor versions, which are the soft margin support tensor 
machine (STM), the nu–STM, the least squares STM, the tensor MPM (TMPM), 
the tensor FDA (TFDA), and the multiple distance metrices learning (MDML). 
Based on STL, we also introduce a method for iterative feature extraction: the 
iterative feature extraction model (IFEM). As an example, we develop the tensor 
rank one discriminant analysis (TR1DA). Finally, we implement TMPM for im age classification and TR1DA for the 
elapsed time problem in human gait r ecognition. By comparing TMPM with 
MPM, we show that TMPM reduces the overfitting problem in MPM. By 
comparing TR1DA with principal component analysis (PCA), linear discriminant 
analysis (LDA), and tensor rank one analysis (TR1A), we show that TR1DA 
reduces the small sample size problem a nd always achieves the best performance 
on the elapsed time problem in human gait recognition. 
 
 189 5. Thesis Conclusion 
 
Linear discriminant analysis (LDA) motivates this thesis. This is because LDA has many problems, although it has been deemed as one of the most important 
linear subspace methods in pattern classifi cation and it has been widely applied in 
many applications, e.g., face recognition [34][168], image retrieval [139][152] [153][154], video data organization [43], ga it recognition [46], speech recognition 
[67][70], document classification [108], music management [85], network flow 
analysis [136], and video surveillance [143][144]. The first type of the problems with LDA is model based: 1) the heteroscedastic 
problem [29][28][70][61][99], 2) the mu ltimodal problem [51][28], and 3) the 
class separation problem [103][95][96][99 ][146]. To deal with the model based 
problems, we begin with the fact that LDA selects the subspace to maximize the arithmetic mean of the Kullback–Leibler (KL) [21] divergences between different 
classes, when samples are sampled from Gaussian distributions [60] with identical covariances. We then generalize LDA in two ways: 1) extending the KL 
divergence to the Bregman divergence [14]; and 2) extending the arithmetic mean 
to the generalized mean [48]. The result of these generalizations is the general 
averaged divergences analysis and the significance of the generalization is a discriminative subspace selection framew ork, from which we can develope a 
number of different methods to select discriminative features. Based on this 
framework, we analyze the geometric based subspace selection: 1) the geometric mean of divergences between different classes, 2) the geometric mean of normalized divergences between different classes, and 3) the geometric mean of 
all divergences (the divergences and the normalized divergences) between different classes. The first method is studied because the geometric mean 
increases the effects of small divergences and at the same time reduces the effects 
of large divergences. The second method is studied to further reduce the effects of 
large divergences. The intuition is the product of normalized divergences is large when they are similar to each other. However, the second method cannot be 
directly used for subspace selection. This  is because there are subspaces in which 
all divergences become small, but all normalized divergences are comparable in size. Consequently, we linearly combine the first and the second methods 190 together. To apply the geometric mean based subspace selection for real 
applications, we suppose the samples are sampled from Gaussian distributions and 
we use the KL divergence to measure differences between different classes. 
Because we drop the identical covariances assumption in LDA, we do not meet 
the heteroscedastic problem. The advantag es of the combination of the geometric 
mean and the KL divergences are: 1) it is compatible with the heteroscedastic 
property of the distributions of samples in different classes; 2) it selects suitable 
discriminative subspace when samples are drawn from Gaussian mixture models; and 3) it significantly reduces the clas s separation problem when KL divergences 
of difffernt classes are not evenly distributed. For real applications, we use the 
Gaussian mixture model to model the samples in each class, and thus avoid the multimodal problem. Based on a large nu mber of experiments, from synthetic 
data to hand writing digital recognition, the geometric mean combined with the 
KL divergence outperforms LDA and its representative extensions. 
The second type of these problems with LDA is the small sample size (SSS) problem [38][49][139][123][19][175][55][173][174][180]. LDA meets this 
problem when the number of training samples is less than the dimension of the 
feature space. In computer vision research, this problem can be reduced in a natural way by introducing the structure information as constraints, because 
objects in computer vision research are always represented by multidimensional 
arrays, i.e., tensors [75]. For example, a face image in face recognition, an averaged gait image in human gait recognition, and a video shot in video management. Although there are some algorithms applying the structure 
information for subspace selection, e.g., tensor rank one analysis (TR1A) [132], general tensor analysis (GTA) [75][162][171], and two dimensional linear 
discriminant analysis (2DLDA) [174], each of these methods has its own 
drawbacks for classification. TR1A and GT A are reconstructive models, i.e., they 
are used to produce representations fo r sufficient reconstruction but not for 
classification. The 2DLDA fails to converge in the training stage, although its 
effectiveness and efficiency have been  demonstrated through face recognition 
applications. Here, we propose a different  discriminative multilinear subspace 
method, the general tensor discriminant analysis (GTDA) [144][147], by 
combining the differential scatter discriminant criterion and the operations in 
multilinear algebra [115][75]. Compared  with TR1A, GTA, and 2DLDA, GTDA 191 has the following benefits: 1) provision with a converged alternating projection 
training algorithm to obtain a solution, while 2DLDA does not; 2) preservation of more discriminative information of training samples; 3) acceptance of general 
tensors as input; and 4) reduction of  the SSS problem in the subsequent 
classification, e.g., by LDA. We further extend GTDA by combining the manifold learning [13] and the operations in multilinear algebra to make the manifold 
learning algorithms accept general tensors as input. To examine the effictiveness 
and the efficiency of GTDA, we apply it for human gait recognition. Based on a great deal of comparison, GTDA combined with LDA always achieves the top 
level performance for USF HumanID gait database [126]. 
In supervised learning [30][39], when the size of training samples is small, learning machines encounter the overfitting problem. Similar to the motivation in GTDA, we also utilize the structure information as constraints to reduce the 
overfitting problem by decreasing the number of parameters needed to model the 
training samples. This results in a supe rvised tensor learning (STL) [149][150] 
framework. The framework extends the convex optimization [11] based learning 
to accept general tensors as input. To obtain the solution of the algorithms under 
the STL framework, we develop an altern ating projection method. Based on STL 
and its alternating projection optimizati on algorithm, we illustrate the following 
examples, which are the support tensor machine (STM), the tensor minimax 
probability machine (TMPM), the tensor Fisher discriminant analysis (TFDA), the multiple distance metrics learning (M DML). Motivated by TR1A and based on 
STL, we develop the tensor rank one discriminant analysis (TR1DA), which is an 
iterative discriminative feature extraction method. Finally, we implement TMPM and TR1DA for image classification and the elapsed time problem in human gait 
recognition, respectively. By compari ng TMPM with MPM, we know TMPM 
reduces the overfitting problem in supervised learning. By comparing TR1DA 
with principal component analysis (PCA), linear discriminant analysis (LDA), and tensor rank one analysis (TR1A), we know TR1DA reduces the SSS problem. 
In summary, this thesis deals with non trivial problems in discriminative subspace 
selection, which are how to select the most discriminative subspace for classification and how to deal with the SSS problem or over-fitting problem. The 
primary contributions of the thesis are as follows: 192 1) we develop a general averaged divergences analysis framework, which is 
a combination of the generalized mean function and the Bregman divergence, for discriminative subspace selection;  
2)
 under this framework, a new method, which combines the geometric 
mean and the Kullback–Leibler divergence, is developed to significantly reduce the class separation problem. With theoretical analysis, we know 
the method also reduces the heterosc edastic problem and the multimodal 
problem; 
3)
 the kernelization of this new method is also developed for nonlinear 
problems. Unlike existing kernel algorithms, we prove the kernel version 
is equivalent to the linear version followed by the kernel principal component analysis; 
4)
 a large number of empirical studies based on both synthetic and real data 
show the new discriminative subspace selection method performs better 
than LDA and its representative extensions; 
5) we propose GTDA to reduce the SSS problem for LDA. The advantages 
of GTDA compared with existing pre-processing methods, e.g., principal 
component analysis (PCA), tensor rank one analysis (TR1A), general tensor analysis (GTA), and two dimensional LDA (2DLDA), are: i) 
reduction of the SSS problem for subse quent classification, e.g., by LDA, 
ii) preservation of discriminative in formation in training tensors, while 
PCA, TR1A, and MSA do not guarantee this, iii) provision with stable recognition rates because the optimiz ation algorithm of GTDA converges, 
while that of 2DLDA does not, and iv) acceptance of general tensors as input, while 2DLDA only accepts matrices as input; 
6)
 we provide a mathematical proof to demonstrate the convergence property 
at the training stage. The significance of the proof is it is the first 
theoretical study for the convergence issue of the alternating projection 
based training algorithms for tensor subspace selection; 
7) we apply the developed GTDA to human gait recognition. By applying 
Gabor filters for averaged gait image representation, GTDA for subspace selection, and LDA for classification, we achieve the stat-of-the-art 
performance; and 193 8) we develop a STL framework and an alternating projection optimization 
algorithm to reduce the over–fitting problem in convex optimization based learning. Based on them, we propose STM, TMPM, TFDA, and 
MDML. Compared with existing popular classifiers, e.g., SVM, MPM, 
FDA, and DML, the advantages of their tensor extensions are: i) generalizing better for the small size training set than vector based 
classifiers, ii) converging well in the training stage compared with 
existing tensor classifiers, e.g., 2DLDA, and iii) reducing training computational complexities of vector based classifiers. 
 194 6. Appendices 
 
 Appendix for Chapter 2 
Let L
ixR∈r (1in≤≤ ) be a zero mean set of training samples, i.e., 
()110n
i imn x===∑rr. Let ()11nT
ii iSn x x==∑rr be the covariance matrix, and let 
U be the linear transformation matrix in PCA. The Frobenius norm is denoted by 
Fro⋅. We have the following properties of PCA as defined in Section 2.1. 
Property 2.1:  PCA maximizes the variance in the projected subspace, i.e., 
()2
11arg max tr arg maxn
TT
iFroUU iUS U Uxn==∑r.  
Proof. 
()
()1
2
111arg max tr arg max tr
11arg max tr arg maxn
TT T
ii
UU i
nn
TT T
ii iFroUU iiUS U U x x Un
Ux x U Uxnn=
==⎛⎞⎛⎞= ⎜⎟⎜⎟⎝⎠⎝⎠
==∑
∑∑rr
rr r 
Therefore, PCA maximizes the variance in the projected subspace.       ■ 
 
Property 2.3:  PCA minimizes the reconstruction error, i.e., 
()2
11arg max tr arg minn
TT
iiFroUU iUS U x U Uxn==−∑rr.  
Proof. 
It is sufficient to prove 22
1111arg max arg minnn
TT
ii iFro FroUU iiUx x U Uxnn===−∑∑rr r. 
() ()
()2
1
1
11arg min
1arg min
1arg minn
T
iiFroU i
nTTT
ii ii
U i
n
TT TT TT T T
i i ii ii i i
U ixU U xn
xU U x xU U xn
xx x UU x x UU x x UU UU xn=
=
=−
=− −= −−+∑
∑
∑rr
rr rr
r r rr rr r r 
In PCA, because TUU I=, we have 195 ()
()
()1
1
1
2
11arg min
1arg min
1arg min
1arg maxn
TT TT TT T T
i i ii ii i i
U i
n
TT T
ii i i
U i
n
TT
ii
U i
n
T
iFroU ixx x UU x x UU x x UU UU xn
xx x U Uxn
xU U xn
Uxn=
=
=
=−−+
=−
=−
=∑
∑
∑
∑r r rr rr r r
rr r r
rr
r 
T h e r e f o r e ,  P C A  m i n i m i z e s  t h e  r e c o n s t r u c t i o n  e r r o r .               ■ 
 
Property 2.4:  PCA decorrelates the training samp les in the projected subspace. 
Proof. 
Let U be the projection, which is calculated according to PCA. Project all 
training samples ixr using U as T
iiyU x=rr, where 'LLUR×∈ . 
Therefore, the covariance of the projected data is 
() ()
1111nnTTT T
yi i i i
iiSy y U x U xnn====∑∑rr r r 
1111nn
TT T T
ii ii
iiUx x U U x x Unn==⎛⎞==⎜⎟⎝⎠∑∑rr rr 
TUS U=  
Because TUS U  is a diagonal matrix, PCA decorrelates the training samples ixr 
i n  t h e  p r o j e c t e d  s u b s p a c e .                        ■ 
 
Property 2.5:  PCA maximizes the mutual information between xr and TyU x=rr 
on Gaussian data. 
Proof. 
Let U be the projection, which is calculated according to PCA. Project all 
training samples xr using U as TyU x=rr, where 'LLUR×∈ . 
The mutual information is () ( )()
()(),,, l o g
YXpxyI X Y p x y dXdYpxpy=∫∫rrrrrr  and we 
have ()()()() ,|IX Y H Y HYX H Y=− = , because TyU x=rr. Here, () HY  is 
the entropy of Y and ()| HY X  is the entropy of Y with the given X. 196 The training samples are normally distributed with zero mean and covariance S, 
so the projected training samples are also  normally distributed with zero mean and 
covariance T
ySU S U= . Therefore, the entropy () Hyr is given by 
()()() log Hy py y d y=−∫rr r r 
()1' 1log 2 log22 2yNS π =+ +  
()1' 1log 2 log22 2T NUS U π ≤+ + . 
Because U consists of the eigenvectors of S corresponding to the first largest 
'L eigenvalues of S, U maximizes the mutual information on Gaussian data. 
■ 
 
Observation 2.1:  LDA maximizes the arithmetic mean of the KL divergences 
between all pairs of classes, under the assumption that the Gaussian distributions 
for different classes all have the same covariance matrix. The optimal projection 
matrix U in LDA can be obtained by maximizing a particular ()VUϕ  defined 
in (2.38). 
Proof. 
According to (2.36) and (2.37), the KL divergence between the ith class and the jth 
class in the projected subspace with the assumption of equal covariance matrices 
(ijΣ= Σ= Σ ) is: 
()()( )1|| tr constantTT
Ui j i jDpp UUU D U−=Σ + .  
Then, we have 
()
1*a r g m a x | |ijU i j
U ij cUq q D p p
≤≠≤=∑  
()( ) ( )1
1    arg max trTT
ij i j
U ij cqq U U U DU−
≤≠≤=Σ∑  
()1
1    arg max trTT
ij i j
U ij cUUU q q D U−
≤≠≤⎛⎞ ⎛⎞=Σ ⎜⎟ ⎜⎟⎜⎟⎝⎠ ⎝⎠∑  
()11
11    arg max trcc
TT
ij i j
U ij iUUU q q D U−−
== +⎛⎞ ⎛⎞=Σ ⎜⎟ ⎜⎟⎜⎟⎝⎠ ⎝⎠∑∑ , 
where 1c
ii k kqn n==∑  is the prior probability of the ith class. 197 Because 1
11cc
bi j i j
ij iSq q D−
== +=∑∑ , as proved by Loog in [96], and tb wSSS=+= Σ  
where bS, wS, and tS are defined in (2.10), we have: 
() ()( )1
1arg max || arg max trTT
ijU i j w b
UU ij cqq D p p U S U U SU−
≤≠≤= ∑ .       ■  
 
The generalized geometric mean is upper bounded by the arithmetic mean of the 
divergences, i.e., 
() ()1
1 1
1|| ||ij
mn
mncqq
ijqq
Ui j Ui j
ij c ij c mn
mncqqDpp Dppqq≤≠≤
≤≠≤ ≤≠≤
≤≠≤⎛⎞
⎜⎟∑ ⎡⎤ ≤⎣⎦ ⎜⎟⎜⎟⎝⎠∑ ∏∑.  
Proof. 
Because 
1
11ij
ij c mn
mncqq
qq ≤≠≤
≤≠≤= ∑∑ and ()|| 0Ui jDpp >, according to the Jensen 
inequality  [21], we have 
()
1
1||ij
Ui j
ij c mn
mncqqDppqq ≤≠≤
≤≠≤∑∑ 
()()()
1
1exp log ||ij
Ui j
ij c mn
mncqqDppqq ≤≠≤
≤≠≤=∑∑ 
()()
1
1exp log ||ij
Ui j
ij c mn
mncqqDppqq ≤≠≤
≤≠≤⎛⎞
⎜⎟≥⎜⎟⎜⎟⎝⎠∑∑ 
()1
1||ij
mn
mncqq
qq
Ui j
ij cDpp
≤≠≤
≤≠≤∑ ⎡⎤=⎣⎦∏ .            
■ 
 
MGMD is a linear combination of 1) the log of the geometric mean of the 
divergences and 2) the log of the geometric mean of normalized divergences, i.e.,  
()()
() ()11
1
1
1log ||
*a r g m a x
1l o g | |ij
mn
mnccc
Ui j
ij c
qq U
qq
Ui j
ij cEpp
U
Dppα
α
≤≠≤−
≤≠≤
≤≠≤⎧⎫⎡⎤⎪⎪⎢⎥⎪⎪⎪⎪⎣⎦ =⎨⎬
⎪⎪∑ ⎡⎤ +−⎪⎪⎣⎦⎪⎪⎩⎭∏
∏ 198 where 01α<< . 
It is equivalent to 
() ()
11arg max log || log ||Ui j i j Ui j
U ij c ij cDpp q q Dpp η
≤≠≤ ≤≠≤⎧⎫ ⎛⎞ ⎪⎪− ⎨⎬ ⎜⎟
⎪⎪ ⎝⎠ ⎩⎭∑∑ ,  
where ()
() ()1
11
11mn
mnc
mn
mnccc qq
cc qqα
ηαα≤≠≤
≤≠≤−
=−− +∑
∑. 
 
Deduction 2.1:  
()()
() ()
() ()11
1
1
1
11log ||
*a r g m a x
1l o g | |
    arg max log || log || .ij
mn
mnccc
Ui j
ij c
qq U
qq
Ui j
ij c
Ui j i j Ui j
U ij c ij cEpp
U
Dpp
Dpp q q Dppα
α
η≤≠≤−
≤≠≤
≤≠≤
≤≠≤ ≤≠≤⎧⎫⎡⎤⎪⎪⎢⎥⎪⎪⎪⎪⎣⎦ =⎨⎬
⎪⎪∑ ⎡⎤ +−⎪⎪⎣⎦⎪⎪⎩⎭
⎧⎫ ⎛⎞ ⎪⎪=−⎨⎬ ⎜⎟
⎪⎪ ⎝⎠ ⎩⎭∏
∏
∑∑  
Deduction 
()()
() ()11
1
1
1log ||
*a r g m a x
1l o g | |ij
mn
mnccc
Ui j
ij c
qq U
qq
Ui j
ij cEpp
U
Dppα
α
≤≠≤−
≤≠≤
≤≠≤⎧⎫⎡⎤⎪⎪⎢⎥⎪⎪⎪⎪⎣⎦ =⎨⎬
⎪⎪∑ ⎡⎤ +−⎪⎪⎣⎦⎪⎪⎩⎭∏
∏ 
()()
()()1
1
11log ||1
    arg max1log ||Ui j
ij c
U
ij U i j
ij c mn
mncEppcc
qq D p pqqα
α≤≠≤
≤≠≤
≤≠≤⎧⎫
⎪⎪−⎪⎪=⎨⎬−⎪⎪+
⎪⎪
⎩⎭∑
∑∑ 
()()
()
()()1
1
1
1|| 1log1| |
    arg max
1log ||ijU i j
ij c mn U m n
mnc
U
ij U i j
ij c mn
mncqq D p p
cc qq D p p
qq D p pqqα
α≤≠≤
≤≠≤
≤≠≤
≤≠≤⎧⎫
⎪⎪−⎪⎪⎪⎪=⎨⎬−⎪⎪+⎪⎪
⎪⎪⎩⎭∑∑
∑∑ 199 ()()()
()1
1
11log ||1    arg max
log ||ijU i j
ij c mn
mnc
U
mn U m n
mncqq D p pcc qq
qqD p pα α
α≤≠≤
≤≠≤
≤≠≤⎧⎫⎛⎞− ⎪⎪⎜⎟+⎪⎪⎜⎟−⎪⎪⎜⎟=⎨⎬⎝⎠
⎪⎪⎛⎞⎪⎪−⎜⎟⎪⎪⎝⎠⎩⎭∑∑
∑ 
() ()
11    arg max log || log ||ijU i j ijU i j
U ij c ij cqq D p p qq D p p η
≤≠≤ ≤≠≤⎧⎫ ⎛⎞ ⎪⎪=−⎨⎬ ⎜⎟
⎪⎪ ⎝⎠ ⎩⎭∑∑  
()
()11
1log log ||
    arg max
log ||ij U i j
ij c ij c
U
ijU i j
ij cqq D p p
qq D p p η≤≠≤ ≤≠≤
≤≠≤⎧⎫ +
⎪⎪⎪⎪=⎨⎬⎛⎞⎪⎪−⎜⎟⎪⎪⎝⎠⎩⎭∑∑
∑ 
() ()
11    arg max log || log ||Ui j i j Ui j
U ij c ij cDpp q q Dpp η
≤≠≤ ≤≠≤⎧⎫ ⎛⎞ ⎪⎪=−⎨⎬ ⎜⎟
⎪⎪ ⎝⎠ ⎩⎭∑∑ , 
where ()
() ()1
11
11mn
mnc
mn
mnccc qq
cc qqα
ηαα≤≠≤
≤≠≤−
=−− +∑
∑ and 01α<<. The supremum of η is 
()1 cc− and the infimum of η is 0. When 0α=/ 0η=, (2.48) reduces to 
(2.43); and when 1α=/()1 ccη=−, (2.48) reduces to (2.46).         ■ 
 
Claim 2.1: ()() LU LU B= , where B is any orthogonal rr× matrix and U 
is the projection matrix, which maximizes () LU  defined in (2.51). 
Proof. 
Because () () ()
11log || log ||Ui j i jUi j
ij c ij cLU K L p p q qK L p p η
≤≠≤ ≤≠≤⎛⎞=− ⎜⎟
⎝⎠∑∑ , it is 
sufficient to prove ()() || ||Ui j U Bi jKLpp K L pp = . Here ip is the probability 
density function for samples in the ith class, i.e., ()() |iip px p xy i===rr. 
According to the definition of ()||Ui jKLpp  in (2.52), we have 
()
() ()( )
()( )1
11|| log log2
                         tr
                         tr ,TT TT
UB i j j i
TT TT
ji
TT TT
ji jK L p p BU U B BU U B
B U UB B U UB
BU U B BU DU B−
−=Σ − Σ
+Σ Σ
+Σ 200 where iΣ  is the covariance matrix of the ith class and 
()() ij i j i jDm m m m=−⊗−rr rr. 
Because ABA B=  when A and B are square matrices, we have  
()
() ()( )
()( )1
11|| log log2
                         tr
                         tr .TT
UB i j j i
TT TT
ji
TT TT
ji jKL p p U U U U
BUU B B U U B
BUU B B U D U B−
−=Σ − Σ
+Σ Σ
+Σ 
Because 
()()()
()11 11
1                        ,TT T T
jj
TT
jBU U B B U U B
BU U B−− −−
−Σ= Σ
=Σ 
we have 
()
() ()( )
()( )
()1
11|| log log2
                         tr
                         tr
                     || .TT
UB i j j i
TT
ji
TT
ji j
Ui jKL p p U U U U
UU U U
UU U D U
KL p p−
−=Σ − Σ
+Σ Σ
+Σ
= 
Therefore, ()() LU LU B= .                              ■ 
 
Lemma 2.1:  If U is a solution to MGMKLD and x x UU U⊥=⊕ , then xU is a 
solution to MGMKLD, and ()()x LU LU= . Here, the column space of xU is 
spanned by the samples {}1
;1|ijn
ij icx≤≤
≤≤r and the column space of xU⊥ is the 
orthogonal complement of the column space of xU. 
Proof. 
Because () ;0T
xi jUx⊥=r and () 0T
xxUU⊥=, we have 
() ()TTT
x xi x i xi UU mU m U m⊥⊥+= +rr r()();
11inTTT
x ix i j x i
jiUm U x Umn⊥
==+ =∑r rr 
and 
()()T
x xi xx UU UU⊥⊥⊕Σ ⊕  201 ()()( )()()( ) ;;
11in TTT
xx i j i xx i j i
jiUU x m UU x mn⊥⊥
=⎛⎞=⊕ − ⊕ −⎜⎟⎝⎠∑rr rr 
()() ()() ( ) ;;
11.inTTT T
x ij i x ij i x i x
jiUx m Ux m UUn==− − = Σ∑rr rr 
That is ()TT
x xi x i UU m U m⊥⊕=rr and ()()TT
x xi xx x i x UU UU U U⊥⊥⊕Σ ⊕ = Σ . With 
these two equations, we can get ()() || ||
x Ui j U i jKLpp K L pp = , because: 
()||Ui jKLpp  
() ()()( )1log log1
2 trTT
ji
TT
ji i jUU U U
UU U D U−⎛⎞Σ− Σ⎜⎟=⎜⎟+Σ Σ +⎜⎟⎝⎠ 
() () () ()
() ()( )
()()() ( )1log log
1
2trTT
xx j xx xxi xx
T
xx j xx
T
xx i i j xxUU UU UU UU
UU UU
UU D UU⊥⊥ ⊥⊥
−
⊥⊥
⊥⊥⎛⎞⊕Σ⊕ − ⊕Σ ⊕⎜⎟
⎜⎟⎛⎞⎜⎟= ⊕Σ⊕⎜⎟⎜⎟+⎜⎟⎜⎟⎜⎟⊕Σ + ⊕ ⎜⎟⎜⎟⎝⎠⎝⎠ 
() ()()( )() 1log log1||2 trxTT
xj x x i x
UijTT
xj x x i i j xUU U U
KL p p
UU U D U−⎛⎞Σ− Σ⎜⎟==⎜⎟+Σ Σ +⎜⎟⎝⎠ 
Therefore ()()x LUL U= , i.e., the matrices U and Ux are equally good 
s o l u t i o n s  t o  M G M K L D .                                         ■ 
 
Deduction 2.2: To obtain the kernel Gram matrix [129] based representation in 
(2.61), we need to get the ();T
i UU Uφ φφΣ  reformulated by the kernel dot product 
trick as 
;
', , , , '11 111 ,
ii i i i i i i i iT
i
T
TT
nH C CC CC CC CC C nH
ii iUU
KI I Knn nφφ φ
××Σ
⎛⎞ ⎛⎞=Λ − − Λ ⎜⎟ ⎜⎟
⎝⎠ ⎝⎠  
where .;jK is the ()1
1thi
k knj−
=+∑  column of the kernel Gram matrix 
()() ;;T
ij ijFnF nKx xφφ
××⎡⎤ ⎡⎤=⎣⎦ ⎣⎦rr () ;;,ij ijkx x⎡ ⎤ =⎣ ⎦rr, () ;;,ij ijkx xrr is the kernel function 
[129], ,
,CCii
iiCCIR∈  is the identity matrix, ,
,1ii
iiCC
CC R∈  is a matrix in which 202 every entry is 1, .;1 iiCjjnKK
≤≤⎡⎤=⎣⎦ is composed of the columns in the kernel 
Gram matrix from the ()1
11thi
k kn−
=+∑  column to the ()1thi
k kn=∑  column. 
Deduction 
;T
i UUφφφΣ  
() () ()
() ()()'; ; ;
1
1
;; ; '
11
1
1i
i
inTT
nH p q ij i kFn nki
Tnji
ij i l p q nHFnlixx xn
n
xx xnφφ φ
φφ φ××=
=
××=⎛⎞ ⎛⎞⎡⎤Λ−⎜⎟ ⎜⎟ ⎣⎦⎝⎠ ⎜⎟=⎜⎟⎛⎞⎜⎟⎡⎤ −Λ⎜⎟ ⎣⎦ ⎜⎟⎝⎠⎝⎠∑
∑
∑rr r
rr r 
' . ;. ; . ;. ; '
11 111 1ii iTnn n
T
nH j k j l nH
jk l ii iKK KKnn n× ×
== =⎛⎞⎛⎞ ⎛⎞⎜⎟=Λ − − Λ ⎜⎟ ⎜⎟⎜⎟⎝⎠ ⎝⎠⎝⎠∑∑ ∑  
'. ; . ; '
111 111i
ii iiTn
T
nH j C C j C C nH
j ii iKK KKnn n× ×
=⎛⎞⎛⎞ ⎛⎞⎜⎟=Λ − − Λ ⎜⎟ ⎜⎟⎜⎟⎝⎠ ⎝⎠⎝⎠∑  
',, '11 111
ii i i ii i iT
T
n H CC C C CC C C n H
ii iKK KKnn n× ×⎛⎞ ⎛⎞=Λ − − Λ⎜⎟ ⎜⎟
⎝⎠ ⎝⎠ 
', , , , '11 111
i i ii i i ii i iT
TT
nH C CC CC CC CC C nH
ii iKI I Knn n××⎛⎞ ⎛⎞=Λ − − Λ ⎜⎟ ⎜⎟
⎝⎠ ⎝⎠.          ■ 
 
Deduction 2.3: To obtain the kernel Gram matrix [129] based representation in 
(2.61), we need to get the ;T
ij UD Uφφφ  reformulated by the kernel dot product trick 
as 
;
' '11 111111 ,
ii j j ii j jT
ij
T
T
n H C C CC C C CC n H
ij ijUD U
KK KKnn nnφφ φ
× ×⎛⎞ ⎛⎞
=Λ − − Λ⎜⎟ ⎜⎟⎜⎟ ⎜⎟⎝⎠ ⎝⎠  
where .;jK is the ()1
1thi
k knj−
=+∑  column of the kernel Gram matrix 
()() ;;T
ij ijFnF nKx xφφ
××⎡⎤ ⎡⎤=⎣⎦ ⎣⎦rr () ;;,ij ijkx x⎡ ⎤ =⎣ ⎦rr, () ;;,ij ijkx xrr is the kernel function 
[129], ,
,CCii
iiCCIR∈  is the identity matrix, ,
,1ii
iiCC
CC R∈  is the unit matrix, 203 .;1 iiCjjnKK
≤≤⎡⎤=⎣⎦ is composed of the columns in the kernel Gram matrix from the 
()1
11thi
k kn−
=+∑  column to the ()1thi
k kn=∑  column. 
Deduction 
;T
ij UD Uφφφ  
() () ()
() () ()'; ; ;
11
;; ; '
1111
11j i
j in nTT
nH p q i k j lHnklij
Tn n
ik jl pq n HHnklijxx xnn
xx xnnφφ φ
φφ φ××==
××==⎛⎞ ⎛⎞⎡⎤Λ−⎜⎟ ⎜⎟⎣⎦ ⎜⎟⎜⎟ ⎝⎠=⎜⎟
⎛⎞⎜⎟⎡⎤ −Λ ⎜⎟⎜⎟ ⎣⎦ ⎜⎟⎜⎟⎝⎠⎝⎠∑∑
∑∑rr r
rr r 
' . ;. ;. ;. ; '
11 111111jj iiTnn nn
T
n H kl kl n H
klklijijKKKKnn nn× ×
====⎛⎞ ⎛⎞
=Λ − − Λ⎜⎟ ⎜⎟⎜⎟ ⎜⎟⎝⎠ ⎝⎠∑∑∑∑  
' '11 111111
ii j j ii j jT
T
n H CC C C CC C C n H
ij ijKK KKnn nn× ×⎛⎞ ⎛⎞
=Λ − − Λ⎜⎟ ⎜⎟⎜⎟ ⎜⎟⎝⎠ ⎝⎠.         ■ 
 
Theorem 2.1:  MGMKLD followed by KPCA is equal to KMGMKLD. 
Proof. 
To simplify the formulation, we assume that () (); 1110i cn
ij ijmn xφ φ=== = ∑∑r r. 
The eigenvectors calculated in KPCA are denoted as: 
;
1
1iiii
innij
ic
jnPRβ×
≤≤
≤≤∑∑⎡⎤=∈⎣⎦. 
Then, 
;;
; ,1 ;1ij ij
ij i K ic jn βλ β=≤ ≤ ≤ ≤ . 
For a sample ;ijx, its corresponding vector in KPCA, the higher dimensional 
space is: 
()( )() ( ) ;; ; ; . ;1,T
KPCA T T
pqi j p q i j jFn nzx P x P k x x P Kφφ
××⎡⎤ ⎡ ⎤== =⎣⎦ ⎣ ⎦rr r r r. 
Then, we have TK P C A
i UUΣ , given by: 
() ();;1 TT KPCA T KPCA KPCA KPCA KPCA
ii j i i j i j
iUU Uxmxm UnΣ= − − ∑rrrr 
.; .; .; .;11 1T
TT T T T
jj jj jl l
ii iU P KP K P KP K Unn n⎛⎞ ⎛⎞=− − ⎜⎟ ⎜⎟
⎝⎠ ⎝⎠∑∑ ∑  204 .; .; .; .;11 1T
TT
jj jj jl l
ii iU P KK KK P Unn n⎛⎞ ⎛⎞=− − ⎜⎟ ⎜⎟
⎝⎠ ⎝⎠∑∑ ∑  
,, ,,11 111
ii i i i i i i i iT
TT T
C C CC C C CC C C
ii iUPK I I KP Unn n⎛⎞ ⎛⎞=− − ⎜⎟ ⎜⎟
⎝⎠ ⎝⎠ 
and TK P C A
ij UD U  is given by: 
;; ;;
11 1111 11jj iiTnn nn
T KPCA T KPCA KPCA KPCA KPCA
i j ik jl ik jl
kl klij ijU D U U xxxx Unn nn== ==⎛⎞ ⎛⎞
=− −⎜⎟ ⎜⎟⎜⎟ ⎜⎟⎝⎠ ⎝⎠∑∑∑∑rrrr 
.; .; .; .;
11 111111jj iiTnn nn
TT T T T
kl kl
klklijijUP K P K P K P K Unn nn====⎛⎞ ⎛⎞
=− −⎜⎟ ⎜⎟⎜⎟ ⎜⎟⎝⎠ ⎝⎠∑∑∑∑  
.; .; .; .;
11 111111jj iiTnn nn
TT
kl kl
klklijijW P KKKK P Unn nn====⎛⎞ ⎛⎞
=− −⎜⎟ ⎜⎟⎜⎟ ⎜⎟⎝⎠ ⎝⎠∑∑∑∑  
11 111111
ii j j ii j jT
TT
CC C C CC C C
ij ijU P KK KK P Unn nn⎛⎞ ⎛⎞
=− −⎜⎟ ⎜⎟⎜⎟ ⎜⎟⎝⎠ ⎝⎠ 
As P is in full rank, denoting ' nH PU×Λ= , we then have 
()''TK P C A T
in H i n H UU φ×× Σ= ΛΣ Λ  
and  
() ''TK P C A T
ij n H ij n H UD U D φ××=Λ Λ . 
Therefore, 
()()()() ' ij ij n H ijKLP U K L K L U φ×=Λ = . 
Consequently, 
()()() LP U L U φ= . 
That is MGMKLD followed by KPCA is equal to KMGMKLD.        ■ 
 
 
 205  Appendix for Chapter 3 
 
Theorem 3.1 (Higher–Order Singular Value Decomposition) 
A tensor 12 M LL LR×××∈XL can be decomposed as the product of a tensor 
12 M LL LR×××∈YL with a series of orthogonal matrices kkLL
kUR×∈ , i.e., 
1kM
k
kU×
==∏ XY ,  
such that, the subtensor of 11 1 kkM
kLLLL
l Rα−+×× ×
=∈ YL, obtained by fixing the kth 
(1kklL≤≤ ) index to α, is orthogonal to 11 1 kkM
kLLLL
l Rβ−+×× ×
=∈ YL, i.e., 
()() ;1 : 1 1 : 1 0
kkll MMαβ==⊗ −− = YY      .  
when αβ≠. 
Finally, 
12 0
kk k kll l LFro Fro Fro== =≥≥ ≥ ≥ YY Y L .  
 
Proof. 
Decompose the mode– k matricizing of X through SVD, 
() matT
kk k k UV=ΣX   
where kU, kV are orthogonal matrices and { }12diag , , ,kL
kk k kσσ σ Σ= L  with 
ij
kkσσ≥  for all ij≤. 
Then, we have 
() () [ ]12 1 1 mat matT
kk k k kN UU U U U U−+ =⊗ ⊗ ⊗ ⊗XY LL .  
Therefore, we have 
()[ ]12 1 1 matT T
kk k k k k k NUV U UU U U U−+ Σ= ⊗ ⊗ ⊗⊗ Y LL .  
i.e., 
()[ ]12 1 1 matT
kk k k kN VU U U U U−+ =Σ ⊗ ⊗ ⊗ ⊗Y LL .  
Because kU and kV are unitary matrices, 
()() ;1 : 1 1 : 1 0
kkll MMαβ==⊗ −− = YY      ,  
for all αβ≠, and 206 12
12 0k
kk k kL
lk l kl L kFro Fro Froσσ σ== ==≥ =≥≥ = ≥ YY Y L .  
I t  f o l l o w s  f r o m  t h e  a b o v e  a n a l y s i s  t h a t  H O S V D  i s  p r o v e d .              ■ 
 
Theorem 3.2 
Minimizing ()2
1
1|M
M
kk k
k Frofu u λ=⊗
==−∏ Xrr (
1kM
T
k
ku λ×
==∏Xr) is equivalent to 
maximizing  
()2
1
1|
kM
MT
kk k
k Frogu u=×
==∏Xrr.  
Moreover, 2
Frof g=−X . 
Proof. 
Because 
()
()2
1
1
2
2 2
11
2 2
2
1|
              2
              
              | ,kM
M
kk k
k Fro
MM
T
kk Fro
kk Fro Fro
Fro
M
kk Frofu u
uu
guλ
λλ
λ=⊗
=
×⊗
==
==−
=− +
=−
=−∏
∏∏X
XX
X
Xrr
rr
r 
minimizing () 1|M
kkfu=r is equivalent to maximizing ()1|M
kkgu=r.        ■ 
 
Theorem 3.3   
Given a sequence of unitary matrices 'kkLL
kUR×∈  (1kM≤≤  and ddLL′< ) and 
a tensor 12 M LL LR××∈XL, the function ()2ˆˆ
Frof=−XX X , where ()ˆ rankdd L′=X ,  
is minimized, when 12 ˆ M LL LR××∈XL is given by 
()
11 1ˆ
kk kMM M
TT
kk k k
kk kUU U U×× ×
== =⎛⎞==⎜⎟⎝⎠∏∏ ∏ XX X .  
 
Proof. 
It is sufficient to prove Y minimizes ()2
1kM
k
k FrogU×
==−∏ YX Y  ,  207 where 
1kM
T
k
kU×
==∏ YX . 
Let 
1kM
k
kU×
==−∏ EX Y . 
Because the columns in kU, 1kM≤≤ , are orthogonal, we have 
()11 1
11 1
11
1                
                
                .kk k
kk k
kk
kMM M
TT
kk k
kk k
MM M
TT
kk k
kk k
MM
TT
kk k
kk
M
T
k
kUU U
UU U
UU U
U×× ×
== =
×× ×
== =
××
==
×
=⎛⎞=−⎜⎟⎝⎠
=−
=−
=−∏∏ ∏
∏∏ ∏
∏∏
∏EX Y
XY
XY
XY 
Therefore, 
1kM
k
kU×
=∏Y  is the least square estimation of X, i.e., () gY is 
minimized when 
1kM
T
k
kU×
==∏ YX .                                ■ 
 
Theorem 3.4   
For a given tensor 12 M LL LR××∈XL, minimizing  
() ()2
1
1|
kM
MT
kk k k
k FrofU U U=×
==−∏ XX   
is equivalent to maximizing 
()2
1
1|
kM
MT
kk k
k FrogU U=×
==∏X .  
Proof. 
Because 
()2
1
11
22
11
22
11
22|
                2 ,
                2 ,
                ,kk
kk
kkMM
MT
kk k k
kkFro
MM
T
kk Fro Fro
kk
MM
TT
kk Fro Fro
kk
Fro FrofU U U
UU
UU=× ×
==
××
==
××
==⎛⎞=−⎜⎟⎝⎠
⎛⎞=− + ⎜⎟⎝⎠
=− +
=−∏∏
∏∏
∏∏XX
XX X Y
XX X Y
XY 208 where 
1kM
T
k
kU×
==∏ YX , minimizing ()1|M
kkfU= is equivalent to maximizing 
() 1|M
kkgU=.                                                 ■ 
 
Deduction 3.1  
()() ( ) arg max tr trTT
bw
UUU S U U S U ζ∗=−  
() ()
() ()1
;;
11tr
   arg max
tricT T
ii i
i
n cU TT
ij i ij i
ijUn m m m m U
Ux m x m Uζ=
==⎛⎞⎛⎞⎡⎤−− ⎜⎟⎜⎟⎢⎥⎣⎦⎝⎠⎜⎟=⎜⎟⎛⎞⎡⎤⎜⎟−− −⎜⎟⎢⎥⎜⎟⎜⎟⎣⎦⎝⎠⎝⎠∑
∑∑ 
() ()
() ()1
;;
11tr
   arg max
tricT T
ii i
i
n cU TT
ij i ij i
ijn U mm mm U
Uxm xmU ζ=
==⎛⎞⎛⎞⎡⎤−− ⎜⎟⎜⎟⎣⎦⎝⎠⎜⎟=⎜⎟⎛⎞⎡⎤ ⎜⎟−− −⎜⎟⎢⎥ ⎜⎟ ⎣⎦⎝⎠⎝⎠∑
∑∑ 
() ()( )
() ()()1
;;
11tr
   arg max
tricT T
ii i
i
n cT UT
ij i ij i
ijn U mm mm U
Uxm xmU ζ=
==⎛⎞⎡⎤−−⎜⎟ ⎣⎦⎜⎟=⎜⎟⎡⎤−− −⎜⎟⎢⎥⎣⎦⎝⎠∑
∑∑ 
() ()( )
() ()()1
;;
11tr
   arg max
tricTTT
ii i
i
n cT UTT
ij i ij i
ijnU m m U m m
Ux mUx m ζ=
==⎛⎞⎡⎤−−⎜⎟ ⎣⎦
⎜⎟=⎜⎟⎡ ⎤ −− −⎜⎟ ⎣ ⎦⎝⎠∑
∑∑ 
()() ()() () ()
()() ()() () ()11
1
;1 ;1
11;1 1
  arg max
;1 1ic
TT
ii i
i
n cUTT
ij i ij i
ijnm m U m m U
xmU xmU ζ=
==⎛⎞−× ⊗ −×⎜⎟
⎜⎟=⎜⎟−− × ⊗ − ×⎜⎟
⎝⎠∑
∑∑      
          
 
Deduction 3.2  209 ()
()() ()
()
()() ()11
1
1
1
|
;
1
11
;
1;1 : 1 :
|a r g m a x
;1 : 1 :k
k
M
ll
ki
kM
T
ikck
iMi T
ik
kM
llMUT
ij i kn ck
Mij T
ij i k
kU
nM M
U
U
U
MM
Uζ=×
=
=
×
=∗
=
×
=
==
×
=⎛⎞⎛⎞− ⎜⎟⎜⎟⎝⎠⎜
⎜⎛⎞⎜⊗−⎜⎟⎜⎝⎠=⎜⎛⎞ ⎜−⎜⎟ ⎜⎝⎠⎜−
⎜ ⎛⎞⊗− ⎜ ⎜⎟⎝⎠ ⎝⎠∏
∑
∏
∏
∑∑
∏MM
MM
XM
XM                    
                              ⎟
⎟⎟
⎟
⎟
⎟
⎟
⎟
⎟⎟ 
()( )
()()() ()
()()
()()() ()11
|;
11;;1 : 1 :
        arg max
;1 : 1 :M
ll iTTcil l l l
iTTiil l l l
TTU n cij i l l l l
TTijij i l l l lUU
nM M
UU
UU
MM
UUζ==
==⎛⎞ −××⎜⎟
⎜⎟⊗− × ×⎜⎟=⎜⎟−××⎜⎟−⎜⎟⎜⊗ − × × ⎟⎝⎠∑
∑∑MM
MM
XM
XM            
                       
()( )
()()()()
()()
()()()()11
|;
11;tr ;
        arg max
tr ;M
lliTcil lT
il lTiil l
TU n cij i l lT
llTijij i l lU
nU l l U
U
U
Ul l U
Uζ==
==⎛⎞⎛⎞ −×⎜⎟⎜⎟
⎜⎟⎜⎟⊗− ×⎝⎠⎜⎟=⎜⎟⎛⎞ −× ⎜⎟⎜⎟−⎜⎟⎜⎟⎜⊗ −× ⎟ ⎜⎟⎝⎠ ⎝⎠∑
∑∑MM
MM
XM
XM            
               
()( )
()()
()()
()()11
|;
11;mat
mat
       arg max tr
mat
matM
ll iTcil i l l
TTili l l
T
llTU n cli j i l l
TTijli ji l lnU
U
UU
U
Uζ==
==⎛⎞⎛⎞⎡⎤ −×⎜⎟⎜⎟⎢⎥⎜⎟⎜⎟⎢⎥ −×⎣⎦⎜⎟⎜⎟=⎜⎟⎜⎟⎡⎤ −×⎜⎟⎜⎟⎢⎥−⎜⎟⎜⎟⎢⎥⎜− × ⎟⎜⎟⎣⎦ ⎝⎠⎝⎠∑
∑∑MM
MM
XM
XM. 
 
Theorem 3.5  The alternating projection method based optimization procedure for 
GTDA converges. 
Proof. 
The alternating projection method ne ver decreases the function value ()1|M
llfU= 
of GTDA between two successive iterations, because it can be interpreted as a 
type of monotonic algorithm. We define a continuous function 
12
1:M
Ml
lfSS S S R+
=××× = → ∏ L , 
where llUS∈ and lS is a closed set, which includes all possible lU. 210 With the definition, f consists of M different mappings: 
() ()
()()
()()
()()
()()1
1
;
11;arg max |
mat
mat
         arg max tr .
mat
matll
lliM
ll l
US
Tcil i l l
TTili l l
T
llTUS n cli j i l l
TTijli ji l lgU f U
nU
U
UU
U
Uζ=
∈
=
∈
==⎛⎞⎛⎞⎡⎤ −×⎜⎟⎜⎟⎢⎥⎜⎟⎜⎟⎢⎥ −×⎣⎦⎜⎟⎜⎟=⎜⎟⎜⎟⎡⎤ −×⎜⎟⎜⎟⎢⎥−⎜⎟⎜⎟⎢⎥⎜− × ⎟⎜⎟⎣⎦ ⎝⎠⎝⎠∑
∑∑MM
MM
XM
XM 
 
The mapping can be calculated with the given 1
1|l
ddU−
= in the tth iteration and 
1|M
dd lU=+ in the ( 1t−)th iteration of the for–loop in Steps 3–5 in Table 3.6. 
Given an initial 11US∈, the alternating projection generates a sequence of items 
(){}1|M
lltU= via ()llUg U∈ , with each {} 1, 2,lM∈L . The sequence has the 
following relationship: 
12 1 2
12 1 2( ,1) ( ,1) ( ,1) ( , 2) ( , 2)
              ( , ) ( , ) ( , ) ( , )
              ( , ) .M
Maf U f U f U f U f U
fU t fU t fU T fU T
fU T b=≤≤ ≤ ≤≤ ≤
≤≤≤ ≤ ≤ ≤
≤=L
LL
L 
where T→+∞ , ( , )lfUt  means ()() ( ) 11 1|, |lM
dd d d lttfU U== + −; and a and b 
are limited values in the R+ space. 
Formally, the alternating projection can be illustrated by a composition of M 
sub–algorithms defined as 
() ()1
1
11:| M a p
ddlM
M
ll l d l d
dd lUU u U−
=× ×
== +Ω×∏∏ra . 
It follows that 12 M ΩΩΩ Ω oo L o  is an algorithm for sets 1|M
llS=. All sub–
algorithms ()lgU increase the value of f, so Ω is monotonic with respect to 
f.                                                        ■ 
 
 
 
 
 211 References 
 
[1]. M. Abramowitz and I. A. Stegun, (Eds.). Handbook of Mathematical 
Functions with Formulas, Graphs, and Mathematical Tables , 9th printing. 
New York: Dover. 1972. 
[2]. M. Aladjem, “Linear Discriminant Analysis for Two Classes via Removal 
of Classification Structure,” IEEE Trans. Pattern Analysis and Machine 
Intelligence , vol. 19, no. 2, pp. 187–192, Feb. 1997. 
[3]. P. Bartlett and J. Shawe–Taylor, “Generalization Performance of Support 
Vector Machines and Other Pattern Classifiers,” Advances in Kernel 
Methods – Support Vector Learning , B. Scholkopf, C. J. Burges, and A. J. 
Smola (eds.), MIT Press, Cambridge, USA, 1998. 
[4]. P. Belhumeur and J. Hespanha and D. Kriegman, “Eigenfaces vs. 
Fisherfaces: Recognition Using Class Specific Linear Projection,” IEEE 
Trans. Pattern Analysis and Machine Intelligence , vol. 19, no. 7, pp. 711–
720, 1997. 
[5]. M. Belkin and P. Niyogi, “Laplacian Eigenmaps and Spectral Techniques 
for Embedding and Clustering,” Neural Information Processing Systems , 
vol. 14, pp. 585–591, 2002. 
[6]. M. Belkin and P. Niyogi, “Laplacian Eigenmaps for Dimensionality 
Reduction and Data Representation,” Neural Computation , vol. 15, no. 6, 
pp. 1,373–1,396, 2003. 
[7]. R. E. Bellman, Adaptive Control Processes . Princeton University Press, 
Princeton, NJ, 1966. 
[8]. Y. Bengio, J.–F. Paiement, P. Vincent, O. Delalleau, N. Le Roux, and M. 
Ouimet, “Out–of–Sample Extensions for LLE, Isomap, MDS, Eigenmaps, 
and Spectral Clustering,” Advances in Neural Information Processing 
Systems , vol. 16, 2004. 
[9]. A. Bissacco, A. Chiuso, Y. Ma and S. Soatto, “Recognition of Human 
Gaits,” Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition , 
vol. 2, pp. 52–57, 2001. 
[10]. W. M. Boothby, An Introduction to Differential Manifolds and 
Riemannian Geometry , 2nd ed. San Diego, CA: Academic, 1986. 
[11]. S. Boyd and L. Vandenberghe, Convex Optimization , Cambridge 
University Press, 2004. 
[12]. S. Boyd, S. J. Kim, L. Vandenberghe, and A. Hassibi, “A Tutorial on 
Geometric Programming,” Optimization and Engineering , 2006. 
[13]. C. J. C. Burges, “Geometric Methods for Feature Extraction and 
Dimensional Reduction,” In Data Mining and Knowledge Discovery 
Handbook: A Complete Guide for Practitioners and Researchers , Eds. L. 
Rokach and O. Maimon, Kluwer Academic Publishers, 2005. 
[14]. L.M. Bregman, “The Relaxation Method to Find the Common Points of 
Convex Sets and Its Application to the Solution of Problems in Convex 212 Programming,” USSR Compt. Math. and Math. Phys. , no. 7, pp. 200–217, 
1967. 
[15]. J.C. Burges, “A Tutorial on Support Vector Machines for Pattern 
Recognition,” Data Mining and Knowledge Discovery , vol. 2, no. 2, pp. 
121–167, 1998. 
[16]. L. J. Buturovic, “Toward Bayes–Optimal Linear Dimension Reduction,” 
IEEE Trans. Pattern Analys is Machine Intelligence , vol. 16, no. 4, pp. 
420–424, April 1994. 
[17]. N. Campbell, “Canonical Variate Analysis – a General Formulation,” 
Australian Journal of Statistics , vol. 26, pp. 86–96, 1984. 
[18]. H. Cevikalp, M. Neamtu, M. Wilkes, and A. Barkana, “Discriminative 
Common Vectors for Face Recognition,” IEEE Trans. Pattern Analysis 
and Machine Intelligence . vol. 27, no. 1, pp. 4–13, 2005. 
[19]. L. F. Chen, H.Y. Liao, M. T. Ko, J. C. Lin, and G. J. Yu, “A New LDA–
based Face Recognition System which Can Solve the Small Sample Size 
Problem,” Pattern Recognition , vol. 33, no. 10, pp. 1,713–1,726, 2000. 
[20]. Y. Chou, Statistical Analysis , Holt International, 1969. 
[21]. T. M. Cover and J. A. Thomas, Elements of Information Theory . New 
York: Wiley, 1991. 
[22]. D. Cunado, M. Nixon, and J. Carter, “Automatic Extraction and 
Description of Human Gait Models for Recognition Purposes,” Computer 
Vision and Image Understanding , vol. 90, no. 1, pp. 1–41, 2003. 
[23]. J. Cutting and L. Kozlowski, “Recognizing Friends by Their Walk: Gait 
Perception without Familiarity Cues,” Bulletin of the Psychonomic Society , 
vol. 9, pp. 353 – 356, 1977. 
[24]. L. S. Daniel and J. Weng, “Hierarchical Discriminant Analysis for Image 
Retrieval,” IEEE Trans. Pattern Analysis Machine Intelligence , vol. 21, 
no. 5, pp. 386–401, May 1999. 
[25]. J. G. Daugman, “Two–Dimensional Spectral Analysis of Cortical 
Receptive Field Profile,” Vision Research , vol. 20, pp. 847–856, 1980. 
[26]. J. G. Daugman, “Uncertainty Relation for Resolution in Space, Spatial 
Frequency, and Orientation Optimized by Two–Dimensional Visual 
Cortical Filters,” Journal of the Optical Society of America , vol. 2, no. 7, 
pp. 1,160–1,169, 1985. 
[27]. J. G. Daugman, “High Confidence Visual Recognition of Persons by a 
Test of Statistical Independence,” IEEE Trans. Pattern Analysis and 
Machine Intelligence , vol. 15, no. 11, pp. 1,148–1,161, 1993. 
[28]. F. De la Torre and T. Kanade “Multimodal Oriented Discriminant 
Analysis,” Int’l Conf. Machine Learning , Bonn, Germany, Aug. 2005. 
[29]. H. P. Decell and S. M. Mayekar, “Feature Combinations and the 
Divergence Criterion,” Computers and Math. With Applications , vol. 3, 
pp. 71–76, 1977. 
[30]. R.O. Duda, P.E. Hart, and D.G. Stork, Pattern Classification . John Wiley 
and Sons Inc. 2001. 213 [31]. S. Dudoit, J. Fridly, and T.P. Speed, “Comparison of Discrimination 
Methods for the Classification of Tumors Using Gene Expression Data”, J. 
of the American Statistical Association , vol. 97, no.457, pp. 77–87, 2002. 
[32]. D. Dunn, W. E. Higgins, and J. Wakeley, “Texture Segmentation Using 2–
D Gabor Elementary Functions,” IEEE Trans. Pattern Analysis and 
Machine Intelligence , vol. 16, no. 2, pp. 130–149, 1994. 
[33]. B. Efron, “Estimating the Error Rate of a Prediction Rule: Improvement on 
Cross–Validation,” Journal of the American Statistical Association , vol. 
78, no. 382, pp. 316–331, 1983. 
[34]. K. Etemad and R. Chellappa, “Discriminant Analysis for Recognition of 
Human Face Images,” Journal of the Optical Society of America A , vol. 
14, no. 8, pp. 1,724–1,733, 1998. 
[35]. L. P. Fatti and D. M. Hawkins, “Variable Selection in Heteroscedastic 
Discriminant Analysis,” Journal of the American Statistical Association , 
vol. 81, pp. 494-500, 1986. 
[36]. M. Figueiredo and A.K. Jain, “Unsupervised Learning of Finite Mixture 
Models,” IEEE Trans. Pattern Analysis and Machine Intelligence , vol. 24, 
no. 3, pp. 381–396, Mar. 2002. 
[37]. R. A. Fisher, “The Statistical Utilization of Multiple Samples,” Ann. 
Eugenics , vol. 8 pp. 376–386, 1938. 
[38]. J. H. Friedman, “Regularized Discriminant Analysis,” Journal of the 
American Statistical Association , vol. 84, pp. 165–175, 1989. 
[39]. K. Fukunaga, Introduction to Statistical Pattern Recognition  (Second 
Edition), Academic Press, 1990. 
[40]. K. Fukunaga and M. Mantock, “Nonpara metric Discriminant Analysis,” 
IEEE Trans. Pattern Analysis and Machine Intelligence , vol. 5, pp. 671–
678, 1983. 
[41]. G. Fung and O. L. Mangasarian, “Proximal Support Vector Machine 
Classifiers,” Proc. 7th ACM SIGKDD Int’l Conf. Knowledge Discovery 
and Data Mining , California, USA, pp. 77–86, 2001. 
[42]. P. Gallinari, S. Thiria, F. Badran, and F. Folgelman–Soulie, “On the 
Relations between Discriminant Analysis and Multilayer Perceptrons,” 
Neural Networks , vol. 4, pp. 349–360, 1991. 
[43]. A. Girgensohn and J. Foote, “Video  Classification using Transform 
Coefficients,” Proc. IEEE Int'l Conf. Acoustics, Speech, and Signal 
Processing , vol. 6, pp. 3045–3048, 1999. 
[44]. G. H. Golub and C. F. van Loan, Matrix Computation  (Third Edition), The 
Johns Hopkins Univ. Press, 1996. 
[45]. J. Han and B. Bhanu, “Statistical Feature Fusion for Gait–Based Human 
Recognition,” Proc. IEEE Int’l Conf. Computer Vision and Pattern 
Recognition , vol. 2, pp. 842–847, Washington, DC, 2004. 
[46]. J. Han and B. Bhanu, “Individual Recognition Using Gait Energy Image,” 
IEEE Trans. Pattern Analysis  and Machine Intelligence , vol. 28, no. 2, pp. 
316–322, 2006. 214 [47]. C.P. Hansen, Rank–deficient and discre te ill–posed problems . SIAM, 
Philadelphia, PA, 1997. 
[48]. G. H. Hardy, J. E. Littlewood, and G. Pólya, Inequalities . Cambridge, 
England: Cambridge University Press, 1952. 
[49]. T. Hastie, A. Buja, and R. Tibshirani, “Penalized Discriminant Analysis,” 
Annals of Statistics , vol. 23, pp. 73–102, 1995. 
[50]. T. Hastie and R. Tibshirani, “Flexibl e Discriminant by Mixture Models,” 
J. Royal Statistical Society  (Series B ), vol. 58, pp. 155–176, 1996. 
[51]. T. Hastie and R. Tibshirani, “Discriminant Analysis by Gaussian 
Mixtures,” Journal of the Royal Statistical Society Series B: 
Methodological , vol. 58, pp. 155–176, 1996. 
[52]. T. Hastie, R. Tibshirani, and A. Buja, “Flexible Discriminant Analysis by 
Optimal Scoring,” Journal of the American Statistical Association , vol. 89, 
pp. 1,255–1,270, 1994. 
[53]. T. Hastie, R. Tibshirani, and J.H. Friedman, The Elements of Statistical 
Learning: Data Mining, Inference, and Prediction , New York: Springer, 
2001. 
[54]. B. Heisele, “Visual Object Recognition with Supervised Learning,” IEEE 
Intelligent Systems , vol. 8, no. 3, pp. 38–42, 2003. 
[55]. P. Howland and H. Park, “Generalizing Discriminant Analysis Using the 
Generalized Singular Value Decomposition,” IEEE Trans. Pattern 
Analysis Machine Intelligence , vol. 26, no. 8, pp. 995–1,006, 2004. 
[56]. L. Itti and C. Koch, “Computational Modeling of Visual Attention,” 
Nature Reviews Neuroscience , vol. 2, no. 3, pp. 194–203, 2001. 
[57]. L. Itti, C. Koch, and E. Niebur, “A Model of Saliency–based Visual 
Attention for Rapid Scene Analysis,” IEEE Trans. Pattern Analysis and 
Machine Intelligence , vol. 20, no. 11, pp. 1,254–1,259, 1998. 
[58]. A. K. Jain, R. P. W. Duin, and J. Mao, “Statistical Pattern Recognition: A 
Review,” IEEE Trans. Pattern Analysis and Machine Intelligence , vol. 22, 
no. 1, pp. 4–37, 2000. 
[59]. A. K. Jain, S. Prabhakar, and L. Hong, “A Multichannel Approach to 
Fingerprint Classification,” IEEE Trans. Pattern Analysis and Machine 
Intelligence , vol. 21, no. 4, pp. 348–359, 1999. 
[60]. E.T. Jaynes, Probability Theory: The Logic of Science  (Principles and 
Elementary Applications), Cambridge University Press, 2003. 
[61]. B. Jelinek, “Review on Heteroscedastic Discriminant Analysis,” 
Unpublished Report.  
http://www.cavs.msstate.edu/hse/ies/ publications/courses/ece_8443/papers
/2001/hda2/p01_paper_v00.pdf#sear ch=%22%22Review%20on%20Heter
oscedastic%20Discriminant%20Analysis%22%22 
[62]. X. Jing and D. Zhang, “A Face and Palmprint Recognition Approach 
based on Discriminant DCT Feature Extraction,” IEEE Trans. Systems, 
Man and Cybernetics , Part B, vol. 34, no. 6, pp. 2,405–2,415, 2004. 
[63]. A.Y. Johnson and A.F. Bobick, “A Multi–View Method for Gait 215 Recognition using Static Body Parameters,” Proc. IEEE Int’l Conf. on 
Audio– and Video– Based Biometric Person Authentication , pp. 301–311, 
2001. 
[64]. I. T. Jolliffe, Principal Component Analysis  (2nd edition), Springer, 2002. 
[65]. A. Kale, A. N. Rajagopalan, N. Cuntoor, and V. Kruger, “Gait–based 
Recognition of Humans us ing Continuous HMMs,” Proc. IEEE Int’l Conf. 
Automatic Face and Gesture Recognition , pp. 321–326, Washington, DC, 
2002. 
[66]. A. Kale, A. Sundaresan, A. N. Rajagopalan, N. P. Cuntoor, A. K. Roy–
Chowdhury, V. Kruger, and R. Chellappa, “Identification of Humans using Gait,” IEEE Trans. Image Processing , vol. 13, no. 9, pp. 1,163–1,173, 
2004. 
[67].
 M. Katz, H.–G. Meier, H. Dolfing, and D. Klakow, “Robustness of Linear 
Discriminant Analysis in Automatic Speech Recognition,” Proc. IEEE. 
Int’l Conf. on Pattern Recognition , vol. 3, 2002. 
[68]. T.K. Kim and J. Kittler, “Locally Linear Discriminant Analysis for 
Multimodally Distributed Classes fo r Face Recognition with a Single 
Model Image,” IEEE Trans. Pattern Analys is Machine Intelligence , vol. 
27, no. 3, pp. 318–327, Mar., 2005. 
[69]. S.J. Kim, A. Magnani, and S. Boyd, “Robust Fisher Discriminant 
Analysis,” Advances in Neural Information Processing Systems , 
Vancouver, British Columbia, 2005. 
[70]. N. Kumar and A. G. Andreou, “Heteroscedastic Discriminant Analysis and 
Reduced Rank HMMs for Improved Speech Recognition,” Speech 
Communcation , vol. 26, pp. 283–297, 1998. 
[71]. S. Kung, M. Mak, and S. Lin, Biometric Authentication . Prentice Hall, 
2004. 
[72]. P. A. Lachenbruch, Discriminant Analysis . Hafner Press, New York, 1975. 
[73]. G. Lanckriet, N. Cristianini, P. Bartlett, L. Ghaoui, and M. Jordan, 
“Learning the Kernel Matrix with Semidefinite Programming,” J. of 
Machine Learning Research , vol. 5, pp. 27–72, 2004. 
[74]. G. Lanckriet, L. Ghaoui, C. Bhattacharyya, and M. Jordan, “A Robust 
Minimax Approach to Classification”, J. of Machine Learning Research , 
vol. 3, pp. 555–582, 2002. 
[75]. L. D. Lathauwer, Signal Processing Based on Multilinear Algebra, Ph.D. 
Thesis, Katholike Universiteit Leuven, 1997. 
[76]. L. De Lathauwer, B. De Moor, and J.  Vandewalle, “A Multilinear Singular 
Value Decomposition,” SIAM J. on Matrix Analysis and Applications , vol. 
21, no. 4, pp. 1253–1278, 2001. 
[77]. L. De Lathauwer, B. De Moor, and J. Vandewalle, “On the Best Rank–1 
and Rank–(r1,r2,...,rn) Approximation of Higher–Order Tensors,” SIAM J. 
on Matrix Analysis and Applications , vol. 21, no. 4, pp. 1,324–1,342, 
2000. 
[78]. L. Lee, G. Dalley, and K. Tieu, “Learning Pedestrian Models for 
Silhouette Refinement,” Proc. IEEE Int’l Conf. Computer Vision , vol. 1, 216 pp. 663–670, Nice, France, 2003. 
[79]. L. Lee and W. E. L. Grimson, “G ait Analysis for Recognition and 
Classification,” Proc. IEEE Int’l Conf. Automatic Face and Gesture 
Recognition , pp. 155–162, Washington, DC, 2002. 
[80]. T. S. Lee, “Image Representation Using 2D Gabor Wavelets,” IEEE 
Trans. Pattern Analysis and Machine Intelligence , vol. 18, no. 10, pp. 
959–971, 2003. 
[81]. Y. Leedan and P. Meer, “Heteroscedastic Regression in Computer Vision: 
Problems with Bilinear Constraint,” International Journal of Computer 
Vision , vol. 37, no. 2, pp. 127-150, 2000. 
[82]. A. Leonardis and H. Bischof, Robust Recovery of Eigenimages in the 
Presence of Outliers and Occlusions, Journal of Computing and 
Information Technology , vol. 4, no. 1, pp. 25–36, 1996. 
[83]. Q. Li, An Integration Framework of Feat ure Selection and Extraction for 
Appearance based Recognition , PhD Thesis, UDEL1472, University of 
Delaware, 2006. 
[84]. S. Z. Li, X. Lu, X. Hou, X. Peng, and Q. Cheng, “Learning Multiview 
Face Subspaces and Facial Pose Es timation using Independent Component 
Analysis,” IEEE Trans. Image Processing , vol. 14, no. 6, pp. 705–712, 
2005. 
[85]. T. Li, M. Ogihara, Q. Li, “A Comparative Study on Content–based Music 
Genre Classification,” Proc. Annual ACM Conf. on Research and 
Development in Information Retrieval , pp. 282–289, 2003. 
[86]. J. J. Little and J. E. Boyd, “Recognizing People by Their Gait: the Shape 
of Motion,” Videre , vol. 1, no. 2, pp. 1–32, 1998. 
[87]. C. Liu, “Gabor–based Kernel PCA with Fractional Power Polynomial 
Models for Face Recognition,” IEEE Trans. Pattern Analysis and Machine 
Intelligence , vol. 26, no. 5, pp. 572–581, 2004. 
[88]. C. Liu and H. Wechsler, “Gabor Feature Based Classification Using the 
Enhanced Fisher Linear Discriminant Model for Face Recognition,” IEEE 
Trans. Image Processing , vol. 11, no. 4, pp. 467–476, 2002. 
[89]. C. Liu and H. Wechsler, “Enhanced Fi sher Linear Discriminant Models 
for Face Recognition,” Proc. IEEE Int’l Conf. Pattern Recognition , vol.2, 
pp. 1,368–1,372, Brisbane, Australia, 1998. 
[90]. X. Liu, A. Srivastava, and K. Galliva n, “Optimal Linear Representations 
of Images for Object Recognition,” IEEE Trans. Pattern Analysis Machine 
Intelligence , vol. 26, no. 5, pp. 662–666, May 2004. 
[91]. Z. Liu and S. Sarkar, “Simplest Representation yet for Gait Recognition: 
Averaged Silhouette,” Proc. IEEE Int’l Conf. Pattern Recognition , vol. 4, 
pp. 211–214, Cambridge, England, 2004. 
[92]. Z. Liu and S. Sarkar, “Improved Gait Recognition by Gait Dynamics 
Normalization,” IEEE Trans. Pattern Analysis and Machine Intelligence , 
vol. 28, no. 6, pp. 863 –876, 2006. 
[93]. M. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret, “Applications of 
Second–Order Cone Programming,” Linear Algebra and its Applications , 217 vol. 284, pp. 193–228, 1998. 
[94]. R. Lotlikar and R. Kothari, “Adaptive Linear Dimensionality Reduction 
for Classification,” Pattern Recognition , vol. 33, no. 2, pp. 185–194, 
February 2000. 
[95]. R. Lotlikar and R. Kothari, “Fractional–Step Dimensionality Reduction,” 
IEEE Trans. Pattern Analysis Machine Intelligence , vol. 22, no. 6, pp. 
623–627, June 2000. 
[96]. M. Loog, “Approximate Pairwise Accuracy Criteria for Multiclass Linear 
Dimension Reduction: Generalizations  of the Fisher Criterion,” Delft 
Univ. Technique Report, 1999. 
[97]. M. Loog, Supervised Dimensionality Reduction and Contextual Pattern 
Recognition in Medical Image Processing, Ph.D. thesis, Image Sciences 
Institute, Utrecht University, 2004. 
[98]. M. Loog, R. P.W. Duin, and R. Haeb–Umbach, “Multiclass Linear 
Dimension Reduction by Weighted Pairwise Fisher Criteria,” IEEE Trans. 
Pattern Analysis Machine Intelligence , vol. 23, no. 7, pp. 762–766, July 
2001. 
[99]. M. Loog and R. P.W. Duin, “Linear Dimensionality Reduction via a 
Heteroscedastic Extension of LDA: The Chernoff Criterion,” IEEE Trans. 
Pattern Analysis Machine Intelligence , vol. 26, no. 6, pp. 732–739, June 
2004. 
[100].  J. Lu, K.N. Plataniotis, and A.N. Venetsanopoulos, “Face Recognition 
Using LDA Based Algorithms,” IEEE Trans. Neural Networks , vol. 14, 
no. 1, pp. 195–200, Jan. 2003. 
[101].  S. Marcelja, “Mathematical Descript ion of the Responses of Simple 
Cortical Cells,” Journal of the Optical Society of America , vol. 70, no. 11, 
pp. 1297–1300, 1980. 
[102].  A. Marshall and I. Olkin, “Multivariate Chebyshev Inequalities,” Annals of 
Mathematical Statistics , vol. 31, no. 4, pp. 1,001–1,014, 1960. 
[103].  G.J. McLachlan, Discriminant Analysis and Statistical Pattern 
Recognition , Wiley–Interscience, New York, 1992. 
[104].  S. Mika, G. Ratsch, J. Weston, B. Sc holkopf, and K. R. Muller, “Fisher 
Discriminant Analysis with Kernels,” IEEE Workshop on Neural Networks 
for Signal Processing , pp. 41–48, 1999. 
[105].  T. Mitchell, Machine Learning , McGraw Hill, New York, 1997. 
[106].  D. C. Montgomery, E. A. Peck, and G. G. Vining, Introduction to Linear 
Regression Analysis  (4th Edition), John Wiley & Sons, 2006. 
[107].  H. Moon and P.J. Phillips, “Computational and Performance Aspects of 
PCA–based Face Recognition Algorithms,” Perception , vol. 30, pp. 303–
321, 2001. 
[108].  T. K. Moon, P. Howland, J. H. Gunther, “Document Author Classification 
using Generalized Discriminant Analysis,” Proc. Workshop on Text 
Mining, SIAM Int'l Conf. on Data Mining , 2006. 
[109].  K.R. Müller, S. Mika, G. Rätsch, K. Tsuda, and B. Schölkopf, “An 218 Introduction to Kernel–Based Learning Algorithms,” IEEE Trans. Neural 
Networks , vol. 12, no. 2, pp. 181–201, Feb. 2001. 
[110].  N. Murata, T. Takenouchi, T. Kanamori, and S. Eguchi, “Information 
Geometry of U–Boost and Bregman Divergence,” Neural Computation , 
vol. 16, no. 7, pp. 1,437–1,481, July, 2004. 
[111].  M. Murray, “Gait as a Total Pattern of Movement,” American Journal of 
Physical Medicine , vol. 16, pp. 290 – 332, 1967. 
[112].  M. Murray, A. Drought, and R. Kory, “Walking Pattern of Normal Men,” 
J. of Bone and Joint Surgery , vol. 46, no. A(2), pp. 335–360, 1964. 
[113].  S. Nikolopoulos, S. Zafeiriou, P. Sidiropoulos, N. Nikolaidis, and I. Pitas, 
“Image Replica Detection Using R– Trees and Linear Discriminant 
Analysis,” Proc. IEEE Int'l Conf. Multimedia and Expo , 2006. 
[114].  J. Nocedal and S. J. Wright, Numerical Optimization , Springer, 1999. 
[115].  D. G. Northcutt, Multilinear Algebra , Cambridge U Press, 1984. 
[116].  X.M. Pardo, P. Radeva, and D. Cabello, “Discriminant Snakes for 3D 
Reconstruction of Anatomical Organs,” Medical Image Analysis , vol. 7, 
no. 3, pp. 293–310, 2003. 
[117].  J.P. Pedroso and N. Murata, “Support Vector Machines for Linear 
Programming: Motivation and Formulations,” BSIS Technical Report No.99–XXX, RIKEN Brain Science Institute, Japan, 1999. 
[118].
 P. J. Phillips, H. Moon, S. A. Rizvi, and P. J. Rauss, “The FERET 
Evaluation Methodology for Face–Recognition Algorithms,” IEEE Trans. 
on Pattern Recognition and Machine Intelligence , vol. 22, no. 10, pp. 
1090–1104, 2000. 
[119].  R. Plamondon and S. N. Srihari On–Line and Off–Line Handwriting 
Recognition: A Comprehensive Survey. IEEE Trans. on Pattern 
Recognition and Machine Intelligence , vol. 22, no. 1, pp. 63–84, 2000. 
[120].  I. Popescu and D. Bertsimas, “Optimal Inequalities in Probability Theory: 
a Convex Optimization Approach,” Technique Report TM62, Insead. 
[121].  B.G. Prasad, K.K. Biswas, and S.K. Gupta, “Region–based Image 
Retrieval using Integrated Color, Shape, and Location Index,” Computer 
Vision and Image Understanding , vol. 94, no. 1–3, pp. 192–233, 2004. 
[122].  C. R. Rao, “The Utilization of Multiple Samples in Problems of Biological 
Classification,” J. Royal Statistical Soc., B , vol. 10, pp. 159–203, 1948. 
[123].  S. Raudys and R. P. W. Duin, “On Expected Classification Error of the 
Fisher Linear Classifier with Pseudo–inverse Covariance Matrix,” Pattern 
Recognition Letter , vol. 19, no. 5–6, 1998. 
[124].  S. T. Roweis and L. K. Saul, “Nonlinear Dimensionality Reduction by 
Locally Linear Embedding,” Science , vol. 290, pp. 2,323–2,326, 2000. 
[125].  Y. Rui, T. S. Huang, and S. F. Chang, “Image Retrieval: Current 
Techniques, Promising Directions and Open Issues,” Journal of Visual 
Communication and Image Representation , vol. 10, pp. 39–62, 1999. 
[126].  S. Sarkar, P. Phillips, Z. Liu, I. Vega, P. Grother, and K. Bowyer, “The 
HumanID Gait Challenge Problem: Data Sets, Performance, and 219 Analysis,” IEEE Trans. Pattern Analysis Machine Intelligence , vol. 27, no. 
2, pp. 162–177, 2005. 
[127].  L. K. Saul and S. T. Roweis, “Think Globally, Fit Locally: Unsupervised 
Learning of Low Dimensional Manifolds,” Journal of Machine Learning 
Research , vol. 4, pp. 119–155, 2003. 
[128].  B. Schölkopf and A. Smola, Learning with Kernels: Support Vector 
Machines, Regularization , Optimization, and Beyond (Adaptive 
Computation and Machine Learning), MIT Press, 2001. 
[129].  B. Schölkopf, A. J. Smola, and K.R. Müller, “Kernel Principal Component 
Analysis,” Advances in Kernel Methods: Support Vector Learning , MIT 
Press, pp. 327–352, Cambridge, MA, USA, 1999. 
[130].  B. Schölkopf, A. Smola, R. C. Williamson, and P. L. Bartlett, “New 
Support Vector Algorithms,” Neural Computation , vol. 12, pp. 1,207 – 
1,245, 2000. 
[131].  J. Shi and J. Malik, “Normalized Cuts and Image Segmentation,” IEEE 
Trans. Pattern Analysis and Machine Intelligence , vol. 22, no. 8, pp. 888–
905, Aug. 2000. 
[132].  A. Shashua and A. Levin, “Linear Image Coding for Regression and 
Classification Using the Tensor–rank Principle,” Proc. IEEE Int’l Conf. 
Computer Vision and Pattern Recognition , vol. 1, pp. 42–49, 2001. 
[133].  J. R. Smith and S.–F. Chang, “Transform Features for Texture 
Classification and Discrimination in Large Image Databases,” Proc. IEEE 
Int'l Conf. Image Processing , vol. 3, pp. 407–411, 1994. 
[134].  A. Smola, T.T. Friess, and B. Schölkopf, “Semiparametric Support Vector 
and Linear Programming Machines,” Neural and Information Processing 
Systems , vol. 11, 1999. 
[135].  T.R. Strohmann, A. Belitski, G.Z. Grudic, and D. DeCoste, “Sparse 
Greedy Minimax Probability Machine Classification,” Advances in Neural 
Information Processing Systems , Vancouver and Whistler, British 
Columbia, 2003. 
[136].  J. Sun, D. Tao, and C. Faloutsosy , “Beyond Streams and Graphs: Dynamic 
Tensor Analysis,” Proc. ACM SIGKDD Int’l Conf. on Knowledge 
Discovery and Data Mining , 2006. 
[137].  J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, and J. 
Vandewalle, Least Squares Support Vector Machines , World Scientific, 
2002. 
[138].  J.A.K. Suykens, J. Vandewalle, “Least Squares Support Vector Machine 
Classifiers,” Neural Processing Letters , vol. 9, no. 3, pp. 293–300, 1999. 
[139].  D. L. Swets and J. Weng, “Using Discriminant Eigenfeatures for Image 
Retrieval,” IEEE Trans. Pattern Analysis and Machine Intelligences , vol. 
18, no. 8, pp. 831–836, 1996. 
[140].  Y. Sun and R. Fisher, “Object–based Visual Attention for Computer 
Vision,” Artificial Intelligence , vol. 146, no. 1, pp. 77–123, 2003. 
[141].  J.A.K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, and J. 
Vandewalle, Least Squares Support Vector Machines , World Scientific, 220 Singapore. 
[142].  R. Tanawongsuwan and A. Bobick, “Modelling the Effects of Walking 
Speed on Appearance–based Gait Recognition,” Proc. IEEE Int’l Conf. 
Computer Vision and Pattern Recognition , 2, 783–790, 2004. 
[143].  D. Tao, X. Li, X. Wu, and S. J. Maybank, “Elapsed Time in Human Gait 
Recognition: A New Approach,” Proc. IEEE Int’l Conf. on Acoustics, 
Speech, and Signal Processing , vol. 2, pp. 177–180, 2006. 
[144].  D. Tao, X. Li, X. Wu, and S. J. Maybank, “Human Carrying Status in 
Visual Surveillance,” Proc. IEEE Int’l Conf. on Computer Vision and 
Pattern Recognition , pp. 1,670–1,677, 2006. 
[145].  D. Tao, X. Li, X. Wu, and S. J. Maybank, “Tensor Rank One Discriminant 
Analysis,” Submitted to IEEE Trans. on Pattern Analysis and Machine 
Intelligence . (Under Major Revision) 
[146].  D. Tao, X. Li, X. Wu, and S. J. Maybank, “General Averaged Divergences 
Analysis,” Submitted to IEEE Trans. on Pattern Analysis and Machine 
Intelligence . (Under Major Revision) 
[147].  D. Tao, X. Li, X. Wu, and S. J. Maybank, “General Tensor Discriminant 
Analysis and Gabor Features for Gait Recognition,” IEEE Trans. on 
Pattern Analysis and Machine Intelligence , 2007. (To appear) 
[148].  D. Tao, X. Li, and S. J. Maybank, “Negative Samples Analysis in 
Relevance Feedback,” IEEE Trans. on Knowledge and Data Engineering , 
2007. (To appear) 
[149].  D. Tao, X. Li, W. Hu, S. J. Maybank, and X. Wu, “Supervised Tensor 
Learning: A Framework,” Knowledge and Information Systems  (Springer), 
2007. (To appear) 
[150].  D. Tao, X. Li, W. Hu, S. J. Maybank, and X. Wu, “Supervised Tensor 
Learning,” Proc. IEEE Int’l Conf. on Data Mining , pp. 450–457, 2005. 
[151].  D. Tao, X. Li, W. Hu, and S. J. Maybank, “Stable Third–Order Tensor 
Representation for Color Image Classification,” Proc. IEEE Int’l Conf. on 
Web Intelligence , pp. 641–644, 2005. 
[152].  D. Tao, X. Tang, X. Li, and Y. Rui, “Kernel Direct Biased Discriminant 
Analysis: A New Content–based Image Retrieval Relevance Feedback 
Algorithm,” IEEE Trans. on Multimedia , vol. 8, no. 4, pp. 716–727, 2006. 
[153].  D. Tao and X. Tang, “Kernel Full–space Biased Discriminant Analysis,” 
Proc. IEEE Int’l Conf. on Multimedia and Expo , pp. 1,287–1,290, 2004. 
[154].  D. Tao and X. Tang, “A Direct Method to Solve the Biased Discriminant 
Analysis in Kernel Feature Space for Content–based Image Retrieval,” 
Proc. IEEE Int’l Conf. on Acoustics, Speech, and Signal Processing , pp. 
441–444, 2004. 
[155].  J. B. Tenenbaum, V. de Silva, and J. C. Langford, “A Global Geometric 
Framework for Nonlinear Dimensionality Reduction,” Science , vol. 290, 
pp. 2,319–2,323, 2000. 
[156].  A. B. Torralba and A. Oliva, “Semantic Organization of Scenes using 
Discriminant Structural Templates,” Proc. IEEE Int'l Conf. Computer 
Vision , pp. 1,253–1,258, 1999. 221 [157].  A.M. Treisman and G. Gelade, “A Feature–Integration Theory of 
Attention,” Cognitive Psychology , vol. 12, no. 1, pp. 97–136, 1980. 
[158].  M. Turk and A. Pentland, “Eigenfaces for Recognition,” J. of Cognitive 
Neurosicence , vol. 3, no. 1, pp. 71–86, 1991. 
[159].  L. Vandenberghe and S. Boyd, “Semidefinite programming,” SIAM 
Review , vol. 38, no. 1, pp. 49–95, 1996. 
[160].  R. Vanderbei, Linear Programming: Foundations and Extensions  (2nd 
edition), Springer, 2001. 
[161].  V. Vapnik, The Nature of Statistical Learning Theory , Springer–Verlag, 
New York, 1995. 
[162].  M. A. O. Vasilescu and D. Terzopoulos, “Multilinear Subspace Analysis 
for Image Ensembles,” Proc. IEEE Int'l Conf. Computer Vision and 
Pattern Recognition , vol.2, pp. 93–99, Madison, WI, 2003. 
[163].  G. Veres, L. Gordon, J. Carter, and M. Nixon, “What Image Information is 
Important in Silhouette–based Gait Recognition?,” Proc. IEEE Int’l Conf. 
Computer Vision and Pattern Recognition , vol. 2, pp. 776–782, 2004. 
[164].  J. Z. Wang, L. Li, and G. Wiederhold, “SIMPLIcity: Semantics–Sensitive 
Integrated Matching for Picture Libraries,” IEEE Trans. Patt ern Analysis 
and Machine Intelligence , vol. 23, no. 9, pp. 947–963, 2001. 
[165].  L. Wang, H. Ning, T. Tan, and W. Hu, “Fusion of Static and Dynamic 
Body Biometrics for Gait Recognition,” Proc. IEEE Int’l Conf. Computer 
Vision , 2, 2003. 
[166].  L. Wang, T. Tan, H. Ning, and W. Hu, “Silhouette Analysis–based Gait 
Recognition for Human Identification,” IEEE Trans. Pattern Analysis and 
Machine Intelligence , vol. 25, no. 12, pp. 1,505–1,518, 2003. 
[167].  L. Wang, Y. Zhang, and J. Feng, “On the Euclidean Distance of Images,” 
IEEE Trans. Pattern Analysis  and Machine Intelligence , vol. 27, no. 8, pp. 
1,334–1,339, 2005. 
[168].  H. Wechslet, J. Phillips, V. Bruse, F. Soulie, and T. Hauhg, editors. Face 
Recognition: From Theory to Application . Springer–Verlag, Berlin, 1998. 
[169].  K. Q. Weinberger, J. Blitzer, and L. K. Saul, “Distance Metric Learning 
for Large Margin Nearest Neighbor Classification,” Neural Information 
Processing Systems , 2005. 
[170].  W. L. Winston, J. B. Goldberg, and M. Venkataramanan, Introduction to 
Mathematical Programming: Operations Research  (4th edition), Duxbury 
Press, 2002. 
[171].  D. Xu, S. Yan, L. Zhang, H.–J. Zhang, Z. Liu, and H.–Y. Shum, 
“Concurrent Subspaces Analysis,” Proc. IEEE Int'l Conf. Computer Vision 
and Pattern Recognition , vol. 2, pp. 203–208, 2005. 
[172].  S. Yan, D. Xu, B. Zhang, and H.J. Zhang, “Graph Embedding: A General 
Framework for Dimensionality Reduction,” Proc. IEEE Int’l Conf. 
Computer Vision and Pattern Recognition , vol. 2 pp. 830–837, 2005. 
[173].  J. Ye, R. Janardan, and Q. Li, “Two–Dimensional Linear Discriminant 
Analysis,” Neural Information Processing Systems , pp. 1,569–1,576, 222 Vancouver, Canada, 2005. 
[174].  J. Ye and Q. Li, “A Two–Stage Linear Discriminant Analysis via QR–
Decomposition,” IEEE Trans. Pattern Analysis and Machine Intelligence , 
vol. 27, no. 6, pp. 929–941, 2005. 
[175].  H. Yu and J. Yang, “A Direct LDA Algorithm for High–dimensional Data 
with Application to Face Recognition,” Pattern Recognition , vol. 34, no. 
12, pp. 2,067–2,070, Dec. 2001. 
[176].  S. Yu, D. Tan, and T. Tan, “Modelling the Effect of View Angle Variation 
on Appearance–Based Gait Recognition,” Proc. Asian Conf. Computer 
Vision , vol. 1, pp. 807–816, 2006. 
[177].  S. X. Yu and J. Shi, “Multiclass Spectral Clustering,” Proc. IEEE Int’l 
Conf. Computer Vision , pp. 313–319, 2003. 
[178].  W.I. Zangwill, Nonlinear Programming: A Unified Approach . Englewood 
Cliffs, NJ: Prentice–Hall, 1969. 
[179].  D. Zhang, A. W.–K. Kong, J. You, and M. Wong, “Online Palmprint 
Identification,” IEEE Trans. Pattern Analysis and Machine Intelligence , 
vol. 25, no. 9, pp. 1,041–1,050, 2003. 
[180].  P. Zhang, J. Peng and N. Riedel, “Discriminant Analysis: A Least Squares 
Approximation View,” IEEE Workshop on Learning in conjunction with 
IEEE Int’l Conf. Computer Vision and Pattern Recognition , San Diego, 
CA, 2005. 
[181].  X. Zhang, Matrix Analysis and Applications , Springer, 2004. 
[182].  W. Zhao, R. Chellappa, and P. Phillips, “Subspace Linear Discriminant 
Analysis for Face Recognition,” Technical Report CAR–TR–914, Center 
for Automation Research, University of Maryland, 1999. 
 
  
 
 